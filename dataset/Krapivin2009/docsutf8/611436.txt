--T
Approaches to zerotree image and video coding on MIMD architectures.
--A
The wavelet transform is more and more widely used in image and video compression. One of the best known algorithms in image compression is the set partitioning in hierarchical trees algorithm which involves the wavelet transform. As today the parallelisation of the wavelet transform is sufficiently investigated, this work deals with the parallelisation of the compression algorithm itself as a next step. Two competitive approaches are presented: one is a direct parallelisation and the other uses an altered algorithm which suits better to the parallel architecture.
--B
Introduction
Image and video coding methods that use wavelet transforms have been successful in providing
high rates of compression while maintaining good image quality and have generated much interest
in the scientic community as competitors to DCT based compression schemes in the context of
the MPEG-4 and JPEG2000 standardisation processes.
Most video compression algorithms rely on 2-D based schemes employing motion compensation
techniques. On the other hand, rate-distortion e-cient 3-D algorithms exist which are able to
capture temporal redundancies in a more natural way [10, 5, 4, 16, 1]. Unfortunately, these
3-D algorithms often show prohibitive computational and memory demands (especially for real-time
applications). Therefore, MIMD architectures seem to be an interesting choice for such an
algorithm.
A signicant amount of work has already been done on parallel wavelet transform algorithms
for all sorts of high performance computers. We nd various kinds of suggestions for 1-D, 2-D
and 3-D algorithms on MIMD computers for decomposition only [8, 19, 15, 13, 3, 6] as well as in
connection with image compression schemes [9, 2].
Zero-tree based coding algorithms e-ciently encode approximations of wavelet coe-cients by
encoding collections of neglectable (insignicant) coe-cients through single symbols. These collections
are called zero-trees because of the tree-like arrangement of wavelet coe-cients which exploits
the wavelet transform's self-similarity property.
A parallelisation of the EZW algorithm { an important zero-tree coding scheme { is presented
in [2] where two approaches are proposed: One is a straight-forward parallelisation which performs
the EZW algorithm locally on each processing element (PE) for distinct blocks. This makes the
resulting bit-stream (BS) incompatible with the sequential algorithm. The other approach reserves
one PE for the collection of the symbols that have to be encoded. This PE performs a reordering
of the symbols before it encodes them. This approach is similar to the approaches presented in
this work and (as far as the author understands) is compatible with the sequential EZW.
An improvement of the EZW is the SPIHT algorithm (Set Partitioning In Hierarchical Trees
[14]). It is a well known, fast and e-cient algorithm which can also be used as 3D-variant in video
compression [5]. However, this algorithm makes use of lists of coe-cients, which makes it hard
to parallelise. This means that although consecutive list entries initially point to neighbouring
wavelet transform coe-cients, the algorithm jumbles the entries after a while. Thus, there is no
easy data driven parallelisation.
An approach to packetise the SPIHT algorithm is presented in [17]. This is similar to the
rst approach in [2]. The output of several executions of SPIHT on spacial blocks is multiplexed
based on a rate allocation technique that approximates distortion reduction by fast SPIHT-specic
statistics. However, the resulting bit-stream (BS) is not compatible with sequential SPIHT.
This work concentrates on the parallelisation of the SPIHT algorithm and presents two competitive
approaches. The rst is a direct parallelisation, i.e. the sequential algorithm is mapped
to the parallel architecture without alteration of the sequential algorithm. Several \tricks" have
to be found to overcome involved parallelisation di-culties. The second approach introduces a
variant of SPIHT that involves a more spacially oriented coe-cient scan order and, thus, avoids
the problems of the rst approach. Similar algorithms are proposed in [18, 11, 12]. Although a
breadth-rst scan order through the trees of coe-cients [18] is PSNR-optimal, a depth-rst scan
order [11, 12] is preferred here because of better spacial separability.
1.1 Parallel Wavelet Transform
The fast wavelet transform can be e-ciently implemented by a pair of appropriately designed
Quadrature Mirror Filters (QMF) consisting of a low-pass and a high-pass lter which decompose
the original data set into two frequency-bands. These sub-bands are down-sampled by 2 and the
same procedure is recursively applied to the coarse scale (low-pass ltered) sub-band. In the 2-D
case consecutive ltering of rows and columns produces four sub-bands (eight in the 3-D case).
Only the one which is ltered with the low-pass lter in each dimension is decomposed further.
This is called the pyramidal wavelet transform.
To perform the wavelet transform in parallel, the data has to be distributed among the PEs in
some way. In this work data is splitted into slices (in the time domain). The ltering is performed
in parallel on local data. Border data has to be exchanged before each decomposition step between
neighbouring PEs due to the lter length. After that, transformed data is found distributed as
shown in Figure 1.
In contrast to the parallelisation of the wavelet transform as presented in a previous paper[7],
the parallel wavelet transform used here dispenses with video data distribution as well as collection
of transformed data. Initial data distribution is not necessary because input is performed in parallel
(i.e. each PE reads its own part of the video data). Note that the speedups reported in this work
do not include I/O operations as I/O is not viewed as a part of the algorithm. Another advantage
of this is that we can drop the host-node paradigm because there is no extra single PE responsible
for data distribution. Likewise, the collection of transformed data is not necessary because data
is passed on to the coding part of the algorithm which is also performed in parallel. There is no
redistribution of data required as we will see in section 2.2 and 3.2.
(a) 2-D case (b) 3-D case

Figure

1: Distribution of coe-cients or list entries after parallel wavelet transform. Dierent
colours indicate dierent PEs
1.2 Zero-Trees
Zero-tree based algorithms arrange the coe-cients of a wavelet transform in a tree-like manner
(as in

Figure

2), i.e. each coe-cient has a certain number of child coe-cients in another sub-band
(mostly 4 in the 2-D, 8 in the 3-D case). We will use the following notations:
o(p) The direct ospring of a coe-cient p, i.e. all coe-cients whose parent coe-cient is p.
desc(p) All descendants of a coe-cient p. This includes o(p), o(o(p)) and so on.
parent(p) The parent coe-cient of p. p 2 o(parent(p)).
Furthermore, a zero-tree is a sub-tree which entirely consists of insignicant coe-cients. The
signicance of a coe-cient is relative to a threshold which plays an important role in the SPIHT
algorithm:
The statistical properties of transformed image or video data (self-similarity) ensures the existence
of many zero-trees. Sets of insignicant coe-cients can be encoded e-ciently with the help of zero-
trees. We will see that sometimes the root coe-cient of the subtree (or even its direct ospring)
does not have to be insignicant.
Zero-trees can be viewed as a collection of coe-cients with approximately equal spacial position.
While this fact implies that the coe-cient's signicances are statistically related, which is exploited
by the SPIHT algorithm, this also means that zero-trees are local objects, corresponding to the
data distribution produced by the parallel wavelet transform (see Figure 2). This can be exploited
by the parallelisation of the zero-tree algorithms (see Section 2.2 and 3.2).
Parallelisation without Algorithm Alteration
2.1 The SPIHT Algorithm
Although the SPIHT algorithm is su-ciently explained in the original paper[14], it is helpful in
this context to reformulate the algorithm.

Figure

2: After parallel decomposition, data is distributed in a way so that each zero-tree resides
on a single PE
threshold
ll LIS, LIP with approximation subband
set LSP empty
for each renement step
threshold threshold =2
process LIS
process LIP
process LSP
(a) Pseudo code LIS LIP LSP
Coefficients
process
process
process
BS
sig.,
sign ref.-bits
sig.,
sign
(b) Data
ow graph

Figure

3: The SPIHT algorithm
Signicance information is represented by three lists:
LIS List of insignicant set of pixels
An entry in this list can be of two types:
Type
Type
LIP List of insignicant pixels
LSP List of signicant pixels
The LIS basically contains all zero-tree roots. An entry of type A corresponds to an insignicant
sub-tree without its root. An entry of type B corresponds to an insignicant sub-tree without its
root and the root's direct ospring. The LIP contains all insignicant coe-cients that are not part
of any zero-tree in the LIS. The LSP contains all signicant coe-cients.
LIS LIP LSP BS
Initialisation Before reading separator After processing separator End
LIS LIP LSP BS LIS LIP LSP BS LIS LIP LSP BS

Figure

4: Functionality of separators. Four states of the three lists and the bit-stream while
processing the LIS.
The algorithm is shown at a coarse level in Figure 3. Initially, the threshold is greater than all
coe-cients. Thus, the LIS and the LIP are lled with the approximation sub-band's coe-cients,
and the LSP is empty. After that, each entry of each list has to be tested for a change of signicance,
and the result of the test has to be encoded as a bit in the bit-stream (BS). If, for instance, a type
A entry of the LIS turns out not to to be insignicant any more (to be precise: its descendants),
a zero bit has to be written into the bit-stream, and the entry has to be deleted from the LIS,
inserted as a type B entry at the end of the LIS and its direct ospring has to be inserted at the
end of the LIP. All entries inserted at the end of a list are also processed in the same renement
step until no more entries are left. Figure 3(b) shows a data
ow graph for a renement step.
The decoding process performs the same algorithm. It does not, however, evaluate the signi-
cance of the list entries, but simply reads this information from the bit-stream and approximates
the value of the corresponding coe-cient as good as it can.
2.2 SPIHT Parallelisation
When parallelising the SPIHT algorithm, we have to face the problem that it uses lists of coe-cient
positions and is, therefore, inherently sequential. The reason is that it is hard to perform general
list operations on distributed lists. Nevertheless, the set of distinct list operations involved in the
SPIHT algorithm is limited. This enables us to develop an e-cient way to manage distributed
lists of coe-cients.
2.2.1 Separators
The basic operations of the algorithm are: Moving an iterator all through a list, deleting elements
at iterator position and appending elements at the end of a list. The aim is to distribute the list so
that each PE-local entry corresponds to a local coe-cient where coe-cients are distributed among
the PEs as shown in Figure 1.
This is a simple task for initial distribution. However, as coe-cients are appended to the end
of lists, one has to provide a mechanism to indicate which parts of a list belong to which PE { or
from a PE's view: where a sequence of local coe-cients ends and parts of another PE's list should
be inserted. This work is done by separators (see Figure 4).
The idea is to insert a separator at the end of each part of the list which entirely belongs to
a single PE. Initially, the approximation sub-band is split into equal slices. Each slice is assigned
to a single PE. On each PE, the (local) lists LIS and LIP are lled with all coe-cients from the
PE's (local) slice, and a separator is appended to the end of each list. From here on, the sequential
algorithm is performed locally with one exception: Each time the iterator meets a separator, the
separator is copied to the end of each destination list. A destination list is a list into which
entries are potentially inserted during the current list processing (see Figure 3(b)). Applying this
principle, the lists L i on PE i are split by separators into parts L ij such that the assembled list
is identical to the list the sequential algorithm would produce. The same
is true for the bit-stream. This enables the parallel algorithm to assemble the bit-stream correctly
after each PE has encoded its part of the wavelet coe-cients.
An important question is when the processing of a list (i.e. a renement step) is completed.
Essentially, the procedure can stop if it has processed the last non-separator entry in the list.
Unfortunately, this does not guarantee that each PE produces the same number of separators.
However, this is a necessary condition for the correctness of the parallel algorithm because, other-
wise, the correct order of the list-parts would be lost. Therefore, the global maximum number of
separators has to be calculated (which unfortunately synchronises the PEs), and the lists have to
be lled up with separators before the algorithm continues with the next renement step (in fact
even before each processing of a list).
As a matter of fact, the number of separators grows exponentially with the number of renement
steps, and very often separators appear in a row together in the list. To avoid unnecessary memory
demands, consecutive separators should be kept together in a single entry associated with a counter.
This means, if a separator entry (containing a counter) is inserted at the end of a list where a
separator entry (also containing a counter) is already sitting, their counters can simply be added.
2.2.2 Algorithm Termination
Another problem is the termination of the whole algorithm. In the sequential case, the process
terminates when the required number of bits have been written to the bit-stream. In the parallel
case, this test (as it is a global test) can only be executed at the end of a renement step. Thus,
the parallel algorithm potentially generates too much bits which of course decreases the speedup
in inconvenient cases. If necessary, super
uous bits can simply be cut after assembling the
bit-stream (due to the nature of the SPIHT algorithm).
The procedure of assembling the bit-stream (after collecting the PE-local bit-streams) is the
only sequential part of the algorithm. Unfortunately, it gets more complicated and, therefore, consumes
more calculation time when the number of PEs is increased. The result will be a signicant
decrease in speedup.
2.2.3 Parallel SPIHT Decompression
Note that for the reverse algorithm { the reconstruction of video data from the bit-stream { the
methods described above are not applicable. Although it is not part of this work, we can shortly
outline the ideas how to implement a parallel SPIHT decoder: First of all, the whole bit-stream
has to be copied to all PEs. Also, each PE has to process all bits of the bit-stream, independent
of whether they belong to local coe-cients. Therefore, the global lists have to be kept at each PE.
The only speedup potentials are:
Not adjusting non-local coe-cients which involves
oating point operations.
threshold
_
_
desc(p) off(p)

Figure

5: Predicates used in the algorithm
Keeping consecutive non-local list entries (entries belonging to non-local coe-cients) together
in a single entry associated with a counter (similar to separators). This is possible because
position information is not needed for non-local entries.
Although this seems to be a very simple approach, it does not imply the necessity of process
synchronisation and it does not contain a sequential part.
3 Parallelisation with Algorithm Alteration
The approach described above reveals some drawbacks as e.g. complicated bit-stream handling,
additional communication needs and non-neglectable sequential code parts. This is a direct consequence
of the fact that the SPIHT algorithm is inherently sequential.
Therefore, we will modify the sequential algorithm itself. Although the resulting bit-stream
will not be compatible with SPIHT, the parallelisation of the altered algorithm will, of course,
be compatible with its sequential version. The basic idea is to substitute the lists of coe-cient
positions involved in the algorithm by bitmaps indicating the membership of each coe-cient to a
certain list. As a result, list iteration, which is used frequently to process the list entries, is turned
into a normal scan of coe-cients that follows a certain spacial direction. Thus, the data driven
parallelisation can be performed more easily by a loop parallelisation of the coe-cient scan.
3.1 Zero-Tree Compression with Signicance Maps (SM)
In the following, we will use three logical predicates A(p), B(p) and C(p) which are dened as
in

Figure

5. A(p) simply denotes the signicance of the coe-cient p. B(p) is true if and only if
at least one of p's descendants is signicant, while C(p) denotes the same but does not include
the direct ospring of p. This is visualised in Figure 5. The state of signicance of a given set
of coe-cients can be described by these predicates in terms of zero-trees. Corresponding to these
predicates, we will use the mappings a, b and c which essentially represent the same as A, B and C.
The dierence is that A, B and C immediately change their values if the threshold is changed and
are, therefore, implemented as a function/procedure in the used programming language. a, b and
c have to be updated explicitly and are, therefore, implemented as array of Boolean values. We
will call a, b and c \signicance maps" (SM). They substitute the lists of coe-cients (see section
2.1).
The algorithm is responsible for the equality of a, b, c and A, B, C respectively while the
ProcessAll :=
threshold
set a, b and c to all false
for each renement step
threshold threshold =2
for p in approximation-subband
c) :=
if a p then Rene(p) else a p A(p)
if
for q in o(p)

Figure

based Zero-tree coding algorithm
threshold is successively decreased by a factor of 1
2 . This should be done by avoiding the evaluation
of A, B and C as far as possible because { following the idea of the SPIHT algorithm { the result
of each evaluation will be encoded into the bit-stream as one bit to allow the decoder to reproduce
the decisions the encoder has made.
The algorithm that obeys these rules is shown in Figure 6. The outer loop is the renement
loop which divides the threshold by 2 in each iteration. This is exactly the same as in the original
algorithm. Within this loop, the algorithm navigates through the set of coe-cients along
trees of coe-cients in a depth-rst manner (this is the major dierence between the SM based
algorithm and SPIHT) starting at the set of coe-cients contained in the approximation sub-band.
This is accomplished by the recursive procedure \ProcessCoe".
For each coe-cient p, the state of a, b and c is checked one after another: a p A(p) has to
be evaluated only if a p is false because the transition true 7! false is not possible for a p . If A(p) is
true then the sign of p has to be encoded as well. If a p is already true the procedure \Rene" is
called which en/decodes another bit of the coe-cients value to rene the decoded approximation.
p B(p) has to be evaluated only if, again, b p is false and ^
C(p) has to be evaluated only if c p is false and b p is true because
last, the recursion to the child coe-cients only has to be performed if b p is true
(for an obvious reason).
The decoding algorithm looks exactly the same. The only dierence is that instead of encoding
the results of the evaluation of A, B and C, this information is read from the bit-stream. Together
with the sign- and renement-bits, this is enough information to enable the decoder to perform
the same steps as the encoder and approximate the coe-cients with an error below the threshold.
Note that this algorithm encodes the same information (in fact the same bits) as the SPIHT
algorithm. The order of the bits is the only dierence. This means that at the end of each
renement step, the compression performance is equal to that of SPIHT. In between, the order of
the bits that are written to the bit-stream is crucial because the bits can have dierent eect on the
decoded image. Thus, it is important to encode bits with greater eect rst. Figure 7 shows the
comparison of the PSNR-performance for the well-known \Lena"-image (2-D case). The algorithm
shows major drawbacks with respect to the original SPIHT which, nevertheless, can almost be
overcome by scanning the set of coe-cients in several passes. The rst pass should only process
those coe-cients that are not part of zero-trees but not signicant. Subsequent passes check the
state of zero-tree roots and process those coe-cients that emerge from decomposed zero-trees. This
method is denoted \sophisticated" in Figure 7. Nevertheless, this improvement of the algorithm is
not used in this work in parallelisation investigations.
PSNR
bpp
SM simple
sophisticated

Figure

7: PSNR of the SM based algorithm compared to the original SPIHT
3.2 Parallelisation
In contrast to the original SPIHT algorithm, the parallelisation of the SM based algorithm is easy.
Again, it is based on the fact that after the parallel wavelet transform, data is distributed in a way
so that zero-trees are local objects (see Figure 2).
All we have to do is to parallelise the inner loop in the procedure ProcessAll (which reads \for p
in approximation-subband") according to the data distribution of the approximation sub-band (see

Figure

1). All other computations within a renement step are localised, i.e. PE-local computations
do not depend on data of neighbouring PEs. So, no communication is required within a renement
step.
Each PE produces one continuous part of the bit-stream for each renement step. At the end,
these parts have to be collected by a single PE and assembled properly (i.e. in an alternating
way). As in the direct SPIHT parallelisation, this is a major bottleneck. However, the number of
bit-stream parts is reduced signicantly, which should speed up the bit-stream assembly.
Because of the termination problem (see section 2.2) the PEs again have to synchronise at the
end of each renement step to determine if the global number of bits produced so far is su-cient.
However, this synchronisation can be dropped if the termination condition is not the bit-stream
size but a xed number of renement steps.
4 Experimental Results
Experimental results were conducted on a Cray T3E-900/LC at the Edinburgh Parallel Computing
Centre using MPI. Video data size is always 864 frames with 88 by 72 pixels. The video sequence
used here is the U-part of \grandma". The wavelet transform is performed up to a level of 3.
Note that the data size is limited by memory constraints in the case when the number of PEs
is 1 and a single PE has to hold all video data. The number of frames has to be high to enable
uniform data distribution for parallelisation as well as down-scaling for the wavelet transform.
output bpp
overall
coding

Figure

8: Sequential speedup of the SM based algorithm with respect to SPIHT
Thus, the frame size is small. In a real-world scenario, however, the frame size can be bigger. For
the same reason, frame size scalability is di-cult to measure. Nevertheless, linear scalability is
assumed due to the authors experiences and the fact that the execution time of the coding part
does not depend on the video data size but on the number of output bits only.
First of all, we have to look at the sequential performance of the SM based algorithm because if
it was slower than the original SPIHT, its parallelisation would not make sense. However, Figure
8 shows that it outperforms the original SPIHT especially for higher bit-rates. On the other hand,
this means that it is even harder to get reasonable speedups.

Figure

9 shows speedups for a xed compression rate: 0:14 bpp (bits per pixel, pixels in dierent
frames are counted as dierent pixels). The fact that the speedup curves are not smooth, i.e. have
discontinuities at and 54, is caused by the divisibility of 864 { the length of the
video sequence which determines the size of the local data sub-sets. Note that due to the depth
of the wavelet transform (3), this size is divided by 8 and the resulting number is then divided by
the number of PEs which is not always possible without remainder.
The sequential bit-stream assembly takes more and more execution time for higher numbers of
PEs. Its share in execution time gets higher than 50% of the coding part. Note at this point that
in a particular hardware implementation, the bit-stream assembly can be integrated in the output
module and separated from the actual coding.
The speedups of the two dierent algorithms are about the same. This shows that the complicated
bit-stream assembly - which is the main problem of a direct SPIHT parallelisation - could
be solved e-ciently. Some dominant problems, such as PE synchronisation and sequential code
parts, are present in both approaches. However, the SM based algorithm is expected to gain a
lower parallelisation e-ciency for two reasons:
It owns the same communication overhead while the sequential algorithm is faster.
There is less potential for positive caching eects in the parallelisation because optimal cache
utilisation (due to the spacially oriented coe-cient scan) is supposed to be the main reason
that the SM based algorithm is faster.
overall
spiht
(a)
overall
coding
(b) SM based algorithm

Figure

9: Speedups for varying #PE and xed compression rate (0.14 bpp).
The fact that the parallelisation e-ciency is about the same strongly suggests that the bit-stream
assembly in the parallel SM based algorithm is more e-cient.

Figure

shows speedup curves for xed #PE and varying compression rate. Of course, the
execution time of the wavelet decomposition does not depend on the compression rate. The reason
for the speedup breakdowns at certain compression rates is the termination problem of the parallel
algorithm (see section 2.2.2). The parallel algorithm is optimal only at compression rates achieved
at the end of a renement step.
Note that although the speedup of the coding part increases with the bit-rate, the overall
speedup remains constant or drops slightly because the share in execution time of the coding part
increases with the bit-rate.
The problem of unevenly distributed complexity is illustrated in Figure 11. Here, approximately
the rst half of the video sequence is substituted by the \car-phone" sequence which
contains much more motion than the \grandma" sequence. This causes bigger coe-cient values at
higher frequency sub-bands for the more complex video parts. Thus, more coe-cients have to be
processed within a renement step, which makes the algorithm consume more computation time.
A load balancing problem is the consequence. One can clearly see that the necessity of process
synchronisation at several points in the algorithm leads to an increase of idle times of PEs waiting
for other PEs. Figure 12 shows the speedups that are { compared to Figure 9 { slightly reduced
because of this problem.
Conclusions
We have seen how an inherently sequential zero-tree coding algorithm can be parallelised. Although
the speedups are not overwhelming, the presented way of parallelisation prevents the necessity to
perform the coding sequentially. Thus, reasonable speedups are possible for higher numbers of
processing elements and the whole range of compression rates.
There are two methods for parallelisation: Either by the use of so called separators or by
output bpp
decomposition
overall
spiht
(a) SPIHT, 8 PEs26100.004
output bpp
decomposition
overall
coding
(b) SM based, 8 PEs5150.004
output bpp
decomposition
overall
spiht
(c) SPIHT,
output bpp
decomposition
overall
coding
(d) SM based,
output bpp
decomposition
overall
spiht
output bpp
decomposition
overall
coding

Figure

10: Speedup for decomposition, coding and overall speedup for varying compression rate.
(a) evenly distributed complexity13579
(b) unevenly distributed complexity

Figure

Execution scheme of decomposition and SPIHT coding for 10 PEs. Time (on horizontal
axis) is measured in milliseconds. (Nearly) vertical black lines indicate data transfer. Horizontal
grey bars indicate calculation phases.515253510 20
overall
spiht

Figure

12: Speedups for video with unevenly distributed complexity (at xed compression rate
(0.14 bpp)).
rewriting the algorithm to t better into the parallel architecture (signicance map based algo-
rithm). The rst method is more complicated but guarantees compatibility with original SPIHT
bit-streams. The second method shows very similar speedup results but better execution times.
Although unevenly distributed image/motion complexity can decrease the speedup potential,
this eect seems to keep within limits.

Acknowledgments

The authors would like to acknowledge the support of the European Commission through TMR
grant number ERB FMGE CT950051 (the TRACS Programme at EPCC). The author was also
supported by the Austrian Science Fund FWF, project no. P13903.



--R


Image coding using parallel implementations of the embedded zerotree wavelet algorithm.
On the scalability of 2D discrete wavelet transform algo- rithms

An embedded wavelet video coder using three-dimensional set partitioning in hierarchical trees (SPIHT)
Parallel algorithm for the two-dimensional discrete wavelet transform
Hardware and software aspects for 3-D wavelet decomposition on shared memory MIMD computers
Optimization of 3-d wavelet decomposition on multiprocessors
Parallelization of the 2D fast wavelet transform with a space- lling curve image scan
Video compression using 3D wavelet transforms.
Listless zerotree coding for color images.
3d listless zerotree coding for low bit rate video.
Scalability of 2-D wavelet transform algorithms: analytical and experimental results on coarse-grain parallel computers

Vector and parallel implementations of the wavelet transform.
Multirate 3-D subband coding of video

image compression without lists.
Parallel discrete wavelet transform on the Paragon MIMD machine.
--TR
On the Scalability of 2-D Discrete Wavelet Transform Algorithms
Hardware and Software Aspects for 3-D Wavelet Decomposition on Shared Memory MIMD Computers
An Embedded Wavelet Video Coder Using Three-Dimensional Set Partitioning in Hierarchical Trees (SPIHT)

--CTR
Roland Norcen , Andreas Uhl, High performance JPEG 2000 and MPEG-4 VTC on SMPs using OpenMP, Parallel Computing, v.31 n.10-12, p.1082-1098, October - December 2005
