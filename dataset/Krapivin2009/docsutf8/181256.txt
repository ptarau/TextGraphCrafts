--T
Factorization of Matrix Polynomials with Symmetries.
--A
An $n \times n$ matrix polynomial $L(\lambda)$ (with real or complex coefficients) is called self-adjoint if Factorizations of selfadjoint and symmetric matrix polynomials of the form are studied, where $D$ is a constant matrix and $M(\lambda)$ is a matrix polynomial.  In particular, the minimal possible size of $D$ is described in terms of the elementary divisors of $L(\lambda)$ and (sometimes) signature of the Hermitian values of $L(\lambda)$.
--B
Introduction
. Let
be a matrix polynomial, where A j (j =
are complex n \Theta n matrices and - is a complex parameter. The polynomial L(-)
is called selfadjoint if
Factorizations of the form
is a constant matrix (not necessarily of the same size as L(-) and M(-) is a
matrix polynomial, have been studied in the literature, under various additional hypotheses
(see [Ja, Co, GLR1, GLR2]). The study of factorizations (1.1) is motivated by several
applied problems, such as filtering (Chapter 9 of [AM]). Factorizations of matrix polynomial
L(-) having other types of symmetries, such as
been studied in the literature as well (see, e.g., [Lyu1, Lyu2]). For such polynomials, it is
natural to seek factorizations of type
is a constant matrix (not necessarily of the same size as L(-)); M(-) is a
matrix polynomial, and
In this paper we identify the minimal possible size of the matrix D in factorization of
types (1.1) and (1.2), where L(-) has the appropriate symmetry. The cases when L(-)
has complex coefficients or real coefficients are studied (if L(-) is assumed to be real, then
in (1.1) and (1.2) M(-) and D are assumed to be real as well). Our result concerning
*Faculteit Wiskunde en Informatica, Vrije Universiteit, De Boelelaan 1081, 1081 HV Amsterdam, The
Netherlands
yDepartment of Mathematics, College of William and Mary Williamsburg, VA 23187-8795, USA. Partially
supported by NSF Grant DMS-9000839 and by the NSF International Cooperation Grant with the
Netherlands.
factorization (1.1) is a generalization of the main result of [GLR2] where only the case of
constant signature was considered under the additional hypothesis that det L(-) 6j 0
We present also (in Section 2) general factorization results in an abstract framework,
for matrix polynomials over a field having suitable symmetries. These results, although independently
interesting, play an auxiliary role in this paper, serving as essential ingredients
in the proofs of the main results given in Sections 3-6.
The following notation will be used throughout the paper. Standard notation R(C) to
denote the real (complex) field, and I k for the k \Theta k unit matrix. A T (resp.
for the transpose (resp. conjugate transpose) of a matrix A, and
abbreviated to A \GammaT (resp. A \Gamma  ). Block diagonal matrix with the blocks Z 1 ; . ; Zm on the
main diagonal will be denoted Z 1 \Phi \Delta \Delta \Delta \Phi Zm or diag (Z 1 ; . ; Zm ). For a Hermitian n \Theta n
matrix X, let -+ (X) be the number of positive (resp. negative, or
zero) eigenvalues of X counted with multiplicities. Thus,
Given a matrix polynomial L(-) over C, its general rank r(L) is defined by
It coincides (when with the notion of general rank introduced and used in Section
2 for matrix polynomial over a field F . The points -
be called regular points of L(-); all other points - 0 2 C will be called singular . Clearly,
the set of singular points is finite (or possibly empty). An n \Theta n matrix polynomial L(-)
is called regular if

Acknowledgement

. The problem concerning the minimal possible size of D in factorizations
for complex selfadjoint matrix polynomials has been posed by
Prof. I. Gohberg.
2. Symmetrix matrix polynomials over general field. Let F be a (commutative)
field, and let F [-] be the ring of polynomials over F in one variable -. Matrices L(-) with
entries in F [-] will be called matrix polynomials (over F ). It is well-known (see, e.g., [M])
that every m \Theta n matrix polynomial L(-) admits a representation (called the Smith form)
where E(-) and F (-) are matrix polynomials with sizes m \Theta m and n \Theta n, respectively, and
having constant non-zero determinants, and d 1 (-); . ; d r (-) are monic scalar polynomials
(over F ) such that d i (-) divides d i+1 (-) 1). The polynomials d i (-) are
called invariant polynomials of L(-); these polynomials, as well as their number r, are
uniquely determined by L(-) : r \Theta r is the maximal size of a square submatrix in L(-)
with determinant not identically zero, and for r the product d 1 (-) . d i (-) is
the greatest common divisor of the determinants of all i \Theta i submatrices in L(-).
The number r will be called the general rank of L(-) and denoted r(L).
In this section we will study factorizations of symmetric matrix polynomials, and the
Smith form will be our main tool.
From now on we assume that the characteristic of F is different from 2. For a given
automorphism oe of F such that oe consider the following
transformation: for
a
a
For an m \Theta n matrix polynomial
where e
We have:
polynomials x(-) and y(-).
These rules will be used often in the sequel.
An m \Theta n matrix polynomial L(-) will be called generally invertible if all its invariant
polynomials are constant 1. The terminology is justified by the fact that L(-) is generally
invertible if and only if L(-) has a generalized inverse, i.e. matrix polynomial N(-) such
that (this fact is easily proved using
the Smith form). A matrix polynomial L(-) will be called right (resp. left) invertible if
there exists a matrix polynomial N(-) such that L(-)N(-) j I (resp. N(-)L(-) j I).
We now state one of the main factorization results of this section.
Theorem 2.1. Let L(-) be an n \Theta n generally invertible matrix polynomial such that
and let r be the general rank of L(-). Then L(-) can be factorized in the form
where M(-) is an r \Theta n right invertible matrix polynomial and D is an r \Theta r constant
matrix such that
Conversely if (2.4) holds for an r \Theta n right invertible matrix polynomial M(-) and a
constant matrix generally invertible and has general
rank r.
Proof. The converse statement is easy: Indeed, if
E(-)[I 0] e
F (-)
is the Smith form for M(-), then
F   (-)
I#
e
E(-)[I 0] e
F (-)
F   (-)
I
I
e
so by uniqueness of the Smith form L(-) is generally invertible and has general rank r.
The verification of (2.3) is trivial.
We now prove the direct statement.
Observe that the proof is easily reduced to the case when
const E(-)D(-)F (-) be the Smith form of L(-), and let e
L(-) is a matrix polynomial, e
L   (-), and because of
the equality e
E(-)D(-) the last rows and columns of e
are zeros.
Obviously, it will suffice to prove the direct statement for the r \Theta r matrix polynomial N(-)
formed by the first r rows and columns of e
L(-). As det N(-) j const : 6= 0, the required
reduction is accomplished.
We assume from now on that det L(-) In this case the direct statement
follows from Theorem 3 in [Lyu1] (see also [Lyu2]). We outline an alternative procedure
developed in [Co]. As in [Co] or [GLR2] (Section 4) we prove that there exists an n \Theta n
matrix polynomial X(-) with det X(-) such that for the matrix polynomial
either ff 11 j 0 or A(-) is diagonally dominant (i.e. for the degree of ff jj (-)
is bigger than the degrees of all non-zero entries in the j-th row and the j-th column in
A(-)). Because of this fact, without loss of generality we can assume that either L(-) is
diagonally dominant or the (1,1) entry in L(-) is identically zero. If L(-) is diagonally
dominant, then it must be constant, and we are done. So let
a A 1
polynomials. Now put
c I
A calculation shows that " 1 \Gammax
I
Y
\Gammac I
and so det Y = 1. Another straightforward calculation shows that
Y
where L 0 (-) is an (n \Gamma 1) \Theta (n \Gamma 1) matrix polynomial. Thus, we have reduced the size of
L by one and can complete the proof by induction on n.
As the proof of Theorem 2.1 shows, the constant matrix D can be taken diagonal.
If L(-) is not generally invertible, then easy examples show that the representation (2.4)
(with D having the size equal to the general rank of L(-)) is not always possible, even if we
omit the requirement that M(-) is right invertible. We can, however, obtain a factorization
result for not generally invertible L(-) if we allow D to be a polynomial (with special
properties). To state and prove this result we need the concept of elementary divisors. Let
L(-) be an m \Theta n matrix polynomial with invariant polynomials d 1 (-); . ; d r (-), where
d s (-); d s+1 (-); . ; d r (-) are non-constant (if L(-) is generally invertible, then we say that
L(-) has no elementary divisors). Factor:
(-) are irreducible and pairwise relatively prime non-constant monic
scalar polynomials (over F ). The collection of factors (f ij
each factor is repeated as many times as it occurs in (2.5), is called
elementary divisors of L(-), and the positive integer ff ij is called the order of the elementary
. Because of the divisibility relations among invariant polynomials,
the collection of elementary divisors of L(-) determines the invariant polynomials uniquely,
and therefore is invariant under the transformations
Theorem 2.2. Let L(-) be an n \Theta n matrix polynomial such that
and let r be the general rank of L(-). Further, let ff 1 (-) ff 1 ; . ; f q (-) ff q g be the collection
of elementary divisors of L(-). Then L(-) admits a factorization
where M(-) is an r \Theta n matrix polynomial, and is an r \Theta r matrix polynomial;
moreover, the collection of elementary divisors of D(-) is ff Jg, where the subset
J of f1; 2; . ; qg consists precisely of those indices j for which f
odd.
Recall that is taken from (2.2).
Proof. As in the proof of Theorem 2.1, we can assume that
E(-)D 1 (-)F (-) be the Smith form L(-), where the invariant polynomials d 1 (-); . d r (-)
are on the main diagonal of D 1 (-). Because and by the uniqueness of invariant
polynomials we have in fact d i r). The factor " degree d i appears
because d i is monic (this is part of the definition of invariant polynomials) while the leading
coefficient of d i  is " degree d i . In the sequel it will be convenient to denote
for a scalar polynomial f . Thus, d . Observe that f+ is monic if f is monic and
that
Replacing L by F \Gamma1
, we can further assume without loss of generality that the
i-th column of L is divisible by d i (-) n). By symmetry, the i-th row of L is
divisible by d i  (-). Let the nonconstant invariant polynomials of L(-) be d s (-); . d r (-)
and factor them as in (2.5). Then
Y
Y
and by the uniqueness of decomposition (2.5) we obtain that the set ff i1 ; . ; f ik i
must
consist of selfsymmetric polynomials (f of pairs of mutually symmetric
case necessarily ff ij
are
selfsymmetric and
here be the smallest index such that ff i
say, no such exists, we put
Y
Y
Y
where we assume that the elementary divisors are numbered so that f
To make the subsequent formulas more uniform we define also
1. The divisibility relations between the d i 's imply that whenever f
Consequently, we obtain that h i divides h
The formulas (2.5) lead to the factorization
where
Y
Y
(the sign in g i is chosen so that g i is monic). Clearly, g i divides g i+1 for
In view of (2.6) we now have a factorization
for some matrix polynomial e
L   . Denote by e
d r (-) the invariant polynomials
of e
L(-). Equality (2.7), together with the Binet-Cauchy formula for determinants
of submatrices in the product of several matrices implies the following: Every determinant
of j \Theta j submatrix in L(-) is a linear combination (with polynomial coefficients) of
the determinants of j \Theta j submatrices in e
L(-) when the determinants are multiplied by
Y
It follows that d 1 (-) . d j (-) divides e
Y
r). The equality (2.6) now shows that g 1 (-) . g j (-) divides e
On
the other hand, for the (i; j)-th entries e
ij of e
L and p ij of L, respectively, we obtain
e
and since (assuming i -
are polynomials, e
divisible by g j .
Also, e
divisible by g i (because g i divides g j if i - j). By the symmetry of e
L, we obtain
that e
divisible by g max(i;j) . Therefore, the determinant of every j \Theta j submatrix of
e
L is divisible by g 1 . g j . Consequently, e
divides g 1 . g j . Comparing with the
previously obtained opposite divisibility relation, we conclude that r are in fact
the invariant polynomials of e
L.
Repeat the procedure given above with L replaced by e
L, and so on, until (after a
finite number of steps) we obtain a matrix polynomial D(-) with the properties required
in Theorem 2.2.
3. Factorization of selfadjoint matrix polynomials on the real axis. In this
section we consider matrix polynomials L(-) over C with the following property:
Such polynomials will be called selfadjoint .
Theorem 3.1. Let L(-) be a selfadjoint n \Theta n matrix polynomial. Then L(-) admits
a factorization
where D is an m \Theta m constant Hermitian matrix and M(-) an m \Theta n matrix polynomial
if and only if
where
Moreover, in all factorizations (3.1) having the minimal size m 0 \Theta m 0 of D, the matrix D
is uniquely determined up to congruence: D has max-+ (L(-)) positive eigenvalues and
negative eigenvalues (multiplicities counted).
We can say more about the spectral properties of the factor M(-) in (3.1). A set   of
non-real numbers is called a c-set (with respect to a selfadjoint matrix polynomial L(-))
if   is a maximal (by inclusion) set of non-real singular points of L(-) with the property
that
2  . (The case when a c-set is empty is not excluded.) The concept
of c-set was introduced and used in [GLR1, GLR3]. It turns out that, given L(-) as in
Theorem 3.1, and given a c-set  , there exists a factorization (3.1) where D is m 0 \Theta m 0
and where the set of non-real singular points of M(-) coincides with  . This statement
follows as a by-product of the proof (given below) of Theorem 3.1.
Theorem 3.1 admits an alternative formulation. An n \Theta n matrix polynomial M(-)
will be called elementary if positive semidefinite for all real -. It
is not difficult to see (this fact is actually a particular case of Theorem 3.1) that M(-) is
elementary if and only if M(-) is of the form is an
polynomial. One can consider elementary matrix polynomials as building
blocks for selfadjoint matrix polynomial, in the same spirit as the constant rank 1 positive
semidefinite matrices are building blocks for constant Hermitian matrices:
Theorem 3.2. Any selfadjoint n \Theta n matrix polynomial L(-) admits a representation
are elementary matrices. The number m of terms in (3.4) is
greater than or equal to m 0 , where m 0 is given by (3.3), and if exactly
max-+ (L(-)) of " j 's are equal to +1 and exactly max- \Gamma L(-)) of '' j 's are equal to \Gamma1.
To obtain Theorem 3.2 from Theorem 3.1, assume (without loss of generality) that
in (3.1) D is a diagonal matrix with \Sigma1's on the main diagonal. Then let
is the j'th row of M(-), to produce the formula (3.4).
Corollary 3.3. Any selfadjoint n \Theta n matrix polynomial admits a factorization (3.1),
or a representation (3.4), where m - 2n.
There are selfadjoint matrix polynomials, for example, L(-I, for which there do
not exist representations (3.1) or (3.4) with
The rest of this section will be devoted to the proof of Theorem 3.1.
We start with the easy direction. Let be given a factorization (3.1), and let - 0 be a
real point for which
As must have at least
positive eigenvalues. Analogously, D must have at least - \Gamma (L(- 1 negative
eigenvalues, where - 1 2 R is chosen so that
We obtain therefore the inequality (3.2). It is also clear that in any factorization (3.1),
where D is m 0 \Theta m 0 , the Hermitian matrix D is unique up to congruence.
It remains to show that a given selfadjoint matrix polynomial L(-) admits a factorization
the size of D. This is the difficult part and we need some
preliminaries. Note that L(-) is selfadjoint if and only if where the transformation
a ! a   is defined as in Section 2, with
Nevertheless, here the general results of Section 2 will not be used because the preliminary
results we need (such as Proposition 3.4 below) are already available in the literature (it
should be noted however that the result of Theorem 2.2 plays an essential role in the proof
of Proposition 3.4).
First observe that there exists an n \Theta n matrix polynomial N(-) with constant non-zero
determinant such that
where L 0 (-) is a selfadjoint k \Theta k matrix polynomial, Theorem 32.4 in
[M], where (3.5) is proved for symmetric matrices over principal ideal rings, with (N(-))
replaced by N(-) T ; the same proof works to produce (3.5); also, (3.5) can be obtained
without difficulties from the Smith form of L(-) (see Section 2). Because of (3.5) we can
(and will) assume from the very beginning that the general rank of L is equal to n, i.e.
det L(-) 6j 0.
Our next observation is that the result of Theorem 3.1 is known in the case L(-) has
constant signature, i.e. -+ (L(-)), and therefore also
for all real regular points -:
Proposition 3.4. ([GLR2]) Let L(-) be a selfadjoint n \Theta n matrix polynomial such
that
(necessarily det L(-) 6j 0). Then L(-) admits a factorization (3.1) with n \Theta n the size of
D.
We will prove the following lemma:
Lemma 3.5. Let L(-) be a selfadjoint n \Theta n matrix polynomial with det L(-) 6j 0,
be defined by (2.3). Then there exists an m 0 \Theta m 0 selfadjoint matrix
polynomial e
L(-) such that
e
and such that
or equivalently e
L is regular and has constant signature.
Proof. By Rellich's Theorem [R] (see also [GLR3]) the eigenvalues - 1 (-n (-) of
L(-) for - real can be enumerated so that - 1 (-n (-) are real analytic functions of
the real variable -. Clearly, - 0 2 R is singular if and only if - 0 is a zero of at least one of
the analytic functions - 1 (-n (-). Let - 0 2 R be singular, and let
For every be the multiplicity of - 0 as a zero of - j (-), and let " j be the
sign of the non-zero real number [- (m j )
suppress the dependence of m j and
" j on - 0 in the notation.) Define the integer q(- 0 ) by
- f# of indices
From the definition of q(- 0 ) it is clear that
for all sufficiently small " ? 0. It is easy to see that
where the maximum is taken over all regular real points - 1 and - 2 . Also, as it follows from
(3.7),
where the summation in the right hand side of (3.9) is over all singular points - 0 in the
Denote the right-hand side of (3.9) by p. We now construct p scalar real polynomials
with the following properties:
(i) all zeros of r j (-) (j = are real and simple and belong to the set S of real
singular points - 0 of L(-) for which q(- 0 ) 6= 0;
(ii) for every - 2 S exactly jq(- 0 )j polynomials among r 1 (-); . ; r p (-) have - 0 as their
zeros; and for each r j (-) such that r j (- 0
The definition of p ensures that such polynomials r 1 (-); . r p (-) can indeed be constructed.
Let
e
By the property (ii), an in view of the qualities (3.8), (3.9), it is easy to see that the
number of positive eigenvalues of e
L(-) is constant for every real - which is a regular point
for L(-). The equality of (3.6) therefore follows.
Now we can easily finish the proof of Theorem 3.1. Indeed, given a selfadjoint matrix
polynomial L(-) with det L(-) 6j 0, construct e
L(-) as in Lemma 3.5 and apply Proposition
3.4 to e
e
where D is a constant m 0 \Theta m 0 Hermitian matrix. Then (3.1) holds for M(-) formed by
first n columns of N(-).
4. Factorization of real symmetric matrix polynomials. Let
be a real symmetric matrix polynomial, i.e. A j (j = 0; . ; ') are real symmetric n \Theta n
matrices. For such polynomials L(-) we consider factorizations
where D is a constant real symmetric m \Theta m matrix and M(-) is a matrix polynomial
with real coefficients.
It will be convenient to state the next theorem in terms of elementary divisors (see
Section 2 for definitions of the concepts related to elementary divisors).
Theorem 4.1. Let L(-) be a real symmetric n \Theta n matrix polynomial, and assume
that the elementary divisors of L(-) which are powers of irreducible quadratic polynomials
(over R) all have even orders. Then L(-) admits a factorization (4.1) if and only if
defined by (3.3). Moreover, in factorization (4.1) with the minimal possible size
of D, the matrix D is uniquely determined up to congruence and has exactly max
positive eigenvalues and exactly max
negative eigenvalues, multiplicities counted.
Alternatively, L(-) admits a representation
are real elementary matrices and " In case
the number of +1's (resp \Gamma1's) among the " exactly
In particular, Theorem 4.1 applies if all singular points of L(-) are real.
Proof. The "only if " part (easy direction) is proved as in the proof of Theorem 3.1.
Also, we can easily reduce the proof to the case when det L(-) 6j 0. Using Theorem 2.2
(with we further can assume that all elementary divisors
of L(-) are first degree polynomials (necessarily with real roots). From now on the proof
proceeds in the same way as that of Theorem 3.1. The role of Proposition 3.4 is played by
Proposition 4.2 below.
Proposition 4.2. Let L(-) be a real symmetry n \Theta n matrix polynomial with all
elementary divisors first degree polynomials. Assume further that
Then L(-) admits a factorization (4.1) with n \Theta n the size of D.
Proposition 4.2 can be proved by repeating the arguments leading to the proof of
Theorem 1 in [GLR2]. We omit the details.
If the hypothesis on the orders of elementary divisors of L(-) is omitted in Theorem
4.1, easy scalar examples (for example, show that the result of Theorem
4.1 is generally not valid. Scalar examples show also that, in this case the matrices D of
minimal size in factorizations (4.1) are not necessarily congruent to each other:
6 42
We have, however, and upper bound on the minimal rise of D:
Theorem 4.3. Let L(-) be a real symmetric n \Theta n matrix polynomial, and let m 0 be
defined by (3.3). Then for every admits a factorization (4.1).
Proof. Assume first that m 0 - n. By Theorem 3.1, we have
where D is m 0 \Theta m 0 constant Hermitian matrix (which can can be chosen to be real
without loss of generality), and M(-) is a complex matrix polynomial. Write
are real matrix polynomials. Then, separating
the real part in (4.3) we obtain
which is the desired factorization (with
use the simple equality:
5. Factorization of symmetric real polynomials on the imaginary axis. In this
section we consider the case of n \Theta n matrix polynomials L(-) such that
and L(-) is real for real -. Note that such a polynomial is selfadjoint on the imaginary
axis, i.e., immediate consequence of Theorem 3.1 (applied
to L(i-)) is that such matrix polynomial admits a factorization
for a complex m \Theta m Hermitian matrix D and a complex m \Theta n matrix polynomial M(-)
if and only if
We shall show in this section that D and M(-) can be taken real. Note that here is
a contrast with the situation of Section 4, where an analogous factorization of a real
symmetric matrix polynomial having real factors is not always possible.
First we shall deal with the case when L(-) is regular and has constant signature
(on the imaginary axis), after which the general case is reduced to the case of constant
signature.
So in view of Theorem 2.2 (with restrict our
attention to matrix polynomials having only elementary divisors of the form - or real and nonzero. Next, we deal with the case when L is regular and has
constant signature.
Theorem 5.1. Suppose L(-) is a real regular n \Theta n matrix polynomial satisfying
having constant signature on the imaginary axis: -+ (L(-)) is constant
for all regular points - 2 iR. Then L admits a factorization
where M is an n \Theta n matrix polynomial with real coefficients and D is an n \Theta n constant
real matrix.
Proof. Again by Theorem 2.2 we may assume L has only pure imaginary eigenvalues
and all elementary divisors are linear (in the sense of C). First we deal with the case when
is an elementary divisor of L. Using the Smith form of L(-), write
are real monic scalar polynomials and E(-); F (-) are real n \Theta n
matrix polynomials with det E(-) we have for
A 22 (-)
B 11 is a q \Theta q matrix polynomial. Moreover b
must be invertible, as
otherwise det b
L(-) and hence also det L(-) would be divisible by (-
L has constant signature, so has b
L, which means b
L(i-) for - 2 R is a Hermitian matrix
having constant signature for all real - except at a finite number of points. Using Rellich's
theorem [R], we can write
where U(-) is unitary valued and analytic, and - j (-) is analytic and real. The functions
have simple zeros only as b
L has only linear elementary divisors (over C). Without
loss of generality we may assume in [GLR2] we
have that q is even, and exactly half of the numbers - 0
is positive, the
other half if negative. Let u j be the j-th column of (U(- 0 ))   . Then one calculates
the quadratic form given by b
positive squares and q
negative squares.
Now by (5.2) Ker L(i- 0 ) is span
d
d-
Therefore we conclude that there is an invertible matrix V such that
#)
where the block
is repeated q
times. Moreover, a simple argument shows that
V can be taken such that it has a real determinant.
Now we will first state and prove a lemma, after which we shall return to the proof of
Theorem 5.1.
Lemma 5.2. Let W be a complex invertible n \Theta n matrix with real determinant, and
let - 0 be a non-zero real number. Then there exists a real n \Theta n matrix polynomial M(-)
with constant determinant such that M(i- 0
Proof. We can decompose W as a product of elementary matrices:
where each W j is either triangular with ones on the diagonal and exactly one non-zero
off-diagonal entry, or W j is a diagonal invertible matrix. Multiplying each diagonal w j by
a suitable complex number ff j so that det(ff j W j ) is real, we can assume without loss of
generality that det W j is real (j = here we use the hypothesis that det W is real.
Furthermore, by writing
(here f0g), we can assume that every diagonal matrix W j in (5.4) has real non-zero
determinant and at most two diagonal entries different from 1 (located in adjacent
positions). Clearly, it will suffice to construct a polynomial M(-) as required such that
fixed j). If W j is triangular, let
real than the constant M(-) j W j will
do. Finally, if W
Here
where d 1R (resp. d 1I ) stands for the real (resp. imaginary) part of d 1 . (The 2 \Theta 2 block
is in the same position in M(-) as the position of
is in
.) It is easy to verify that q(-) is in fact a real polynomial, M(-) (defined by (5.5)) is
a real matrix polynomial with constant non-zero determinant, and M(i- 0
Now let us return to the proof of Theorem 5.1. Let V be as in (5.3), and choose M(-),
a real q \Theta q polynomial with constant non-zero determinant such that M(i- 0 This
is possible by Lemma 5.2 (recall det V is real). Now we may replace b
L(-) by
e
I
As M has constant non-zero determinant, e
L is a matrix polynomial, and we may write
e
where
Now put
\Phi I n\Gammaq
where the leading block is repeated q
times.
We shall show that
L(-)K(-) \Gamma1 is a polynomial. Note that it may
have a pole only at \Sigmai- 0 , and it suffices to show it has no pole at either one of these points.
Moreover, any pole of N must appear in its leading q \Theta q block. This leading q \Theta q block
equals
as an easy computation shows. Now at we have that
Recalling that B 11 (i- 0
we see that N(-) is a polynomial.
Taking determinants we see that N(-) has no eigenvalue at \Sigmai- 0 . Applying the same
argument at each non-zero, singular point of L we reduce the proof of Theorem 5.1 to
the case when zero is the only possible singular point of L(-). However, for that case a
similar argument shows that L(-) admits a representation
with K(-) a real matrix polynomial and N(-) a real matrix polynomial without singular
points (cf. the proof of Proposition 3.4 given in [GLR2]).
So we have reduced to the case where L has no singular points. In this case the result
follows from Theorem 2.1.
Next we state the main result of this section.
Theorem 5.3. Let be an n \Theta n matrix polynomial with real coeffi-
cients. Then there is a real m \Theta m matrix
with real coefficients such that
if and only if
the matrix D is unique up to congruence by a real orthogonal
matrix.
Analogously to Theorem 3.1, the polynomial M(-) in Theorem 5.3 can be chosen with
additional spectral properties. Given a polynomial L(-) as in Theorem 5.3, a set   of
numbers with non-zero real parts will be called a d-set (with respect to L(-)) if   is a
maximal set of singular points of L(-) with non-zero real parts having the property that
d-set may be empty.) It turns out that under the hypotheses
of Theorem 5.3, for every given d-set   there exists a factorization (5.6) where
is and where the set of non-real singular points of M(-) coincides with  . This
follows as a by-product of the proof of Theorem 5.3 (including Theorem 5.1 and Theorem
2.2 with
Proof. The uniqueness of D is verified as in the proof of Theorem 3.1, as well as the
fact that m - m 0 is necessary for the existence of real and M(-) such that (5.6)
is satisfied is seen as in the proof of Theorem 3.1. It remains to prove sufficiency. We may
reduce to the regular case again as in Section 3. In case L has constant signature we are
finished, using Theorem 5.1. In case L does not have constant signature on the imaginary
axis, it will be shown that there exists a real m 0 \Theta m 0 matrix polynomial e
L(-) such that
e
L(\Gamma-) T and
while e
L is regular and has constant signature on the imaginary axis. Indeed, as L(i-) is
selfadjoint for real - we can write (using Rellich's theorem [R], also [GLR3])
is analytic and real valued and U(-) is analytic and unitary. Since
R, the matrices L(i-) and L(\Gammai-) have the same eigenvalues, and therefore
for every point - 0 2 R there is a permutation oe on f1; . ; ng such that
in a neighborhood of - 0 .
R be such that \Sigmai- 0 are singular points of L(-).
as in the proof of Lemma 3.5. It follows from (5.8) that q(- 0
singular point of L(-)). Furthermore (analogously to (3.8) and
and
where the summation is over all - is a singular point of L.
(here the real numbers - 1 and - 2 are such that i- 1 and i- 2 are regular points of L).
Denote the number (5.9) by p, as in the proof of Lemma 3.5. Now construct p polynomials
real coefficients having the following properties:
is real for - 2 R,
(ii) all zeros of r j are pure imaginary, non-zero numbers and belong to the set S of pure
imaginary singular points - 0 of L for which q(- 0 ) 6= 0,
(iii) for every - 0 2 S exactly jq(- 0 )j polynomials among r 1 ; . ; r p have - 0 as a zero and
for each r j having - 0 as a zero we have
d-
(Note that because of (i) and q(\Gamma- 0 is satisfied at \Gamma- 0 if it is
satisfied at - 0 .) Put
e
L(-) is regular and has constant signature on the imaginary axis as desired. Thus
by Theorem 5.1 e
L admits a factorization
e
with D an m 0 \Theta m 0 real matrix and N(-) an m 0 \Theta n real matrix polynomial. Taking for
M(-) the matrix polynomial formed by the first n columns of N now finishes the proof.
Analogously to Theorem 3.2, the result of Theorem 5.3 can be put in terms of additive
representations of L(-) via elementary matrix polynomials. Here, a real n \Theta n matrix
polynomial M(-) will be called elementary if positive semidefinite
Hermitian for all - 2 iR.
Theorem 5.4. Let L(-) be as in Theorem 5.3. Then L(-) admits a representation
are elementary matrix polynomials, if and only if
defined by (5.7). Moreover, when exactly
in (5.10) are equal to +1, and exactly max
of them are equal to \Gamma1.
We omit the easy derivation of Theorem 5.4 from Theorem 5.3.
6. Factorization of complex symmetric polynomials. In this section we consider
n \Theta n matrix polynomials L(-) with complex coefficients having the symmetry
is fixed, and their factorizations of the form
where M(-) is an m\Thetan matrix polynomial (over C), and D is a constant complex symmetric
matrix. Observe that every m \Theta m complex symmetric matrix D can be factored as
(see, e.g., p. 159 in [HJ]). Therefore, we may
assume that I in (6.2).
Here (in contrast with Sections 3-5) signatures of Hermitian matrices do not play a
role.
We start with the case
Theorem 6.1. Let L(-) be an n \Theta n matrix polynomial satisfying (6.1), where
Then the minimal size m for which L(-) admits a factorization
with an m \Theta n matrix polynomial M(-) is equal to the general rank r of L(-).
Proof. We use the same ideas as in the proofs of results in the previous sections.
Therefore, the proof of Theorem 6.1 will be presented with less detail.
Clearly, a factorization (6.3) is impossible if m ! r. Therefore we have to prove only
that such a factorization exists for r. We can (and will) assume that in fact
i.e., det L(-) 6j 0.
Apply Theorem 2.2 with \Gamma1. Since the only irreducible
monic complex polynomial f satisfying is f(-, by Theorem 2.2 we
can assume (replacing L(-) by D(-)) that the elementary divisors of L(-) are -
times). Here k is necessarily even. Indeed, the property ensures that
det (const :)- k is an even function.
and application of Theorem 2.1 gives the desired
result. Suppose therefore that k ? 0. Using the Smith form of L(-) write
I n\Gammak
where E(-) and F (-) are n \Theta n matrix polynomials with constant non-zero determinants.
Replacing L(-) by assume that the first k columns (and,
by symmetry, also the first k rows) of L(-) are divisible by -. Thus:
where the matrix polynomials
respectively. Moreover, \GammaL 1 We claim that
were not invertible, then det
would be divisible by -, and consequently det L(- k det
would
be divisible by - k+1 , an impossibility. Now L 1 (0) is skew-symmetric, and therefore admits
a factorization
for some invertible matrix Q. Let
\Phi I
I n\Gammak
the summand
is repeated ktimes
. Then
for some matrix polynomial e
L(-) such that e
The only thing not immediate here is the claim that e
L(-) is indeed a polynomial. But the
only point in C where e
could conceivably have a pole is - We have
\Phi I n\Gammak
I n\Gammak
I n\Gammak
\Phi I n\Gammak
Clearly, -Z(-) is analytic at - and the coefficient of - \Gamma1 in the Laurent series of
Z(-) in a neighborhood of -
I n\Gammak
I n\Gammak
To finish the proof, it remains to apply Theorem 2.1 to Z(-).
Finally, we consider matrix polynomials L(-) having symmetry (6.1) with
Theorem 6.2. Let be an n \Theta n matrix polynomial over C, and let r be
the general rank of L(-). Then L(-) admits a factorization
for some m \Theta n matrix polynomial M(-) if and only if in case the
product of invariant polynomials of L(-) is a square of some complex polynomial (resp. is
not a square of any complex polynomial).
Proof. Again, we omit many details here. We will assume that r = n. If L(-) admits
factorization (6.5) with M(-) n \Theta n, then det so the product of
invariant polynomials of L(-) must be a square as well. This implies the "only if " part.
To prove the "if " part, first of all observe that it suffices to consider only the case
when det L(-) is the square of a polynomial (if it is not, replace L(-) by
where f(-) is a judiciously chosen scalar polynomial so that f(-) det L(-) is a square).
By Theorem 2.2 we may assume that all elementary divisors of L(-) are first degree
polynomials. Since det L(-) is a square, the number of elementary divisors
a of L(-) (where a 2 C is fixed) is even. As in the proof of Theorem 6.1, we
can further assume that
for some matrix polynomials L 1
respectively. Moreover, L 1 (a) is invertible and
symmetric, and therefore
for some invertible matrix q (the direct summand
is repeated here k
times). As
in the proof of Theorem 6.1, we verify that
defined by (6.4), and e
L(-) is a matrix polynomial such that e
and e
L(-) has no elementary divisors of the form - \Gamma a. Apply the above procedure to e
in place of L(-), using elementary divisors
L(-) for some b 2 C, and so
on, until a matrix polynomial L 1
Now apply Theorem 2.1 to get the desired factorization of L \Gamma 1(-).
Theorems 6.1 and 6.2 can be recast in terms of elementary matrices (analogously to
Theorem 3.2). An n \Theta n matrix polynomial M(-) (over C) is called "-elementary if
for some 1 \Theta n row polynomial x(-) 6= 0 (here
Theorem 6.3. Let L(-) be an n\Thetan matrix polynomial satisfying (6.1), and let r be the
general rank of L(-). Then L(-) can be written as sum of r "-elementary matrices, unless
and the product of elementary divisors of L(-) is not a square of any polynomial.
In this latter case L(-) can be written as sum of r 1-elementary matrices, and cannot
be represented as sum of any r 1-elementary matrices.



--R


Linear Systems
Spectral analysis of selfadjoint matrix polynomials
Factorization of selfadjoint matrix polynomials with constant signature
Matrix Polynomials
Topics in Matrix Analysis

Factorization of symmetric matrices with elements from a ring with involution I
Factorization of symmetric matrices with elements from a ring with involution II

Perturbation Theory for Eigenvalue Problems
--TR
