--T
Estimation of Generalized Multisensor Hidden Markov Chains and Unsupervised Image Segmentation.
--A
AbstractThis paper attacks the problem of generalized multisensor mixture estimation. A distribution mixture is said to be generalized when the exact nature of components is not known, but each of them belongs to a finite known set of families of distributions. Estimating such a mixture entails a supplementary difficulty: One must label, for each class and each sensor, the exact nature of the corresponding distribution. Such generalized mixtures have been studied assuming that the components lie in the Pearson system. Adaptations of classical algorithms, such as Expectation-Maximization, Stochastic Expectation-Maximization, or Iterative Conditional Estimation, can then be used to estimate such mixtures in the context of independent identically distributed data and hidden Markov random fields. We propose a more general procedure with applications to estimating generalized multisensor hidden Markov chains. Our proposed method is applied to the problem of unsupervised image segmentation. The method proposed allows one to: (i) identify the conditional distribution for each class and each sensor, (ii) estimate the unknown parameters in this distribution, (iii) estimate priors, and (iv) estimate the "true" class image.
--B
Introduction
Hidden Markov chains are a useful tool for tackling numerous
concrete problems, for instance in speech processing
[27], communications [15], and image processing [1],
[8], [25]. These all fall in the framework of estimating some
discrete phenomenon from observed noisy data. The noise
is often modelled as Gaussian, but in many applications,
such as radar, sonar, ultrasound, infrared or magnetic resonance
images, the noise is not necessarily Gaussian [15],
[18]. Furthermore, for a given sensor and a given class,
the nature of the noise distribution can vary with time.
For example, the form of the grey level of the sea surface
in radar images can vary with the weather [6]. Thus, it
may be desirable to determine automatically the correct
noise distribution for each class and each sensor at a
given time. Early algorithms treating this problem were
proposed in [6] and [26] and applied to unsupervised image
segmentation. They combine mixture estimation algorithms
such as Expectation-Maximization (EM) [7], [28],
Stochastic Expectation-Maximization (SEM) [20], [22], or
Iterative Conditional Estimation (ICE) [23], [24], with the
recognition of the form of a distribution in the Pearson
system, assuming that a given sample is generated from a
unique distribution.
The present paper lies within the scope of this general prob-
lem. We first propose a multisensor generalized mixture
estimation method based on ICE and valid in a general
hidden data context. We then tailor our method to generalized
multisensor hidden Markov chain estimation. The
effectiveness of the proposed method is validated by simulations

The algorithms are then applied to the problem of unsupervised
image segmentation. In image segmentation, statistical
methods are based on random field models: for the
set of pixels S, we consider two sets of random variables
called "random fields". Each
takes its values in a finite set of
and each Y s takes its values in IR. The segmentation problem
is then to estimate the unobserved realisation
the field X from the observed realisationY = y of the field
Y , where is the observed image. There are t-
wo families of methods: global methods, which use hidden
Markov field models [2], [4], [5], [9], [11], [16], [19], [31], [32],
and local methods, in which each pixel is classified from observations
of a local neighbourhood [13], [20], [21], [22], [30].
The efficiency of the global methods is striking in many cas-
es, although local methods still remain of interest [3]. We
proposed in [1] a third method, which uses hidden Markov
chains instead of hidden Markov fields, with transformation
of the two-dimensional set of pixels to a one-dimensional
set using Hilbert-Peano scans [29]. The results are comparable
to those obtained with hidden Markov field based
methods, with the added benefit of being sufficiently versatile
to treat spatio-temporal unsupervised segmentation
problems. The use of Markov chains instead of Markov
fields also affords computational advantages. Indeed, several
relevant distributions can be calculated analytically in
the case of the Markov chains, whereas iterative estimation
procedures, like the Gibbs sampler, would be required
using Markov fields. Parameter estimation and restoration
algorithms [1], [10] are thus faster in the Markov chain case.
Furthermore, the use of the EM algorithm is complicated
in the hidden Markov field context and modifications may
be required [4], [32].
The paper is organized as follows. In the next section we
address the generalized mixture estimation problem in a
broad setting, in order to present our ICE-based method.
Section III describes the hidden Markov chain model. In
Section IV we present some particular methods and simulation
2results in the case of monosensor hidden Markov
chains, with the multisensor case treated in Section V. Applications
to the problem of unsupervised statistical image
segmentation are described in Section VI, while Section VII
contains the conclusions.
II. Generalized mixture estimation
Let us consider a finite set S and random variables
We consider first the
monosensor case. Thus each X s takes its values
in\Omega and
each Y s takes its values in IR. The distribution of X depends
on a parameter ff and is denoted by - ff . The random
variables (Y s ) s2S are independent conditionally on X and
the distribution of each Y s conditional on X is equal to its
distribution conditional on X s . Thus all distributions of
Y conditional on X are determined by the k distributions
of Y s conditional on each of respectively, specified
by densities f with respect to the Lebesgue
measure. The problem of mixture estimation is to find
ff and f y. In the "classical" mixture
case the general forms of f are known and they depend
on some parameter fi which is to be estimated from
y. For instance, if f are Gaussian, fi contains
k means and k variances. In the "generalized" mixture
case, by contrast, the general form of f k is not
known exactly; however, the form of each f j is assumed to
belong to a given finite set of forms. To be more precise,
be a set of families of distributions.
For instance, F 1 may be Gaussian distributions, F 2 Gamma
distributions, and so on. Then each f j belongs to one
of the families F but we do not know which. The
problem of determining of f is then two-fold: for
each f j find the family F i to which f j belongs, and then
find the parameter which fixes f j in F i .
We propose a general algorithm, called ICE-GEMI (GEMI
for generalized mixture), to solve such problems, based on
the following assumptions.
ff(X) of ff from X is available.
may simulate realizations of X according to
its distribution conditional on Y .
Each family F j of is characterized
by a parameter fi j , i.e., F In
practice B j is a subset of IR n j with n j depending on
are Gaussian.
fi M are available such that if
a sample z generated by a distribution
rule D is available, such that, for any
sample
, the rule D associates with z the "best suited"
density among f according to some criterion

We call our method ICE-GEMI as it can be seen as a generalization
of ICE, which in turn is a general method for
estimating hidden models and, in particular, for estimating
classical mixtures [23], [24].
The ICE-GEMI algorithm is an iterative method: at step
q, let ff q and f q
k be current prior parameters and
current densities f . The updating is as follows.
(a) Simulate x q , a realization of X, according to its dis-
tribution, conditional on and based on ff q and
k .
(b) Calculate ff
denotes the conditional expectation given
ff q and (f
k ). If this calculation
is impossible, calculate ff
(c) For
g.
Let y q
. For each estimate the
parameters fi 1
(d) For
g.
by putting (f q+1
(D(y q
In the multisensor case each Y s takes its values in IR m .
s ) . We shall assume
s are independent
conditionally on X s .
This means that for a given class the observations in different
sensors are independent. Thus each f i of densities
are densities on IR m , is given by m densities
i on IR:
Note that in many practical situations probably
too strong an assertion. Taking into account the stochastic
dependence among sensors, however, is non trivial in the
context of our model and further study would be necessary
to solve this problem.
The ICE-GEMI algorithm in the multisensor case differs
little from the ICE-GEMI algorithm in the monosensor
case. Roughly speaking, all formulas of the model stay
valid, except that f i (y s ) is replaced by f i (y 1
Thus steps (a) and (b) of the ICE-GEMI described above
remain. The steps (c), (d), and (e) differ slightly, as follows.
They become:
(c) For
g.
Let y q
and y q;r
(y r
. For each sensor and each
class parameters fi 1;r
(d) Apply (d) above to each sensor, giving
(e) For each class
and update (f
(f q+1
Once the sets S q
are known, the
workload for m sensors is m times that of the monosensor
case, resulting in m monosensor updates followed by the m-
sensor update using the product of monosensor functions
so obtained. We should remark that this applies only to the
noise parameter updates, and that the complete m-sensor
estimation algorithm cannot be reduced to the use of the
monosensor algorithm m times; the latter procedure would
give, in particular, m different priors.
III. Hidden Markov Chains
We present here a brief review of the hidden Markov
chain model; for the sake of simplicity, we describe it in
the monosensor case, the generalization to the multisensor
case being immediate.
We shall assume that (Xn ) n2IN   is a Markov chain, with
each g, and with stationary transition
probabilities. We assume that
does not depend on n. Thus the initial distribution is given
by
and the transition matrix has entries
The define, by
virtue of Kolmogorov's theorem, the distribution of X.
We assume that the conditional structure of the
as described at the
beginning of Section II.
We will denote (X
their realizations by
Let
and
the so-called forward and backward probabilities, which
can be calculated by the following forward and backward
recursions:
Initialization
Also, let
Thus
indicating that \Psi t (i; are computable using forward
and backward procedures. For the multisensor hidden
Markov chain model, all formulas stay valid, except that
replaced by f i (y 1
IV. Estimation of Generalized Hidden Markov
Chains
A. ICE-GEMI algorithm
We consider first the monosensor case, and verify the
assumptions
parameter ff is given here by (2):
use the empirical frequencies as estimators

It can be shown that, conditional on y, X is
a nonstationary Markov chain with transition matrix
at time t given by
a t
are defined in (9) and (10).
Thus X can be simulated.
There are numerous standard families
verifying these conditions.
be a sample and
Gn be the
empirical cumulative distribution function -
Gn (t) =n
GM be the cumulative
distribution functions associated with the distributions
. Then one can base the choice of
an f j on any measure of distance between -
Gn and the
G j . For instance, let K j
which is the Kolmogorov distance between -
Gn and G j ,
and define D by
are verified, we may apply the corresponding
ICE-GEMI algorithm by following (a)-(e) of
Section II. Note that in step (b) the calculation of
In fact, for each
ij we have
c q+1
which results from (9) and (11).
The multisensor case is analogous, just replacing densities
on IR by densities on IR m .
In the sequel we denote by ICE-KOLM the ICE-GEMI
based on the minimization of the Kolmogorov distance
above.
B. ICE-PEAR algorithm
The algorithms proposed in our earlier work in [6], [26]
use the Pearson family, which contains eight subfamilies.
The first four moments of Pearson a distribution determine
both the subfamily to which it belongs and the values of the
parameters [17]. Methods in [26] were based on the SEM
algorithm, which is a stochastic variant of the EM algorithm
that can be used to estimate classical mixtures [20].
ICE-based procedures, which we call ICE-PEAR here, were
later proposed in [6], with applications to independently identically
distributed samples and hidden Markov random
fields. For the model considered in this paper, ICE-PEAR
is slightly different. It resembles ICE-GEMI save for steps
(c) and (d) which become the following:
(c*) For
g.
Calculate the four first moments from y k;q
and decide in which family F j(i) 2 fF the
distribution lies. Calculate fi j(i)
i from the four first
moments and set fi q+1
C. ICE-KOLM, ICE-PEAR, and ICE-GAUSS: numerical
results
We present some numerical results of unsupervised Maximum
Posterior Mode (MPM) restoration using our meth-
ods. We compare their efficiency, in cases involving non-Gaussian
noise, with a classical method that assumes that
the noise is Gaussian. The classical method used is ICE-
based MPM, so that, when the noise is Gaussian, both
approaches are the same theoretically. We also compare
ICE-KOLM and ICE-PEAR.
The procedure for the MPM restoration is as follows:
(i) for each t 2 ng and i
(ii) for each t 2 ng let ! t be a class maximizing
with respect to i
Although we have chosen the MPM algorithm, the so-called
MaximumA Posteriori (MAP) algorithm could also be ap-
plied. MAP is based on the principle
With the analytic solution given by the Viterbi algorithm
[10]. The hidden Markov chain model enjoys a computational
advantage over the hidden Markov field model when
MPM is used at the restoration step, and this is even more
pronounced when using MAP. With the hidden Markov
field model, one must resort to simulated annealing algorithm
[11], which can be very time consuming. Note that
Iterated Conditional Mode (ICM [2]) can be used as a fast
approximation to MAP, but then the convergence is not
ensured and the segmentation result depends strongly on
the initialisation.
To initialize the three methods we take c 0
, and c 0
kg. We
calculate the empirical mean - and the empirical variance
oe 2 of the sample, and all noise distributions are assumed
Gaussian with variances equal to oe 2 , and means distributed
around - and distanced oe
2 from each other.
We take distributions
of the first kind [14], F 2 denotes gamma distributions
[14], and F 3 denotes Gaussian distributions, along
with the Markov chain defined by (c
(0:49; 0:01; 0:01; 0:49). The common marginal distribution
is and the transition matrix is
. Observe that the initialization we
use gives
22
is far from the true values. However, the results below show
that this poor initialization has little effect on the efficiency
of the different methods. The chain is corrupted by two
noise sequences (Noise 2, Noise 3), whose distributions are
presented in Figure 1. The noises Noise 1 and Noise 4 are
used in sections VI-B and VI-D. Their influence is measured
by the theoretical blind error rates - blind , which are
theoretical Bayes error rates when using the true parameters
and estimating each x t from y t , i.e., without reference
to any context.
In order to generate different noises used we apply methods
described in [17]. Table I and other results presented
in [12] lead to the following remarks:
1. The noises used are rather strong. Comparing - blind
to different - MPM s obtained with real parameters, one
can note the advantage in using the Markovian model
considered.
2. ICE-KOLM is quite efficient in recognizing the nature
of the mixture components, while ICE-PEAR can encounter
some difficulties.
3. Once the correct components are determined, the
noise parameters appear correctly estimated.
4. The restoration results obtained from generalized
mixture estimation are always better than those obtained
from Gaussian mixture estimation.
5. The error rates of MPM based on the ICE-KOLM estimates
are often very close to the error rates of MPM
based on the true parameters, which attests to the
stability of the whole procedure.
V. Multisensor generalized hidden Markov
chains
The restoration of multisensor hidden Markov chains is
of interest in many situations. For example, in image pro-
cessing, sensors can be of different natures, such as radar,
infrared, optical, or ultrasound, and thus the distributions
corresponding to a given class can have different natures.
Also, the nature of the noise can vary with the class, with
the sensor for a given class and, for a given class and sensor,
with time. We briefly present in this section some numerical
results comparing ICE-KOLM and ICE-GAUS in the
multisensor case. Table II and additional results presented
in [12] lead to the following remarks about two-sensor
experiments:
1. Again, results obtained from generalized mixture estimation
are better than those obtained from Gaussian
mixture estimation.
2. The error rates of MPM based on the ICE-KOLM estimates
are still close to the error rates of MPM based
on the true parameters; however, this is less striking
than in the monosensor case. Stability of the whole
procedure is retained. When there are more than two
sensors, the problem of choosing a maximum number
of "useful" sensors could arise.
VI. Unsupervised generalized image
segmentation
We present in this section some applications of generalized
mixture estimation, with ICE-KOLM and ICE-
PEAR, to unsupervised image segmentation. As we use
the Markov hidden chain model, all results of the previous
sections apply almost immediately. To transform a set
of pixels into a support set for a hidden Markov chain,
we proposed in [1] the use of the Hilbert-Peano scan and
showed that results obtained, in the case of Gaussian noise,
were better than those obtained using classical raster scan-
s. The model is intuitively less satisfying than the hidden
Markov field model, but several simulations performed in
the Gaussian case reveal that it is competitive [1].
A. Monosensor image segmentation
Let S be the set of pixels, and s
according to the Hilbert-Peano curve the first three
stages of whose construction in S is presented in Figure
2, starting with a four pixel image and at each step multiplying
the image by four. Continuation of this sequence
creates the Hilbert-Peano scan on an image of size 2 n \Theta 2 n ,
for any n. Other Hilbert-Peano scans can be defined on
images of any size [29].
is the random field of classes and
is the observed field, i.e., y is the digital
image to be segmented. Considering (X
making the usual assumptions
about that
problem of image segmentation is now one of generalized
hidden Markov chain restoration.
B. Synthetic images
Consider the family of Section IV, and
two synthetic images: "Letter B" and "Ring". Each is corrupted
by Noise 3 and Noise 4 of Section IV-C.
As mentioned above (Section IV-C, Table I), the noise perturbations
are quite sizeable: for priors equal to 0:5, the
blind classification error rates are 31.3% and 27.5%, re-
spectively. Images, their noisy versions with Noise 4, and
segmentation results are presented in Figure 3. Tables
III and IV display the families estimated by ICE-KOLM
and ICE-PEAR and the error rates given by MPM based
on the true distributions, ICE-GAUS-MPM, ICE-PEAR-
MPM, and ICE-KOLM-MPM. We also present the results
obtained with an unsupervised hidden Markov-field based
algorithm, called ICE-FIELD, in which the parameter-estimation
step is done by ICE and the segmentation step
with MPM. Note that we use the Ising model, which is the
simplest one, and more complex models could produce better
results. Note that ICE-KOLM is always more efficient
than ICE-PEAR. This advantage is generally slight, but
can also be pronounced, as in the Letter B+Noise 3 case.
On the other hand, both ICE-KOLM and ICE-PEAR perform
better than ICE-GAUS. In some cases, such as Letter
B+Noise 4 and Ring+Noise 3, ICE-GAUS gives very poor
results and the advantage of the generalized mixture based
methods is quite apparent. Otherwise, these two simple examples
show that we have to be very careful in comparing
our method with the classical methods based on hidden
Markov fields with Gaussian noises. The former clearly
gives better results in the case of Letter B, but the second
takes the upper hand in the case of Ring. As the noise is
the same in both cases, this reveals that the structure of
images, which determines the prior distribution, plays an
important role.
C. Real images
We present now the results of different segmentations
of three real images. Our purpose is two-fold; on the
one hand we show that ICE-KOLM yields better results
than ICE-GAUS, and on the other hand, we compare
the efficiency of ICE-KOLM with that of ICE-FIELD. We
do not address the important problem of estimating the
number of classes in this paper, although we refer to [20]
for a procedure, proposed using estimating classical mixtures
with SEM, that can be quite useful for fixed images.
According to the different visual impressions, we can say
that no clear general tendency appears and the hierarchy
of efficiencies of the methods considered is subject to each
particular image. Concerning the image Clouds, Figure 4,
the efficiency of the three methods seems comparable. As
nearly all distributions detected by ICE-KOLM are nor-
mal, the equivalence between ICE-KOLM and ICE-GAUS
is not surprising. The fact that the results obtained with
both ICE-KOLM and ICE-GAUS are comparable to results
obtained with ICE-FIELD indicates that they are
competitive and should be used in images of this kind because
of their speed. In "San Francisco" image, Figure 5,
ICE-GAUS gives clearly better results than ICE-FIELD,
which reflects how hidden Markov chains can be suitable
even outside the generalized mixture considerations. This
could be due to the fact, as noted in [3], that the hidden
Markov field based methods can encounter difficulties in
detecting very fine details and the hidden Markov chain
based methods seem to be better suited to such situation-
s. Furthermore, ICE-KOLM-MPM is clearly more effective
than ICE-GAUS-MPM. This is undoubtedly due to the fact
that ICE-KOLM detects beta and gamma distributions
and leads us to two conclusions. First, noise of different
forms associated with different classes can exist in a same
image. Second, their detection with ICE-KOLM can improve
the final unsupervised MPM segmentation results.
Comparison of computer times is not very reliable because
our computer programs are not optimized and, what is
more, the time used by every method is subject to different
numbers of iterations subjectively chosen. Working with
Ultra Spark, Enterprise 2 and in the case of "San Francis-
co", which is of 256 \Theta 256 size, the respective times for ICE-
GAUS-MPM, ICE-FIELD-MPM, and ICE-KOLM-MPM
are about 1 min, 15 min, and 50 min. This shows that
ICE-GAUS-MPM is very interesting with respect to ICE-
FIELD-MPM and that ICE-KOLM-MPM is rather time
expensive. However, in contrast to ICE-GAUS-MPM and
ICE-FIELD-MPM, the time of ICE-KOLM-MPM could
undoubtedly be reduced. In particular, we use the whole
subsets S q
i , which maybe is a little superfluous and the
use, at each iteration, of subsets of S q
could speed up the
whole procedure significantly. In the case of the "Amazo-
nia" image, Figure 6, the result obtained with ICE-FIELD-
MPM seems visually better than that obtained with ICE-
GAUS-MPM, which suggests that the Markov field model
is more appropriate than the Markov chain model. Other-
wise, ICE-KOLM detects three different forms of noise dis-
tributions, which actually makes ICE-KOLM-MPM more
effective than ICE-GAUS-MPM and shows again the advantage
of generalized mixture estimation. The comparison
between ICE-FIELD-MPM and ICE-KOLM-MPM is diffi-
cult; the latter seems to restore fine details better, but it is
difficult to see in the real image whether such details exist.
D. Synthetic multisensor images segmentation
Multisensor ICE-GAUS, ICE-KOLM based segmentations
of noisy Letter B and the Ring are presented in Table
V. These results allow conclusions analogous to those
in Section V. ICE-KOLM recognizes the correct families,
and the efficiency of the corresponding unsupervised segmentation
is close to that of the method based on the true
parameters. Visual results of segmentations are presented
in

Figure

7.
Real multisensor image segmentation
We consider in this section unsupervised segmentation
of a real multisensor radar image of the area surrounding
the town of Sunbury, Pennsylvania. The colours in the image
are assigned to different frequencies and polarizations
of the SIR-C radar. We only consider two sensors which
seem complementary by their different visual aspect, as presented
in Figure 8. We present the results of two-sensor
ICE-GAUS and ICE-KOLM based MPM segmentations in
2, 3, and 4 classes in Figure 9. Rational comparison of the
results is difficult in the absence of clear knowledge of the
ground truth. However, note that the nature of the components
varies with the class, and the ICE-KOLM based
MPM segmentation seems richer.
VII. Discussion
We addressed in this work the problem of estimating generalized
multisensor mixtures with applications to generalized
multisensor hidden Markov chains and, more specifi-
cally, to unsupervised statistical image segmentation prob-
lems. The contribution of this paper rests, with respect to
earlier works [6], [26], on three points:
1. A more general family, based on Iterative Conditional
Estimation (ICE), of generalized mixture estimation
methods is proposed. In particular, this family
is valid in the i.i.d. case, hidden Markov fields, hidden
Markov chains, and any family
provided each F i is parametrized and can be estimated
separately.
2. The effectiveness of a particular method based on ICE
and Kolmogorov distance in the frame of generalized
multisensor hidden Markov chains is shown using simulations

3. Applications to unsupervised image segmentation
generalize results obtained in the Gaussian monosen-
case [1].
Concerning the results obtained using generalized hidden
Markov chains, we may state, as a general conclusion, that
component recognition and parameter estimation are quite
efficient. However in the multisensor case further investigations
would be desirable in order to choose judiciously
which sensors among those available should be used.
Concerning the unsupervised image segmentation, the results
presented allow us to put forth two conclusions. First,
the form of the noise can indeed change with the class in
real images, which renders the proposed techniques more
effective than classical methods which assume the same form
of noise for all classes. Second, little can be said about
the relative value of the proposed method and classical hidden
Markov field based approaches in the general case, but
when inhomogeneous zones, like urban areas, are present
in the image the Markov chain based methods can be more
appropriate than the Markov field ones.
We close with directions for future work. As concerns the
proposed general method, it would be desirable to introduce
nonparametric components and study how they are
detected and estimated with ICE-GEMI. Another direction
is the unsupervised generalized segmentation of sequences
of multisensor images, or even 3D multisensor images. The
flexibility of the proposed model renders it well suited to
such tasks: it suffices to define a Hilbert-Peano scan in
a three-dimensional set of pixels. Preliminary results of
spatio-temporal segmentation of mono-sensor Gaussian im-
ages, presented in [1], are encouraging, although the important
problem of estimating the number of classes remains.
Devising a reliable method for estimating the number of
classes remains of definite interest.
The authors thank Professor Titterington, Associated Edi-
tor, and the anonymous reviewers for their help in improving
the readability of the paper. This work was supported by
the Convention CNET-INT 93 PE 74 05. The authors also
thank Bruno Choquet and Danielle Pele, CCETT Rennes,
for their collaboration. The image "Amazonia" was provided
by CIRAD SA under the project GDR ISIS.



--R

Estimation des param'etres dans les cha
On the statistical analysis of dirty pictures.
Global and local methods of unsupervised Bayesian segmentation of images.
An iterative gibbsian technique for reconstruction of m-ary images
Markov random fields.
Estimation of generalized mixture and unsupervised statistical radar image segmentation.
Maximum likelihood from incomplete data via the EM algorithm.
Hidden Markov mesh random field models in image analysis.
Random field models in image anal- ysis
The Viterbi algorithm.
Stochastic relaxation
Segmentation non supervis'ee d'images multispec- trales par cha-ines de Markov cach'ees. PhD thesis, Universit'e de Technologie de Compi'egne
A context classifier.
Distributions in Statistics: Continuous Univariate Distributions
Joint parameter estimation and symbol detection for linear or nonlinear uknown channels.
Simultaneous parameter estimation and segmentation of Gibbs random fields.
Simulation Modelling and Anal- ysis
The modified Beta density function as a model for synthetic aperture radar clutter statistics.
Probabilistic solution of ill-posed problems in computational vision
SEM algorithm and unsupervised segmentation of satellite images.
A simulation study of some contextual clasification methods for remotely sensed data.
Adaptive Mixture Estimation and Unsupervised Local Bayesian Image Segmentation.
Mixture of distributions
Statistical image segmentation.
On the use of Gibbs Markov chain models in the analysis of images based on second-order pairwise interactive distributions
Unsupervised Bayesian segmentation of SAR images using the Pearson sys- tem
A tutorial on hidden Markov models and selected applications in speech recognition.
Mixture densities
Generalized Hilbert scan in image printing.
Estimation of context for statistical classification of multispectral image data.
Parametric inference for imperfectly observed Gibbsian fields.
The mean field theory in EM procedures for blind Markov random field image restoration.
--TR

--CTR
Yihua Yu , Qiansheng Cheng, MRF parameter estimation by an accelerated method, Pattern Recognition Letters, v.24 n.9-10, p.1251-1259, 01 June
J.-N. Provost , C. Collet , P. Rostaing , P. Prez , P. Bouthemy, Hierarchical Markovian segmentation of multispectral images for the reconstruction of water depth maps, Computer Vision and Image Understanding, v.93 n.2, p.155-174, February 2004
Greg Breinholt , Christoph Schierz, Algorithm 781: generating Hilbert's space-filling curve by recursion, ACM Transactions on Mathematical Software (TOMS), v.24 n.2, p.184-189, June 1998
Nicolas Brunel , Wojciech Pieczynski, Unsupervised signal restoration using hidden Markov chains with copulas, Signal Processing, v.85 n.12, p.2304-2315, December 2005
Wojciech Pieczynski, Pairwise Markov Chains, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.25 n.5, p.634-639, May
Franois Destrempes , Max Mignotte, A Statistical Model for Contours in Images, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.26 n.5, p.626-638, May 2004
Stphane Derrode , Grgoire Mercier, Unsupervised multiscale oil slick segmentation from SAR images using a vector HMC model, Pattern Recognition, v.40 n.3, p.1135-1147, March, 2007
