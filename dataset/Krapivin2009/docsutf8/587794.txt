--T
Data Fitting Problems with Bounded Uncertainties in the Data.
--A
An analysis of a class of data fitting problems, where the data uncertainties are subject to known bounds, is given in a very general setting. It is shown how such problems can be posed in a computationally convenient form, and the connection with other more conventional data fitting problems is examined. The problems have attracted interest so far in the special case when the underlying norm is the least squares norm. Here the special structure can be exploited to computational advantage, and we include some observations which contribute to algorithmic development for this particular case. We also consider some variants of the main problems and show how these too can be posed in a form which facilitates their numerical solution.
--B
Introduction
. Let A 2 R mn arise from observed data, and for
Then a conventional tting problem is to minimize krk over x 2 R n , where the norm
is some norm on R m . This involves an assumption that A is exact, and all the errors
are in b, which may not be the case in many practical situations; the eect of errors in
A as well as b has been recognized and studied for many years, mainly in the statistics
literature. One way to take the more general case into account is to solve the problem
subject to
where the matrix norm is one on (m(n+1)) matrices. This problem, when the matrix
norm is the Frobenius norm, was rst analyzed by Golub and Van Loan [10], who used
the term total least squares and developed an algorithm based on the singular value
decomposition of [A : b]. Since then, the problem has attracted considerable attention:
see, for example, [18], [19].
While the formulation (1.1) is often satisfactory, it can lead to a solution in which
the perturbations E or d are quite large. However, it may be the case that, for exam-
ple, A is known to be nearly exact, and the resulting correction to A may therefore be
excessive. In particular, if bounds are known for the size of the perturbations, then
it makes sense to incorporate these into the problem formulation, and this means
that the equality constraints in (1.1) should be relaxed and satised only approxi-
mately. These observations have motivated new parameter estimation formulations
where both A and b are subject to errors, but in addition, the quantities E and d are
bounded, having known bounds. This idea gives rise to a number of dierent, but
Received by the editors May 25, 1999; accepted for publication (in revised form) by L. El Ghaoui
December 5, 2000; published electronically April 6, 2001.
http://www.siam.org/journals/simax/22-4/35659.html
y Department of Mathematics, University of Dundee, Dundee DD1 4HN, Scotland (gawatson@
maths.dundee.ac.uk).
closely related, problems and algorithms and analysis for problems of this type based
on least squares norms are given, for example, in [1], [2], [3], [4], [5], [8], [9], [15], [17].
The general problem (1.1) is amenable to analysis and algorithmic development
for a wide class of matrix norms, known as separable norms, a concept introduced by
Osborne and Watson [13]. The main purpose of this paper is to show how problems
with bounded uncertainties also can be considered in this more general setting. In
particular, it is shown how such problems can be posed in a more computationally
convenient form. As well as facilitating their numerical solution, this enables connections
with conventional data tting problems to be readily established. Motivation
for extending these ideas beyond the familiar least squares setting is provided by the
important role which other norms can play in more conventional data tting contexts.
We continue this introductory section by dening separable norms and by introducing
some other necessary notation and tools. We rst introduce the concept of the
dual norm. Let k:k be a norm on R m . Then for any v 2 R m , the dual norm is the
norm on R m dened by
r T v:
The relationship between a norm on R m and its dual is symmetric, so that for any
r
r T v:
Definition 1.1. A matrix norm on m  n matrices is said to be separable if
given vectors u there are vector norms k:k A on R m and k:k B on
R n such that
Most commonly occurring matrix norms (operator norms, orthogonally invariant
norms, norms based on an l p vector norm on the elements of the matrix treated as an
extended vector in R mn ) are separable. A result which holds for separable norms
and will be subsequently useful is that
see, for example, [13] or [20].
Another valuable tool is the subdierential of a vector norm, which extends the
idea of the derivative to the nondierentiable case. A useful characterization of the
subdierential (for k:k A ) is as follows.
Definition 1.2. The subdierential or set of subgradients of krkA is the set
A  1g:
If the norm is dierentiable at r, then the subdierential is just the unique vector
of partial derivatives of the norm with respect to the components of r.
The main emphasis of this paper is on problems which address the eects of worst
case perturbations. This gives rise to problems of min-max type. In section 2, we
consider problems where separate bounds on kEk and kdkA are assumed known, and
in section 3, we consider a similar problem except that a single bound on kE : dk is
given. In both cases, the matrix norm is assumed to be separable. In section 4, some
variants of the original problems are considered, and nally, in section 5 we consider
a related class of problems which are of min-min rather than min-max type.
1276 G. A. WATSON
2. Known bounds on kEk and kdkA . Suppose that the underlying problem
is such that we know bounds on the uncertainties in A and b so that
where the matrix norm is a separable norm, as in Denition 1.1. Then instead of
forcing the equality constraints of (1.1) to be satised, we wish to satisfy them approximately
by minimizing the A-norm of the dierence between the left- and right-hand
sides, over all perturbations satisfying the bounds. This leads to the problem
Therefore x minimizes the worst case residual, and this can be interpreted as permitting
a more robust solution to be obtained to the underlying data tting problem:
for an explanation of the signicance of the term robustness in this context, in the
least squares case, see, for example, [9], where a minimizing x is referred to as a robust
least squares solution. Another interpretation of the problem being solved is that it
guarantees that the eect of the uncertainties in the data will never be overestimated,
beyond the assumptions made by knowledge of the bounds.
We now show that (2.1) can be restated in a much simpler form as an unconstrained
problem in x alone.
Theorem 2.1. For any x, the maximum in (2:1) is attained when
where
otherwise u is arbitrary but 1. The maximum
value is
Proof. We have for any E; d such that kEk  , kdkA   d ,
Now let E and d be as in the statement of the theorem. Then
and further
The result follows.
An immediate consequence of this result is that the problem (2.1) is solved by
minimizing with respect to x
kAx bk A
and it is therefore appropriate to analyze this problem. In particular, we give conditions
for x to be a solution and also conditions for that solution to be
results are a consequence of standard convex analysis, as is found, for example, in
[14].
Theorem 2.2. The function (2:2) is minimized at x if and only if there exists
@kxkB such that
Theorem 2.3. Let there exist v 2 @kbkA so that
Proof. For to give a minimum we must have v 2 @kbkA so that (2.3) is
satised with kwk
1. The result follows.
2.1. Connections with least norm problems. We will next establish some
connections between solutions to (2.2) and solutions to traditional minimum norm
data tting problems. In [9], coincidence of solutions in the least squares case is said
to mean that the usual least squares solution may be considered to be robust.
Consider the least norm problem
minimize kAx bk A :
Then x is a solution if and only if there exists v 2 @kAx bk A such that
solves (2.4), then clearly it also solves (2.2). (Note that we can take
(2.3).) Otherwise, if k:k A is smooth, solutions to (2.2) and to (2.4) can coincide only
is unique). In this case, let b be any solution
to denote the minimum A-norm solution to A T
Then if x minimizes (2.2), (2.3) is satised with w 2 @kxkB and kvk
A  1, otherwise
v is unrestricted. Because
it follows that we must have
A
In other words, A is also a solution to (2.2) only if b 2 range(A) and
A
For example, if both norms are least squares norms, then this condition is
Note that if b is the minimum B-norm solution to immediately
solves (2.2), and so there must exist w such that this inequality is satised
independently of .
1278 G. A. WATSON
The case when k:k A is nonsmooth is more complicated.
Example 2.1. Let
corresponds to the separable norm being the sum of moduli of the components.) Then
(2:4) is solved by any x; 1  x  2. Further a solution to (2:2) provided that
2.
To summarize, we can augment Theorem 2.3 by the following, which can be
interpreted as a generalization of a result of [9].
Theorem 2.4. If b 2 range(A) and any solution to
provided that
A
We can also prove the following, which connects Theorems 2.3 and 2.4.
Theorem 2.5. Let b 2 range(A), and
A
min
Proof. Let otherwise arbitrary. It follows by
denition of A + and
A T
Thus
and
Now
using (2.5) and (2.6). The result follows.
A consequence of the above results is that if b 2 range(A) and
A
min
then any point in the convex hull of f0; A + bg is a solution.
2.2. Methods of solution. From a practical point of view, it is obviously of
importance to e-ciently solve (2.1) (or, equivalently, (2.2)) in appropriate cases.
Let
Most commonly occurring norms are either smooth (typied by l p norms, 1 < p < 1)
or polyhedral (typied by the l 1 and l 1 norms). If the norms in the denition of f are
smooth, then derivative methods are natural ones to use. A reasonable assumption
in most practical situations is that
range(A), so that would then give the
only derivative discontinuity. If is not a solution, then f is dierentiable in a
neighborhood of the solution, and once inside that neighborhood, derivative methods
can be implemented in a straightforward manner. Theorem 2.3 tells us when
is a solution; the following theorem gives a way of identifying a descent direction at
that point in the event that it is not. It applies to arbitrary norms.
Theorem 2.6. Assume that
vk
B be such that
g. Then d is a descent direction for f at
Proof. Let the stated conditions be satised, and let d be dened as in the
statement of the theorem. By theorem 2.3, is not a solution. For d to be a
descent direction at the directional derivative of f at in the direction d
must be negative, that is,
arbitrary. Then
vk
< 0:
The result follows.
If k:k A is smooth, then ^ v is unique, and the construction of d using this result is
straightforward. If the norm on E is the norm given by
1=p
or
then f becomes
G. A. WATSON
In fact, the minimization of f for any p; q satisfying 1 <
can be readily achieved by derivative methods, using Theorem 2.6 to get
started. Indeed, it is normally the case that second derivatives exist and can easily
be calculated so that Newton's method (damped if necessary) can be used following
a step based on Theorem 2.6. The Hessian matrix of f is positive denite (because f
is convex), so that the Newton direction is a descent direction away from a minimum.
Some numerical results are given in [21].
For polyhedral norms (typied by l 1 and l 1 norms), the convex objective function
(2.2) is a piecewise linear function. Therefore, it may be posed as a linear programming
problem, and solved by appropriate methods.
Arguably, the most interesting case from a practical point of view is the special
case when both k:k A and k:k B are the least squares norm, so that
This case has particular features which greatly facilitate computation, and Chandrasekaran
et al. [1], [3] exploit these in a numerical method. In contrast to the
problems considered above, which involve a minimization problem in R n , special features
of the l 2 case can be exploited so that the problem reduces to one in R. When
(2.7) is dierentiable, (2.3) becomes
or
where
Let the singular value decomposition of A be
where U 2 R mm and V 2 R nn are orthogonal and ng is the
matrix of singular values in descending order of magnitude. Let
It will be assumed in what follows that A has rank
n, and further that is not a solution (which means, in particular, that b 1 6=
(which means that b 2 6= 0). From Theorem 2.3, we require that
<
Then it is shown in [3] that  satises the equation
where
This can be rearranged as
where
It is also shown in [3] that (2.8) is both necessary and su-cient for (2.10) to have
exactly one positive root   . In addition, G 0 (  ) > 0. Dierent methods can be used
for nding   in this case. One possibility which is suggested by (2.9) is the simple
iteration process
and it is of interest to investigate whether or not this is likely to be useful. It turns
out that this method is always locally convergent, as the following result shows.
Theorem 2.7. Let
<
Then (2:10) has exactly one positive root   and (2:11) is locally convergent to   .
Proof. Let  satisfy (2.12). Then (2.10) has a unique positive root   . Dieren-
tiating G() gives
and so
using G(  Now g() and G() are related by
and so
using g(
G. A. WATSON

Table
Simple iteration: stack loss data.
9
Substituting from (2.13) gives
0:
It follows using (2.15) and G 0 (  ) > 0 that
and the result is proved.
Indeed, simple iteration seems to be remarkably eective, and in problems tried,
it converged in a satisfactory way from obvious starting points.
For example, for the stack loss data set of Daniel and Wood [6]
4), performance for dierent values of  is shown in Table 1, where the iteration is
terminated when the new value of  diers from the previous one by less than 10 6 .
Another example is given by using the Iowa wheat data from Draper and Smith
9). The performance of simple iteration in this case is shown in

Table

2.
Although simple iteration is in some ways suggested by the above formulation, of
course higher order methods can readily be implemented, such as the secant method
or Newton's method. Actual performance will depend largely on factors such as the
nature and size of the problem and the relative goodness of starting points.
3. A known bound on kE : dk. Suppose now that the underlying problem is
such that we know upper bounds on the uncertainties in A and b, in the form
where  and the (separable) matrix norm are given. Consider the problem of deter-
mining

Table
Simple iteration: Iowa wheat data.
6 5.912536 12.925885 30.839779
9 12.927008 30.881951
14 30.882685
where the A-norm on R m is dened by the particular choice of separable norm (or vice
versa). This problem and variants have been considered, for example, by El Ghaoui
and Lebret [8], [9], where the matrix norm is the Frobenius norm, so that both the Aand
B-norms are least squares norms. Arguing as in Theorem 2.1 gives the following
result.
Theorem 3.1. For any x, the maximum in (3:1) is attained when
where
any vector with 1. The maximum value is
The problem (3.1) is therefore equivalent to the problem of minimizing with respect
to x
kAx bk A
Standard convex analysis then gives the following result.
Theorem 3.2. The function (3:4) is minimized at x if and only if there exists
where u 1 denotes the rst n components of u.
3.1. Connection with least norm problems. As before, it is of interest to
establish connections with the corresponding least norm problems. If
(2.4), then it will also minimize (3.4) for monotonic norms k:k B . (k:k B is a monotonic
norm on R n+1 if kck B  kdkB whenever jc does
not solve (2.4), then just as before when k:k A is smooth, solutions to this problem
and (3.1) cannot coincide unless b 2 range(A). In that case, as in section 2.1 let
a solution to denote the minimum
A-norm solution to A T For a solution to (3.4), there must exist v; kvk
(otherwise unrestricted) so that
1284 G. A. WATSON
where consists of the rst n components of u Therefore,
and so
A
In other words the solutions will coincide if b 2 range(A) and
A
Note that if k:k B is smooth, then u is unique. For example, when both norms are
least squares norms, this gives
as given in [9]. The situation when k:k A is not smooth is, of course, once again more
complicated. Consider again Example 2.1 where  > 0 is arbitrary. Recall that (2.4)
is solved by any x; 1  x  2: the unique solution to the problem of minimizing (3.4)
is
3.2. Connection with total approximation problems. The nature of the
bound in (3.1) means that there is a connection to be made with the total approximation
problem (1.1). It is known [13], [20] that a minimum value of (1.1) coincides
with the minimum of the problem
subject to
the smallest -generalized singular value of the matrix [A : b]. In particular, if the
vector norms are least squares norms, then this is just the smallest singular value of
at a minimum of (1.1) is obtained from a z = z T at a minimum
of (3.7) by scaling so that
z T
corresponds to nonexistence of a
solution to (1.1).) It is known also that a minimizing pair E; d is given by
and consider the problem
(3.
or equivalently,
so if k:k A is smooth, x T is a solution to
this problem provided that
A
as a consequence of the previous analysis. For example, when both norms are least
squares norms, this gives (see also [9])
For the least squares case, El Ghaoui and Lebret [8] suggest using robust methods
in conjunction with total approximation to identify an appropriate value of . The
idea is rst to solve the total approximation problem. Then (3.8) is constructed from
the total approximation solution and solved with  set to  T , the minimum value in
(3.7), that is,
Of course if  T does not exceed the right-hand side of (3.9), there is nothing to solve.
3.3. Methods of solution. For the special case of (3.4) when the norms k:k A
and k:k B are (possibly dierent) l p norms, we have
derivative methods may again be used. Let us again make
the (reasonable) assumption that there is no x which makes kAx bk so that
kAx bk p is dierentiable for all x. Then in contrast to the earlier problem, since
the second term cannot be identically zero, f is dierentiable for all x. We can easily
compute rst and second derivatives of f , and so Newton's method, for example,
can be implemented. A line search in the direction of the Newton step will always
guarantee descent, because f is convex, so eventually we must be able to take full
steps and get a second order convergence rate. Some numerical results are given in
[21]. For polyhedral norms occurring in (3.4), linear programming techniques may be
used.
Now consider the special case when 2. An analysis similar to that
given in section 2.2 can be given in this case, leading to a similar numerical method.
This particular problem is considered by El Ghaoui and Lebret [8], [9]. The main
emphasis of those papers is on structured perturbations, which is a harder problem,
and an exact solution to that problem is obtained. For the present case, the method
suggested is similar to that given for the problem of section 2 by Chandrasekaran
et al. in [1], [3].
Let A have singular value decomposition as before and have full rank. Assume
also that
range(A). Then optimality conditions are
1286 G. A. WATSON
or
where
It can be shown as before that  satises the equation
where
This can as before be rearranged as
where
with G() dened as in (2.10). It is easily seen that H() has at least one positive
root for any  > 0. As in [3], it may be shown that H() in fact has exactly one
positive root, ^
, with
H(^) > 0:
Note that here there is no restriction on  except that it should be positive. Consider
the simple iteration process
Theorem 3.3. The iteration scheme (3:11) is locally convergent to ^
.
Proof. We can rst show that
2:
We can then show that h() and H() are related by
h()
where
Thus
using
Substituting from (3.12) gives
It follows using (3.13) and H 0 (^) > 0 that
and the result is proved.
The performance of simple iteration in this case is, of course, similar to the same
method applied in the previous situation. Other methods like the secant method, or
Newton's method, are more complicated but can give potentially better performance.
4. Some modications. There are dierent ways in which additional information
may be incorporated into the problems of the last two sections, resulting in
appropriate modications of these problems. For example, some components of A or
b may be exact, in which case the corresponding components of E or d will be zero.
The bounds may take dierent forms and may be on submatrices of E rather than E
itself. Also the perturbation matrices may have known structure, which we want to
preserve. Examples of all these possibilities are considered in this section.
4.1. Exact columns and rows. Some problems are such that some of the
columns and possibly rows of A are known to be exact (see, for example, [3]). A
treatment can be given for both the problems of sections 2 and 3, and we will demonstrate
only for those of section 2; the appropriate requirements for the problems of
section 3 are obvious. We begin by considering the case when certain columns only
of A are known to be exact. In that case (following suitable reordering of columns if
necessary) the general problem is to minimize
and the (separable) matrix norm is one dened
on m  t matrices. We can partition x as x
arguing as in Theorem 2.1, we have the following.
Theorem 4.1. For any x, the maximum in (4:1) is attained when
where
otherwise u is arbitrary, but 1. The maximum value
is
1288 G. A. WATSON
Therefore, the problem is solved by minimizing with respect to x
kAx bk A
Now consider the case when some columns and rows of A are exact. This corresponds
to the requirement to perturb only a submatrix of A. Assume this to be the
lower right-hand s  t submatrix. An appropriate problem is then to minimize

(b d)

A
where A 2 and A 4 have t columns, A 3 and A 4 have s rows, and the matrix norm is
a separable norm on s  t matrices. Unfortunately, the separable norm is dened in
terms of two vector norms k:k A on R s and k:k B on R t , and k:k A as used in (4:3)
is on R m . We get around this potential con
ict by assuming that k:k A is dened
for any length of vector; we will also assume that the introduction of additional zero
components does not change the value of the norm.
The attainment of the maximum in (4.3) is not quite so straightforward as before.
However, we can prove the following result.
Theorem 4.2. Let denote the rst m s components of r,
and let r 2 denote the last s components. Let x solve the problem
subject to r
Then x solves (4:3):
Proof. Arguing as in previous results, an upper bound for the maximum (subject
to the constraints) in (4.3) is
Now dene the set
For any x 2 X, dene
where u rst (m s) components zero, and last s components forming
the vector u 2 with
arbitrary except that

A 3 A 4 +E
(b d)

A


A


A


The result is proved.
Of course the set X may be empty. In that case, while the problem (4.3) is still
well dened, it is not clear that a matrix E and a vector d can be dened such that
the maximum in the problem is attained. That being the case, there is no obvious
equivalent simpler problem.
4.2. Bounded columns of E. Suppose that the columns of E are individually
bounded so that
where e i is the ith unit vector, and consider the problem of nding
As for Theorem 2.1, we can prove the following result.
Theorem 4.3. For any x, the maximum in (4:5) is attained when
otherwise u is arbitrary but
1. The maximum value is
Even in the least squares case, this objective function is not normally dieren-
tiable, being a combination of a least squares norm and a weighted l 1 norm. It can
be reposed as a smooth constrained optimization problem, and solved by standard
techniques.
4.3. Structured problems. In some applications, the perturbation matrices
have known structure, as in the following problem considered by El Ghaoui and Lebret
[9]. Given A
min
k-k

x

A
where k:k A is a given norm on R m and k:k is a given norm on R p . Dene for any
Consider the maximum in (4.6), which will be attained at the solution to the problem
maximize kr 0 +M-kA subject to
assuming that - maximizing kr exceeds  in norm. Because the functions
involved are convex, necessary conditions for a solution can readily be given: these
are that there exists R such that
1290 G. A. WATSON
Using these conditions, it is easily seen that
Therefore, an equivalent (in a sense dual) problem is
subject to
Consider the special case when both norms are least squares norms. Then
and so the necessary conditions can be written
Thus
provided that I F is nonsingular. A way of solving this problem based on those
results is given by El Ghaoui and Lebret [9]. They also consider the problem when k:k
is the Chebyshev norm. Extending the ideas to more general norms, however, does
not look straightforward.
5. A min-min problem. The problems (2.1) and (3.1) are examples of min-max
problems: minimization is carried out with respect to x over all allowed perturbations
in the data. This is justied if the emphasis is on robustness. However, from other
considerations it may be su-cient to minimize with respect to x while simultaneously
minimizing with respect to the perturbations. This gives rise to a min-min problem,
as considered (least squares case) in [2], [3], [5]. In this nal section, we will brie
y
consider this problem. Again there are two versions, consistent with those treated in
sections 2 and 3. To illustrate the ideas involved, we will consider nding
min
(5.
In contrast to the min-max case, here we are seeking to nd a solution x which gives
the smallest possible error over allowable perturbations.
Again the problem can be replaced by an equivalent unconstrained optimization
problem.
Theorem 5.1. Let  be small enough that
Then (5:1) is equivalent to the problem of minimizing with respect to x
kAx bk A k[x
Proof. Let (5.2) be satised and let x be arbitrary. Let
otherwise arbitrary. Then
Now x
where
otherwise u is arbitrary with
and further
using (5.2). The result follows.
There are two important dierences between (5.3) and (3.4): rst, the relationship
leading to (5.3) requires a condition on , and second, the resulting problem is not a
convex problem. The nonconvexity of (5.3) is interpreted in [2] as being equivalent
to using an \indenite" metric, in the spirit of recent work on robust estimation and
ltering: see, for example, [11], [12], [16].
The condition (5.2) is satised if
that is, if  does not exceed  T (see section 3.2). If
attained at Indeed if  is set to any local minimum of (3.7), with value  T ,
then the corresponding point x T generated from the local minimizer z T is a stationary
point of (5.1), as the following argument shows.
Necessary conditions for x to solve (3.7) are that there exist v
and a Lagrange multiplier  such that
G. A. WATSON
Multiplying through by z T
T shows that
Now the relationship z
implies that sign()v 2 @kAx T bk A and
In other words, there exist v 2 @kAx T bk A , w 2
denotes the rst n components of w. It follows from standard convex
analysis that x T is a stationary point of the problem of minimizing
A similar treatment can be given if (5.1) is replaced by the related problem of
nding
min
Provided that  is small enough that
then this is equivalent to the problem of nding
fkAx bk A kxkB g:
An algorithm is given in [2] for solving the least squares case of this problem. It
has similarities to the algorithms given before, involving the solution of a nonlinear
equation for  and a linear system for x. Indeed it is clear that many of the ideas which
apply to min-max problems carry over to problems of the present type. However, we
do not consider that further here.
6. Conclusions. We have given an analysis in a very general setting of a range
of data tting problems, which have attracted interest so far in the special case when
least squares norms are involved. While this case is likely to be most useful in practice,
consideration of other possibilities can be motivated by the valuable role that other
norms play in a general data tting context. The main thrust of the analysis has been
to show how the original problems may be posed in a simpler form. This permits the
numerical treatment of a wide range of problems involving other norms, for example,
l p norms. We have also included some observations which contribute to algorithmic
development for the important least squares case.

Acknowledgment

. I am grateful to the referees for helpful comments which
have improved the presentation.



--R


Parameter estimation in the presence of bounded modeling errors
Parameter estimation in the presence of bounded data uncertainties

The degenerate bounded errors-in-variables model
Fitting Equations to Data
Applied Regression Analysis
Robust solutions to least squares problems with uncertain data
Robust solutions to least-squares problems with uncertain data
An analysis of the total least squares problem
Recursive linear estimation in Krein spaces-Part I: Theory
Filtering and smoothing in an H 1 setting
An analysis of the total approximation problem in separable norms
Convex Analysis
Estimation in the presence of multiple sources of uncertainties with applications
Inertia conditions for the minimization of quadratic forms in inde
Estimation and control in the presence of bounded data uncertainties
ed., Recent Advances in Total Least Squares Techniques and Errors-in- Variables Modeling
The Total Least Squares Problem: Computational Aspects and Analysis
Choice of norms for data
Solving data
--TR
