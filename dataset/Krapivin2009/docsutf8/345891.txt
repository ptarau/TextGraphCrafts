--T
On the Minimizing Property of a Second Order Dissipative System in Hilbert Spaces.
--A
We study the asymptotic behavior at infinity of solutions of  a second order evolution equation with linear damping and convex potential. The differential system is defined in a real Hilbert space. It is proved that if the potential is bounded from below, then the  solution trajectories are  minimizing for it and converge weakly towards a minimizer of $\Phi$ if one exists; this convergence is strong  when $\Phi$ is even or when the optimal set has a nonempty interior.  We  introduce a second order proximal-like iterative algorithm for the minimization of a convex function. It is defined  by an implicit discretization of the continuous evolution problem and is valid for any closed proper convex function.  We find conditions on some parameters of the algorithm in order to have a convergence result similar to the continuous case.
--B
Introduction
. Consider the following dierential system dened in a real
Hilbert space H
where
is dierentiable. It is customary to call this equation non-linear
oscillator with damping. Here, the damping or friction has a linear dependence
on the velocity. This is a particular case of the so called dissipative systems. In fact,
given u solution of
is direct to check that
Thus, the energy of the system is dissipated as t increases. Although (1:1)
appears in various contexts with dierent physical interpretations, the motivation for
this work comes from the dynamical approach to optimization problems.
Roughly speaking, any iterative algorithm generating a sequence fx k g k2IN may
be considered as a discrete dynamical system. If it is possible to nd a continuous
version for the discrete procedure, one expects that the properties of the corresponding
continuous dynamical system are close to those of the discrete one. This occurs,
for instance, for the now classical Proximal method for convex minimization: given
solve the iterative scheme
is a closed proper convex function and @f denotes
the usual subdierential in convex analysis. Prox is an implicit discretization for the
Steepest Descent method, which consists in solving the following dierential inclusion
Under suitable conditions, both the trajectory dened by (SD) and
the sequence fx k g generated by (P rox) converge towards a particular minimizer of
Research partially supported by FONDECYT 1961131 and FONDECYT 1990884.
y Departamento de Ingeniera Matematica, Universidad de Chile, Casilla 170/3 Correo 3, Santiago,
Chile. Email: falvarez@dim.uchile.cl. Fax: (56)(2)6883821.
f (see [5, 6, 7] for (SD) and [18] for (P rox), see also [12] for a survey on these
and new results). The dynamical approach to iterative methods in optimization has
many advantages. It provides a deep insight on the expected behavior of the method,
and sometimes the techniques used in the continuous case can be adapted to obtain
results for the discrete algorithm. On the other hand, a continuous dynamical system
satisfying nice properties may suggest new iterative methods.
This viewpoint has motivated an increasing attention in recent years, see for
instance [1, 2, 3, 4, 8, 13, 14]. In [3], Attouch et al. deal with non convex functions
that have a priori many local minima. The idea is to exploit the dynamics dened
by (1:1) to explore critical points of  (i.e. solutions of coercive
(bounded level sets) and of class C 1 with gradient locally Lipschitz, then it is possible
to prove that for any u solution of (1:1) we have 1. The
convergence of the trajectory 1g is a more delicate problem. When  is
coercive, an obvious su-cient condition for the convergence of the trajectory is that
the critical points, also known as equilibrium points, are isolated. Certainly, this is not
necessary. In dimension one additional conditions, the solution
always converges towards an equilibrium (see for instance [10]). The proof relies on
topological arguments that are not generalizable to higher dimensions. Indeed, this is
no longer true even in dimension two: it is possible to construct a coercive C 1 function
dened on IR 2 whose gradient is locally Lipschitz and for which at least one solution
of (1:1) does not converge as t ! 1 (see [3]). Thus, a natural question is to nd
general conditions under which the trajectory converge in the degenerate case, that is
when the set of equilibrium points of  contains a non-trivial connected component.
A positive result in this direction has been recently given by Haraux and Jendoubi in
[11], where convergence to an equilibrium is established when  is analytic. However,
this assumption is very restrictive from the optimization point of view.
Motivated by the previous considerations, in this work we focus our attention on
the asymptotic behavior as t !1 of the solutions of (1:1) when  is assumed to be
convex. The paper is organized as follows. In x2 we prove that if  is convex and
bounded from below then the trajectory minimizing for . If the
inmum of  on H is attained then u(t) converges weakly towards a minimizer of .
The convergence is strong when  is even or when the optimal set has a nonempty
interior. In x2.2 we give a localization result for the limit point, analogous to the
corresponding result for the steepest descent method [13]. In x2.4 we generalize the
convergence result to cover the equation
is a bounded self-adjoint linear operator, which we assume to be elliptic: there is
> 0 such that for any x 2 H , h x; xi
We refer to this equation as non-linear
oscillator with anisotropic damping. This equation appears to be useful to
diminish oscillations or even eliminate them, and also to accelerate the convergence
of the trajectory. In x2.3 we give an heuristic motivation of the above mentioned
facts, which is based on an analysis of a quadratic function. Still under the convexity
condition on , x3 deals with the discretization of (1:1). Here, we consider the implicit
scheme
where h > 0. Since  is convex, the latter is equivalent to the following variational
problem
SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 3
where z
procedure does not require  to be dier-
entiable and allows us to introduce the following more general iterative-variational
algorithm k
closed proper convex function and
@  f is the -approximate subdierential in convex analysis. We call (1:2) Proximal
with impulsion method. We nd conditions on the parameters  k ;  k and  k in order
to have a convergence result similar to the continuous case. Finally, in the Appendix
we illustrate with an example the behavior of the trajectories dened by (1:1), and
we also state some of the questions opened by this work. Let us mention that the
rst to consider equation (1:1) for nite dimensional optimization problems was B.
T. Polyack in [16]. He studied a two-step discrete algorithm called \heavy-ball with
friction" method, which may be interpreted as an explicit discretization of (1.1). Both
approaches are complementary: the analysis and the type of results in the implicit
and explicit cases are dierent.
2. Dissipative dierential system. Throughout the paper, H is a real Hilbert
space, h; i denotes the associated inner product and j  j stands for the corresponding
norm. We are interested in the behavior at innity of solution of the
following abstract evolution equation
where
are given. Note that if we assume that the
gradient r is locally Lipschitz then the existence and uniqueness of a local solution
for
standard results of dierential equations theory. In that
case, to prove that u is innite extendible to the right, it su-ces to show that its
derivative u 0 is bounded. Set
, the function E is non-increasing. If we suppose that  is
bounded from below then u 0 is bounded.
2.1. Asymptotic convergence. In the sequel, we suppose the existence of a
global solution of
for the inmum value of  on H ; thus,
mean that  is bounded from below. We denote by Argmin the
set fx g. On the nonlinearity we shall assume
Theorem 2.1. Suppose that (h ) holds. If u 2 C 2 ([0; 1[; H) is a solution of
lim
Furthermore, if Argmin  6= ; then there exists b
such that u(t) * b
weakly in H as t !1.
4 F. ALVAREZ
Proof. We begin by noticing that u 0 is bounded (see the above argument). In
order to prove the minimizing property (2:1), it su-ces to prove that
lim sup
for any x 2 H . Fix x 2 H and dene the auxiliary function '(t) := 1
u is solution of
it follows that
which together with the convexity inequality (u)
We do not have information on the behavior of (u(t)) but we know that E(t) is
non-increasing. Thus, we rewrite (2:2) as
Given t > 0, for all  2 [0; t] we have
After multiplication by e
and integration we obtain
We write this equation with t replaced by , use the fact that E(t) decreases and
integrate once more to obtain
where
h(t) := 3Z tZ e
Since E(t)  (u(t)), (2:3) gives2
Dividing this inequality by 1(
letting t !1 we get
lim sup
It su-ces to show that h(t) remains bounded as t !1. By Fubini's theorem
e
SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 5
Note that from the equality
and in particular
Then
h(t)  3Z tju 0 ()j 2 d  3Z 1ju 0 ()j 2 d < 1
as was to be proved.
On the other hand, since E() is non-increasing and bounded from below by inf ,
it converges as t ! 1. If lim
E(t) > inf , then lim
because of (2:1).
This contradicts the fact that u 0 2 L 2 . Therefore, lim
as t !1.
The task is now to establish the weak convergence of u(t) when Argmin  6= ;.
For this purpose, we shall apply the Opial lemma [15], whose interest is that it allows
one to prove convergence without knowing the limit point. We state it as follows.
Lemma [Opial]. Let H be a Hilbert space , H be a trajectory
and denote by W the set of its weak limit points
If there exists ; 6= S  H such that
then W 6= ;. Moreover, if W  S then u(t) converges weakly towards b u 2 S as
In order to apply the above result, we must nd an adequate set S. Suppose that
there exists b
H such that u(t k ) * b u for a suitable sequence t k !1. The function
is weak lower-semicontinuous, because  is convex and continuous, hence
(bu)  lim inf
and therefore b u 2 Argmin . According with the Opial lemma, we are reduced to
prove that
exists:
For this, x z 2 Argmin  and dene '(t) := 1
provides a su-cient condition on [' 0 , the positive part of the derivative, in order to
ensure convergence for '.
Lemma 2.2. Let  2 C 1 ([0; 1[; IR) be bounded from below. If [
then (t) converges as t !1.
6 F. ALVAREZ
Proof. Set
w(t) := (t)
Since w(t) is bounded from below and w 0 (t)  0, then w(t) converges as t !1, and
consequently (t) converges as t !1.
On account of this result, it su-ces to prove that [' 0 belongs to L 1 (0; 1). Of
course, to obtain information on ' 0 we shall use the fact that u(t) is solution of
Due to the optimality of z, it follows from (2:2) that
Lemma 2.3. If the dierential inequality
with
Proof. We can certainly assume that g  0, for if not, we replace g by jgj.
Multiplying (2:6) by e
t and integrating we get
Z te
Thus
Z te
and Fubini's theorem gives
Z 1Z te
Recalling that ju 0 the proof of the theorem is completed by
applying lemma 2.3 to equation (2:5).
We say that r is strongly monotone if there exists  > 0 such that for any
A weaker condition is the strong monotonicity over bounded sets, that is to say, for
all K > 0 there exists K > 0 such that for any x; y 2 B[0; K] we have
If the latter property holds, then we have strong convergence for u(t) when the inmum
of  is attained. The argument is standard: let b
u be the (unique) minimum
point for  and set K := maxfsup t0 ju(t)j; jbujg, then from (2:7) we deduce
Since we have proven that lim
b u strongly in H . Note that we do not need to apply the Opial lemma.
SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 7
The latter is the case of a non-degenerate minimum point. When  admits
multiple minima, it is not possible to obtain strong convergence without additional
assumptions on  or the space H . For instance, we have the following
Theorem 2.4. Under the hypotheses of Theorem 2.1, if either
(i) Argmin 6= ; and  is even
or
then
u strongly in H as t !1;
Proof. The proof is adapted from the corresponding results for the steepest descent
method; see [7] for the analogous of (i) and [6] for (ii).
quently
decreasing and  is even, we deduce that
for all t 2 [0; t 0 ]: By the convexity of  we conclude
hence
Thus
The standard integration procedure yields
Therefore, for all t 2 [0;
where
8 F. ALVAREZ
On the other hand, in the proof of Theorem 2.1 we have shown that h(t) is convergent
as t ! 1. We also proved that for all z 2 Argmin  the lim
ju(t) zj exists. Since
is convex and even, we have 0 2 Argmin whenever the inmum is realized. In
that case, ju(t)j is convergent as t ! 1 and we infer from (2.9) that
is a Cauchy net. Hence u(t) converges strongly as t ! 1 and, by Theorem 2.1, the
limit belongs to Argmin .
There exists  > 0 such that for every z 2 H with
In particular, if jz z 0 j   then
Consequently,
for every x 2 H and z with jz z 0 j  . Hence,
for every x 2 H . Applying this inequality to x = u(t) we deduce that
We thus obtain
Integrating this inequality yields
But we have already proved that the lim
'(t) exists and lim
As a conclusion, u
We deduce that the
lim
u(t) exists, which nishes the proof because u
2.2. Localization of the limit point. In the proof of Theorem 2.1 we have
used the dierential inequality (2:2), which in some sense measures the evolution of
the system. A simpler but analogous inequality appears in the asymptotic analysis
for the steepest descent inclusion (SD). This was used by B. Lemaire in [13] to locate
the limit point of the trajectories of (SD). Following this approach, in this section we
give a localization result of the limit point of the solutions of (E
). For simplicity of
notation, set S := Argmin and we denote by proj the projection operator
onto the closed convex set S.
Proposition 2.5. Let u be solution of
be such that
Consequently
where is the distance between u 0 and the set S.
SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 9
(ii) if S is an a-ne subspace of H then
If moreover  is a quadratic form then
strongly in H as t !1:
Proof. Let x 2 S and set '(t) := 1
. The inequality (2:2) and the
optimality of x give
Hence
Z tZ e
Due to the weak lower semi-continuity of the norm and Fubini's theorem, we can let
t !1 to obtain2 jbu xj 2  1
On the other hand, from the energy equation2 ju
it follows Z 1ju 0 ()j 2 d  1
Replacing the last estimate in (2.11), it easy to show that (2.10) holds.
For (i), it su-ces to take
For (ii), let e := b u proj S
which belongs to S. An easy computation shows that
which together with (2.10) yields
Letting r !1 we get the result.
Finally, suppose that
positive and self-adjoint
bounded linear operator. Then the null space of A.
Let z 2 S; for all t  0 we have that
strongly ( is even) as t !1, we can deduce that
for all z 2 S, which completes the proof.
2.3. Linear system: heuristic comparison. Before proceeding further it is
interesting for the optimization viewpoint to compare the behavior of the trajectories
dened by
with the steepest descent equation
and with the continuous Newton's method
For simplicity, in this section we restrict ourselves to the associated linearized systems
in a nite dimensional space. We shall consider and assume that  2
IR). Related to (SD) we have the linearized system around some x 0 2 IR N ,
which is dened by
We assume that the Hessian matrix r 2 positive denite. An explicit computation
shows that In fact, the solutions
of (LSD) are of the form
solves the homogeneous equation
0: Take a matrix P such that
where  i > 0, and set P We obtain the system  0
solutions
are  i Generally speaking, if there is a  i << 1, we will have a relative
slow convergence towards the solution; on the other hand, when dealing with large  0
the numerical integration by an approximate method will present stability problems.
Thus we see that the numerical performance of (SD) is strongly determined by the
local geometry of the function .
We turn now to the linearized version of (N ), given by
The solutions are of the form
which are much better than the
previous ones. The major properties are : 1) the straight-line geometry of the tra-
jectories; 2) the rate of convergence is independent of the quadratic function to be
minimized. Certainly, this is just a local approximation of the original function and
the global behavior of the trajectory may be complicated. Nevertheless, this outstanding
normalization property of Newton's system makes it eective in practice, due to
the fact that the associated trajectories are easy to follow by a discretization method.
Of course, an important disadvantage of (N) is the computation of the inverse of the
Hessian matrix, which may be involved for a numerical algorithm.
Finally, we consider
z
SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 11
For this equation we have solves the homogeneous problem
It is a simple matter to show that
)t with  i :]0; 1[!]0; 1[
continuous and C i a constant independent of
. In fact,  i (
is non-increasing on ]2 p
then the corresponding
does not present oscillations. Thus the choice
greatest rate that can be obtained. But we can get any value in the interval ]0; p
for instance, when  i > 1 we obtain
1. The
last choice has the advantage that the associated trajectory is not oscillatory, which
is interesting by numerical reasons. Note that we should take a dierent parameter
according to the corresponding eigenvalue  i . See the Appendix for an illustration of
this simple analysis.
Therefore, the presence of the damping parameter
gives us a control on the
behavior of the solutions of (E
and, in particular, on some qualitative properties
of the associated trajectories. For a general  we must take on account: a) a careful
selection of the damping parameter
should depend on the local geometry of the
function , leading to a nonautonomous damping; b) this selection could give a different
value of
for some particular directions, leading to an anisotropic damping.
No attempt has been made here to develop a theory in order to guide these choices.
2.4. Linear and anisotropic damping. In the preceding section we have seen
that it may be of interest to consider an anisotropic damping. With the aim of
contributing to this issue, in this section we establish the asymptotic convergence for
the solutions of the following system
H is a bounded self-adjoint linear operator, which we assume to be
elliptic
> 0 such that for any x 2 H; h x; xi
Theorem 2.6. Suppose (h ) and (h ) hold. If u 2 C 2 ([0; 1[; H) is a solution
of
lim
Furthermore, if Argmin  6= ; then there exists b
such that u(t) * b
weakly in H as t !1.
Proof. We only need to adapt the proof of Theorem 2:1. First, note that the
properties of existence, uniqueness and innite extendibility to the right of the solution
follow by similar arguments. Likewise, the energy E(t) := 1
and we can deduce that u
Next, dene the operator
x, with
such a way that
As in the proof of Theorem 2:1, equation (2:13) gives
Z te
(2.
with
the only dierence being the term
An integration by parts yields
Z te
Z te
Setting f(t) :=
Z te
Thus, we can rewrite (2:14) as
We leave it to the reader to verify that the minimizing property (2:12) can now be
established as in Theorem 2:1. Analogously for the proof of u
When Argmin  6= ;, we x z 2 Argmin and consider the corresponding functions
' and  as above (with x replaced by z). Using the optimality of z, it follows
Z te
with f associated with  as above. Integrating this inequality we conclude that '(t)
stays bounded as t ! 1, but we cannot deduce its convergence. Then, we rewrite
(2:15) in the form
Z te
Z te
and we conclude that [' 0 (t)
We note that
Z te
where
and
Z te
In virtue of Lemma 2.2, if we show that (t) is bounded from below then (t)
converges as t ! 1. Since  0 there exists a constant M > 0
independent of t such that j 0 (t)j  M ju 0 (t)j
'(t) for any t > 0. We conclude that
SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 13
1. From this fact it follows easily that (t) ! 0 as t ! 1.
Therefore, converges as t !1, hence (t) converges as well.
The proof is completed by applying the Opial lemma to the trajectory
1g, where the Hilbert space H is endowed with the inner product hh; ii
dened by hhx; yii := 1
h x; yi and its associated norm.
Remark 1. In Theorems 2.1 - 2.6 we do not require any coerciveness assumption
on . When Argmin 6= ;, the dissipativeness in the dynamics su-ces for the
convergence of the solutions. If the inmum value is not realized the trajectory may
be unbounded as in the one dimensional equation
whose solutions are so that u(t) ! 1 and u
In any case, our results assert that the dynamical system dened by (E
(or more
generally by (E )) is dissipative in the sense that every trajectory evolves towards a
minimum of the energy. Certainly, there is a strong connection with the concept of
point dissipativeness or ultimately boundedness in the theory of dynamical systems,
where the Lyapunov function associated with the semigroup is usually supposed to
be coercive (c.f. [9, gradient systems]).
Remark 2. To ensure local existence and uniqueness of a classical solution
for the dierential equation, it su-ces to require a local Lipschitz property on r.
Actually, in some situations this hypothesis is not necessary and the existence may
be established by other arguments. For instance, that is the case of the Hille-Yosida
theorem for evolution equations governed by monotone operators and the theory of
linear and non-linear semigroups for partial dierential equations. Note that such
a Lipschitz condition on the gradient is not used in the asymptotic analysis of the
trajectories. Therefore, the previous asymptotic results remain valid for other classes
of innite-dimensional dissipative systems, provided the existence of a global solution.
It is not our purpose to develop this point here for the continuous system because it
exceeds the scope of this paper. However, in the next section we consider an implicit
discretization of the continuous system. As we will see, the existence of the discrete
trajectory is ensured by variational arguments. This allow us to apply the discrete
scheme to nonsmooth convex functions and to adapt the asymptotic analysis to this
case.
3. Discrete approximation method. Once we have established the existence
of a solution of an initial value problem, we are interested in its numerical values. We
must accept that most dierential equations cannot be solved explicitly; we are thus
lead to work with approximate methods. An important class of these methods is based
on the approximation of the exact solution over a discrete set ft n g: associated with
each point t n we compute a value un , which approximates u(t n ) the exact solution at
Generally speaking, these procedures have the disadvantage that a large number
of calculations has to be done in order to keep the discretization error e n := un u(t n )
su-ciently small. In addition to this, the estimates for the errors strongly depends on
the length of the discretization range for the t variable. It turns out that these methods
are not well adapted to the approximation of the exact solution on an unbounded
domain.
Nevertheless, there is an important point to note here. If our objective is the
asymptotic behavior of the solutions as t goes to 1, then the accurate approximation
of the whole trajectory becomes immaterial. We present a discrete method whose
feature is that no attempt is made to approximate the exact solution over a set of
14 F. ALVAREZ
points, but the discrete values are sought only to preserve the asymptotic behavior of
the solutions.
3.1. Implicit iterative scheme. Dealing with the discretization of a rst order
dierential equation y it is classical to consider the implicit iterative scheme
y
where h > 0 is a parameter called step size. In the case of equation (E
or more
precisely its rst order equivalent system, (3:1) corresponds to recursively solve
Since  is convex, (3:2) is equivalent to the following variational problem
where z
This motivates the introduction of the more general
iterative procedure
where z are positive. Note that when
the standard Prox iteration. If  > 0, the starting point for the next iteration is
computed as a development in terms of the velocity of the already generated sequence.
Therefore, this iterative scheme denes a second order dynamics, while Prox is actually
of rst order nature.
We have been working under the assumption that  is dierentiable. However,
for the above iterative variational method this regularity is no longer necessary. Thus,
in the sequel f denotes a closed proper convex function (see [17]),
which eventually realizes the value 1, and we consider
where z In terms of the stationary condition, (3:3) is equivalent
to
where @f is the standard convex subdierential [17].
3.2. Convergence for the variational algorithm. By numerical reasons, it
is natural to consider the following approximate iterative scheme k
where  k in non-negative,  k is positive and @  f is the -subdierential. Note that a
sequence fu k g  H satisfying (3:4) always exists. Indeed, given u
SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 15
take u k+1 as the unique solution of the strongly convex problem
as above.
Theorem 3.1. Assume that f is closed proper convex and bounded from below.
Let fu k g  H be a sequence generated by (3:4), where
is bounded from below by a positive constant.
(ii) the sequence f is non-increasing and
Then
lim
and in particular lim
When Argmin f 6= ;, assume in addition that
(iii) there exists
2]0; 1[ such that 0   k
, and f k g is bounded from above
if there is at least one  k > 0.
Then, there exists b u 2 Argmin f such that u k * b
u weakly as k !1.
Proof. The proof consists in adapting the analysis done for the dierential equation
We begin by dening the discrete energy by
and we study the successive dierence E
By denition of @  k
f , (3:4) yields
As we can write
we have
and consequently
Noting that
and because 0   k  1, we deduce that
and
As 0   k  1 and  k is bounded from below by a positive constant, we have
lim
Writing
we conclude that (3:5) holds.
Suppose now that Argmin f 6= ;. We apply the Opial lemma to prove the weak
convergence of fu k g. On account of (3:5), it is su-cient to show that for any z 2
Argmin f , the sequence of positive numbers fju k zjg is convergent. Fix z 2 Argmin f ;
since u k+1 satises (3:4), we have
and by the optimality of z
. It is direct to check that for any k 2 IN
that
and therefore
Using (iii) and (3:6) it follows
that
the above inequality
implies
Thus
which yieldsX

SECOND-ORDER DISSIPATIVE SYSTEM AND MINIMIZATION 17
is bounded from
below. As fw k g is non-increasing we have that it converges. Hence, f' k g converges,
which completes the proof of the theorem.
For simplicity, we have considered in this section the isotropic damping system.
However, a similar analysis can be done for the anisotropic damping associated with
an elliptic self-adjoint linear operator . The variational problem associated
with the implicit discretization is
where z
For a function f closed proper and convex, the latter motivates the
scheme
H is a linear positive denite operator and S linear
and positive semi-denite. If we assume both R and I S are elliptic, it is possible
to obtain a convergence result like the previous one. It su-ces to adapt the main
arguments. Since the basic ideas are contained in the proof of Theorems 2.6 and 3.1,
we do not go further in this matter.
4. Some open problems. In the case of multiple optimal solutions, our convergence
results does not provide additional information on the point attained in the
limit. A possible approach to overcome this disadvantage may be to couple the dissipative
system with approximation techniques as regularization, interior-barrier or
globally dened penalizations and viscosity methods. In the continuous case, this
alternative has been considered with success for the steepest descent equation in [2]
and for Newton's method in [1], giving a characterization for the limit point under
suitable assumptions on the approximate scheme. On account of these results, one
may conjecture that this can be done for the equations considered in the present work.
On the other hand, we have seen that the behavior of the trajectories depends
on a relation between the damping and the local geometry of the function we wish to
minimize. This remark leads us to the obvious problem of the choice of the damping
parameter, in order to have a better control on the trajectory. This is also a problem
in the discrete algorithm. Usually we have an incomplete knowledge of the objective
function, which makes the question more di-cult. We think that a rst step in this
direction may be the study of more general damped equations, with non-linear and/or
non-autonomous damping.

Acknowledgments

. I wish to express my gratitude to the Laboratoire d'Analyse
Convexe de l'Universite Montpellier II for the hospitality and support, and specially
to Professor Hedy Attouch. I gratefully acknowledge nancial support through a
French Foreign Scholarship grant from the French Ministry of Education and a Chilean
National Scholarship grant from the CONICYT of Chile. I wish to thank the helpful
comments of a referee concerning theorems 2.2 and 3.1.



--R


A dynamical approach to convex minimization coupling approximation with the steepest descent method
A dynamical method for the global exploration of stationary points of a real-valued mapping: the heavy ball method
The nonlinear geometry of linear programming (parts I and II)
Monotonicity methods in Hilbert spaces and some applications to nonlinear partial di

Asymptotic convergence of nonlinear contraction semi-groups in Hilbert spaces
Asymptotic convergence of the steepest descent method for the exponential penalty in linear programming


Convergence of solutions of second-order gradient-like systems with analytic nonlinearities
About the convergence of the proximal method
An asymptotical variational principle associated with the steepest descent method for a convex function
The projective SUMT method for convex programming
Weak convergence of the sequence of successive approximations for nonexpansive mappings
Some methods of speeding up the convergence of iterative methods


--TR

--CTR
A. Moudafi , M. Oliny, Convergence of a splitting inertial proximal method for monotone operators, Journal of Computational and Applied Mathematics, v.155 n.2, p.447-454, 15 June
