--T
Parameter Estimation in the Presence of Bounded Data Uncertainties.
--A
We formulate and solve a new parameter estimation problem in the presence of data uncertainties. The new method is suitable when a priori bounds on the uncertain data are available, and its solution leads to more meaningful results, especially when compared with other methods such as total least-squares and robust estimation. Its superior performance is due to the fact that the new method guarantees that the effect of the uncertainties will never be unnecessarily  over-estimated, beyond what is reasonably assumed by the a priori bounds. A geometric interpretation of the solution is provided, along with a closed form expression for it. We also consider the case in which only selected columns of the coefficient matrix are subject to perturbations.
--B
Introduction
. The central problem in estimation is to recover, to good ac-
curacy, a set of unobservable parameters from corrupted data. Several optimization
criteria have been used for estimation purposes over the years, but the most im-
portant, at least in the sense of having had the most applications, are criteria that
are based on quadratic cost functions. The most striking among these is the linear
least-squares criterion, which was first developed by Gauss (ca. 1795) in his work on
celestial mechanics. Since then, it has enjoyed widespread popularity in many diverse
areas as a result of its attractive computational and statistical properties (see, e.g.,
[4, 8, 10, 13]). Among these attractive properties, the most notable are the facts
that least-squares solutions can be explicitly evaluated in closed forms, they can be
recursively updated as more input data is made available, and they are also maximum
likelihood estimators in the presence of normally distributed measurement noise.
Alternative optimization criteria, however, have been proposed over the years
including, among others, regularized least-squares [4], ridge regression [4, 10], total
addresses: shiv@ece.ucsb.edu, golub@sccm.stanford.edu, mgu@math.ucla.edu, and
sayed@ee.ucla.edu.
least-squares [2, 3, 4, 7], and robust estimation [6, 9, 12, 14]. These different formulations
allow, in one way or another, incorporation of further a priori information about
the unknown parameter into the problem statement. They are also more effective in
the presence of data errors and incomplete statistical information about the exogenous
signals (or measurement errors).
Among the most notable variations is the total least-squares (TLS) method, also
known as orthogonal regression or errors-in-variables method in statistics and system
identification [11]. In contrast to the standard least-squares problem, the TLS formulation
allows for errors in the data matrix. But it still exhibits certain drawbacks that
degrade its performance in practical situations. In particular, it may unnecessarily
over-emphasize the effect of noise and uncertainties and can, therefore, lead to overly
conservative results.
More specifically, assume A 2 R m\Thetan is a given full rank matrix with m  n,
is a given vector, and consider the problem of solving the inconsistent linear
system A"x  b in the least-squares sense. The TLS solution assumes data uncertainties
in A and proceeds to correct A and b by replacing them by their projections, "
A
and " b, onto a specific subspace and by solving the consistent linear system of equations
b. The spectral norm of the correction
in the TLS solution is
bounded by the smallest singular value of
\Theta
A b

. While this norm might be small
for vectors b that are close enough to the range space of A, it need not always be so.
In other words, the TLS solution may lead to situations in which the correction term
is unnecessarily large.
Consider, for example, a situation in which the uncertainties in A are very small,
say A is almost known exactly. Assume further that b is far from the column space of
A. In this case, it is not difficult to visualize that the TLS solution will need to rotate
may therefore end up with an overly corrected approximant for
A, despite the fact that A is almost exact.
These facts motivate us to introduce a new parameter estimation formulation with
prior bounds on the size of the allowable corrections to the data. More specifically, we
formulate and solve a new estimation problem that is more suitable for scenarios in
which a-priori bounds on the uncertain data are known. The solution leads to more
meaningful results in the sense that it guarantees that the effect of the uncertainties
will never be unnecessarily over-estimated, beyond what is reasonably assumed by the
a-priori bounds.
We note that while preparing this paper, the related work [1] has come to our
attention, where the authors have independently formulated and solved a similar estimation
problem by using (convex) semidefinite programming techniques and interior-point
methods. The resulting computational complexity of the proposed solution is
is the smaller matrix dimension.
The solution proposed in this paper proceeds by first providing a geometric formulation
of the problem, followed by an algebraic derivation that establishes that the
optimal solution can in fact be obtained by solving a related regularized problem. The
parameter of the regularization step is further shown to be obtained as the unique
positive root of a secular equation and as a function of the given data. In this sense,
the new formulation turns out to provide automatic regularization and, hence, has
some useful regularization properties: the regularization parameter is not selected by
the user but rather determined by the algorithm. Our solution involves an SVD step
and its computational complexity amounts to O(mn 2 is again the
smaller matrix dimension. A summary of the problem and its solution is provided in
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
Sec. 3.4 at the end of this paper. [Other problem formulations are studied in [15].]
2. Problem Formulation. Let A 2 R m\Thetan be a given matrix with m  n and
be a given vector, which are assumed to be linearly related via an unknown
vector of parameters x 2 R n ,
The vector v 2 R m denotes measurement noise and it explains the mismatch between
Ax and the given vector (or observation) b.
We assume that the "true" coefficient matrix is A + ffiA, and that we only know
an upper bound on the 2\Gammainduced norm of the perturbation ffiA:
with j being known. Likewise, we assume that the "true" observation vector is b
and that we know an upper bound j b on the Euclidean norm of the perturbation ffib:
We then pose the problem of finding an estimate that performs "well" for any allowed
perturbation (ffiA; ffib). More specifically, we pose the following min-max problem:
Problem 1. Given A 2 R m\Thetan , with m  n, nonnegative real
numbers (j; j b ). Determine, if possible, an "
x that solves
x
The situation is depicted in Fig. 2.1. Any particular choice for "
x would lead to
many residual norms,
one for each possible choice of A in the disc in the disc (b
second choice for "
x would lead to other residual norms, the maximum value of which
need not be the same as the first choice. We want to choose an estimate " x that
minimizes the maximum possible residual norm. This is depicted in Fig. 2.2 for two
choices, say "
. The curves show the values of the residual norms as a function
of
A
Fig. 2.1. Geometric interpretation of the new least-squares formulation.
We note that if problem (2.4) reduces to a standard least squares
problem. Therefore we shall assume throughout that j ? 0. [It will turn out that the
solution to the above min-max problem is independent of j b ].
kresidualk
Fig. 2.2. Two illustrative residual-norm curves.
2.1. A Geometric Interpretation. The min-max problem admits an interesting
geometric formulation that highlights some of the issues involved in its solution.
For this purpose, and for the sake of illustration, assume we have a unit-norm
vector b, kbk uncertainties in it (j Assume further that A is
simply a column vector, say a, with j 6= 0. That is, only A is assumed to be uncertain
with perturbations that are bounded by j in magnitude (as in (2.2)). Now consider
problem (2.4) in this context, which reads as follows:
x
This situation is depicted in Fig. 2.3. The vectors a and b are indicated in thick
black lines. The vector a is shown in the horizontal direction and a circle of radius j
around its vertex indicates the set of all possible vertices for a
a
Fig. 2.3. Geometric construction of the solution for a simple example.
For any " x that we pick, the set f(a+ ffia)"xg describes a disc of center a"x and radius
j"x. This is indicated in the figure by the largest rightmost circle, which corresponds
to a choice of a positive "
x that is larger than one. The vector in f(a + ffia)"xg that
is furthest away from b is the one obtained by drawing a line from b through the
center of the rightmost circle. The intersection of this line with the circle defines a
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
residual vector r 3 whose norm is the largest among all possible residual vectors in the
set f(a ffia)"xg.
Likewise, if we draw a line from b that passes through the vertex of a, it will
intersect the circle at a point that defines a residual vector r 2 . This residual will have
the largest norm among all residuals that correspond to the particular choice "
More generally, any "
x that we pick will determine a circle, and the corresponding
largest residual is obtained by finding the furthest point on the circle from b. This is
the point where the line that passes through b and the center of the circle intersects
the circle on the other side of b.
We need to pick an "
x that minimizes the largest residual. For example, it is clear
from the figure that the norm of r 3 is larger than the norm of r 2 . The claim is that
in order to minimize the largest residual we need to proceed as follows: we drop a
perpendicular from b to the lower tangent line denoted by ' 1 . This perpendicular
intersects the horizontal line in a point where we draw a new circle (the leftmost
circle) that is tangent to both ' 1 and ' 2 . This circle corresponds to a choice of " x
such that the furthest point on it from b is the foot of the perpendicular from b to ' 1 .
The residual indicated by r 1 corresponds to the desired solution (it has the minimum
norm among the largest residuals).
To verify this claim, we refer to Fig. 2.4 where we have only indicated two circles;
the circle that leads to a largest residual that is orthogonal to ' 1 and a second circle
to its left. For this second leftmost circle, we denote its largest residual by r 4 . We
also denote the segment that connects b to the point of tangency of this circle with ' 1
by r. It is clear that r is larger than r 1 since r and r 1 are the sides of a right triangle.
It is also clear that r 4 is larger than r, by construction. Hence, r 4 is larger than r 1 .
A similar argument will show that r 1 is smaller than residuals that result from circles
to its right.
The above argument shows that the minimizing solution can be obtained as fol-
lows: drop a perpendicular from b to ' 1 . Pick the point where the perpendicular
meets the horizontal line and draw a circle that is tangent to both ' 1 and ' 2 . Its
radius will be j"x, where "
x is the optimal solution. Also, the foot of the perpendicular
on ' 1 will be the optimal " b.
The projection " b (and consequently the solution " x) will be nonzero as long as b
is not orthogonal to the direction ' 1 . This imposes a condition on j. Indeed, the
direction ' 1 will be orthogonal to b only when j is large enough. This requires that
the circle centered around a has radius a T b, which is the length of the projection of
a onto the unit norm vector b. This is depicted in Fig. 2.5.
Hence, the largest value that can be allowed for j in order to have a nonzero
solution "
x is
Indeed, if j were larger than or equal to this value, then the vector in the set (a
that would always lead to the maximum residual norm is the one that is orthogonal
to b, in which case the solution will be zero again. The same geometric argument will
lead to a similar conclusion had we allowed for uncertainties in b as well.
For a non-unity b, the upper bound on j would take the form
We shall see that in the general case a similar bound holds, for nonzero solutions, and
6 CHANDRASEKARAN, GOLUB, GU, AND SAYED
r
Fig. 2.4. Geometric construction of the solution for a simple example.
a
Fig. 2.5. Geometric condition for a nonzero solution.
is given by
We now proceed to an algebraic solution of the min-max problem. A final statement
of the form of the solution is given further ahead in Sec. 3.4.
3. Reducing the Minimax Problem to a Minimization Problem. We
start by showing how to reduce the min-max problem (2.4) to a standard minimization
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
problem. To begin with, we note that
which provides an upper bound for k
. But this upper bound
is in fact achievable, i.e., there exist (ffiA; ffib) for which
To see that this is indeed the case, choose ffiA as the rank one matrix
and choose ffib as the vector
For these choices of perturbations in A and b, it follows that
are collinear vectors that point in the same direction. Hence,
which is the desired upper bound. We therefore conclude that
which establishes the following result.
Lemma 3.1. The min-max problem (2.4) is equivalent to the following minimization
problem. Given A 2 R m\Thetan , with m  n, nonnegative real numbers
possible, an " x that solves
3.1. Solving the Minimization Problem. To solve (3.2), we define the cost
function
It is easy to check that L("x) is a convex continuous function in "
x and hence any local
minimum of L("x) is also a global minimum. But at any local minimum of L("x), it
either holds that L("x) is not differentiable or its gradient 5L("x) is 0. In particular,
note that L("x) is not differentiable only at " and at any "
x that satisfies A"x
8 CHANDRASEKARAN, GOLUB, GU, AND SAYED
We first consider the case in which L("x) is differentiable and, hence, the gradient
of L("x) exists and is given by
A T A+ ffI
where we have introduced the positive real number
By setting we obtain that any stationary solution " x of L("x) is given by
We still need to determine the parameter ff that corresponds to "
x, and which is defined
in (3.3).
To solve for ff, we introduce the singular value decomposition (SVD) of A:
where U 2 R m\Thetam and V 2 R n\Thetan are orthogonal, and
nal, with
being the singular values of A. We further partition the vector U T b into
m\Gamman .
In this case, the expression (3.4) for "
x can be rewritten in the equivalent form
and hence,
Likewise,
ff
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
which shows that
Therefore, equation (3.3) for ff reduces to the following nonlinear equation that is
only a function of ff and the given data (A; b; j),
Note that only the norm of b 2 , and not b 2 itself, is needed in the above expression.
Remark. We have assumed in the derivation so far that A is full rank. If this were not
the case, i.e., if A (and hence \Sigma) were singular, then equation (3.8) can be reduced to
an equation of the same form but with a non-singular \Sigma of smaller dimension. Indeed,
if we partition
k be the first k components of b 1 ;
n\Gammak be the last components of b 1 ; and let
Then equation (3.8) reduces to
r
the same form as (3.8). From now on, we assume that A is full rank and, hence, \Sigma is
A full rank is a standing assumption in the sequel :
3.2. The Secular Equation. Define the nonlinear function in ff,
It is clear that ff is a positive solution to (3.8) if, and only if, it is a positive root of
G(ff). Following [4], we refer to the equation
as a secular equation.
The function G(ff) has several useful properties that will allow us to provide
conditions for the existence of a unique positive root ff. We start with the following
result.
Lemma 3.2. The function G(ff) in (3.10) can have at most one positive root. In
ff ? 0 is a root of G(ff), then "
ff is a simple root and G 0 ("ff) ? 0.
Proof. We prove the second conclusion first. Partition
where the diagonal entries of \Sigma 1 2 R k\Thetak are those of \Sigma that are larger than j, and the
diagonal entries of \Sigma 2 2 R (n+1\Gammak)\Theta(n+1\Gammak) are the remaining diagonal entries of \Sigma
and one 0. It follows that (in terms of the 2\Gammainduced norm for the diagonal matrices
for all ff ? 0.
Let u 2 R k be the first k components of
the last components of
It follows that we can rewrite G(ff) as the difference
and, consequently,
ff ? 0 be a root of G(ff). This means that
ffI
ffI
which leads to the following sequence of inequalities:
ffI
ffI
ffI
ffI
ffI
ffI
ffI
Combining this relation with the expression for G 0 (ff), it immediately follows that
ff must be a simple root of G(ff).
Furthermore, we note that G(ff) is a sum of n rational functions in ff and
hence can have only a finite number of positive roots. In the following we show by
contradiction that G(ff) can have no positive roots other than "
ff. Assume to the
contrary that "
ff 1 is another positive root of G(ff). Without loss of generality, we
further assume that "
ff 1 and that G(ff) does not have any root within the open
It follows from the above proof that
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
But this implies that G(ff) ? 0 for ff slightly larger than "
ff and G(ff) ! 0 for ff slightly
smaller than "
consequently, G(ff) must have a root in the interval ("ff; "
contradiction to our assumptions. Hence G(ff) can have at most one positive root.
Now we provide conditions for G(ff) to have a positive root. [The next result was
in fact suggested earlier by the geometric argument of Fig. 2.3]. Note that A"x can be
written as
Therefore solving possible, is equivalent to solving
This shows that a necessary and sufficient condition for b to belong to the column
span of A is b
Lemma 3.3. Assume j ? 0 (a standing assumption) and b 2 6= 0, i.e., b does
not belong to the column span of A. Then the function G(ff) in (3.10) has a unique
positive root if, and only if,
Proof. We note that
lim
and that
lim
First we assume that condition (3.13) holds. It follows then that G(ff) changes
sign on the interval (0; +1) and therefore has to have a positive root. By Lemma 3.2
this positive root must also be unique.
On the other hand, assume that
This condition implies, in view of (3.14), that G(ff) ! 0 for sufficiently large ff. We
now show by contradiction that G(ff) does not have a positive root. Assume to the
contrary that "
ff is a positive root of G(ff). It then follows from Lemma 3.2 that G(ff)
is positive for ff slightly larger than "
ff since G 0 ("ff) ? 0, and hence G(ff) must have a
root in ("ff; +1), which is a contradiction according to Lemma 3.2. Hence G(ff) does
not have a positive root in this case.
Finally, we consider the case
We also show by contradiction that G(ff) does not have a positive root. Assume to
the contrary that "
ff is a positive root of G(ff). It then follows from Lemma 3.2 that
ff must be a simple root, and a continuous function of the coefficients in G(ff). In
ff is a continuous function of j. Now we slightly increase the value of j so
that
By continuity, G(ff) has a positive root for such values of j, but we have just shown
that this is not possible. Hence, G(ff) does not have a positive
root in this case either.
We now consider the case b lies in the column span of A. This case
arises, for example, when A is a square invertible matrix
and
It follows from b
Now note that
Therefore, by using the Cauchy-Schwarz inequality, we have
and we obtain, after applying the Cauchy-Schawrtz inequality one more time, that
Lemma 3.4. Assume j ? 0 (a standing assumption) and b lies in
the column span of A. Then the function G(ff) in (3.10) has a positive root if, and
only, if
Proof. It is easy to check that
lim
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
and that
lim
lim
Arguments similar to those in the proof of Lemma 3.3 show that G(ff) does not have
a positive root. Similarly G(ff) does not have a positive root if
arguments similar to those in the proof of Lemma 3.3 show that G(ff) does not have
a positive root if
However, if
lim
Hence G(ff) must have a positive root. By Lemma 3.2 this positive root is unique.
3.3. Finding the Global Minimum. We now show that whenever G(ff) has a
positive root "
ff, the corresponding vector " x in (3.4) must be the global minimizer of
L("x).
Lemma 3.5. Let "
ff be a positive root of G(ff) and let " x be defined by (3.4) for
ff.
x is the global minimum of L("x).
Proof. We first show that
where 4L("x) is the Hessian of L at " x. We take the gradient of L,
Consequently,
A T A \GammakA"x \Gamma bk 3\Gamma
k"xk 3"
We now simplify this expression. It follows from (3.4) that
ffI
and hence
Substituting this relation into the expression for the Hessian matrix 4L("x), and
simplifying the resulting expression using equation (3.3), we obtain
ffI
x
14 CHANDRASEKARAN, GOLUB, GU, AND SAYED
Observe that the matrix
ffI
is positive definite since "
can have at most one non-positive eigenvalue. This implies that 4L("x) is
positive definite if and only if det (4L("x)) ? 0. Indeed,
det
ffI
x
x
ffI
x
The last expression can be further rewritten using the SVD of A and (3.8):
det
x
ffI
x
ffI
="
x
ffI
x
ffI
ff
x
ffI
Comparing the last expression with the function G(ff) in (3.10), we finally have
det
ff
By Lemma 3.2, we have that G 0 ("ff) ? 0. Consequently, 4L("x) must be positive
definite, and hence " x must be a local minimizer of L("x). Since L("x) is a convex
function, this also means that " x is a global minimizer of L("x).
We still need to consider the points at which L("x) is not differentiable. These
include " any solution of
Consider first the case b 2 6= 0. This means that b does not belong to the column
span of A and, hence, we only need to check "
follows from Lemma 3.3 that G(ff) has a unique positive root "
ff and it follows from
Lemma 3.5 that
ffI
is the global minimum. On the other hand, if condition (3.13) does not hold, then it
follows from Lemma 3.3 that G(ff) does not have any positive root and hence
is the global minimum.
Now consider the case b which means that b lies in the column span of
A. In this case L("x) is not differentiable at both "
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
condition (3.16) holds, then it follows from Lemma 3.4 that G(ff) has a unique positive
ff and it follows from Lemma 3.5 that
ffI
is the global minimum. On the other hand, if
where we have used the Cauchy-Schwartz inequality. It follows that
is the global minimum in this case. Similarly, if j   2 , then
is the global minimum.
We finally consider the degenerate case  j. Under this condition, it
follows from (3.15) that
Hence,
This shows that L
L(0). But since L("x) is a convex function in "
x, we
conclude that for any "
x that is a convex linear combination of 0 and
we also obtain Therefore, when  there are many solutions " x
and these are all scaled multiples of V \Sigma as in (3.17).
3.4. Statement of the Solution of the Constrained Min-Max Problem.
We collect in the form of a theorem the conclusions of our earlier analysis.
Theorem 3.6. Given A 2 R m\Thetan , with m  n and A full rank, b
nonnegative real numbers (j; j b ). The following optimization problem:
min
x
always has a solution "
x. The solution(s) can be constructed as follows.
ffl Introduce the SVD of A,
where U 2 R m\Thetam and V 2 R n\Thetan are orthogonal, and
is diagonal, with
being the singular values of A.
ffl Partition the vector U T b into
m\Gamman .
ffl Introduce the secular function
and
First case: b does not belong to the column span of A.
1. If j   2 then the unique solution is "
2. If j !  2 then the unique solution is "
is the
unique positive root of the secular equation
Second case: b belongs to the column span of A.
1. If j   2 then the unique solution is "
2. If  then the unique solution is "
ff is
the unique positive root of the secular equation
3. If j   1 then the unique solution is "
4. If then there are infinitely many solutions that are given by
The above solution is suitable when the computation of the SVD of A is feasible.
For large sparse matrices A, it is better to reformulate the secular equation as follows.
Squaring both sides of (3.3) we obtain
After some manipulation, we are led to
\Theta

where we have defined Therefore, finding ff reduces to finding
the positive-root of
\Theta

In this form, one can consider techniques similar to those suggested in [5] to find ff
efficiently.
A NEW METHOD FOR PARAMETER ESTIMATION WITH UNCERTAIN
4. Restricted Perturbations. We have so far considered the case in which all
the columns of the A matrix are subject to perturbations. It may happen in practice,
however, that only selected columns are uncertain, while the remaining columns are
known precisely. This situation can be handled by the approach of this paper as we
now clarify.
Given A 2 R m\Thetan , we partition it into block columns,
\Theta

and assume, without loss of generality, that only the columns of A 2 are subject to
perturbations while the columns of A 1 are known exactly. We then pose the following
Given A 2 R m\Thetan , with m  n and A full rank, b 2 R m , and nonnegative real
numbers (j
x
\Theta

If we partition "
x accordingly with A 1 and A 2 , say
then we can write
\Theta

Therefore, following the argument at the beginning of Sec. 3, we conclude that the
maximum over (ffiA 2 ; ffib) is achievable and is equal to
In this way, statement (4.1) reduces to the minimization problem
min
\Theta
This statement can be further reduced to the problem treated in Theorem 3.6 as
follows. Introduce the QR decomposition of A, say
R 11 R 12
where we have partitioned R accordingly with the sizes of A 1 and A 2 . Define4
Then (4.2) is equivalent to
min
R 11 R 12
\Gamma4
which can be further rewritten as
min
This shows that once the optimal "
x 2 has been determined, the optimal choice for "
is necessarily the one that annihilates the entry R
That is,

The optimal "
x 2 is the solution of
R 22
This optimization is of the same form as the problem stated earlier in Lemma 3.1
with " x replaced by "
replaced by j 2 , A replaced by
R 22
, and b replaced by
Therefore, the optimal "
x 2 can be obtained by applying the result of Theorem 3.6.
Once "
x 2 has been determined, the corresponding " x 1 follows from (4.5).
5. Conclusion. In this paper we have proposed a new formulation for parameter
estimation in the presence of data uncertainties. The problem incorporates a-priori
bounds on the size of the perturbations and admits a nice geometric interpretation. It
also has a closed form solution that is obtained by solving a regularized least-squares
problem with a regression parameter that is the unique positive root of a secular
equation.
Several other interesting issues remain to be addressed. Among these, we state
the following:
1. A study of the statistical properties of the min-max solution is valuable for a
better understanding of its performance in stochastic settings.
2. The numerical properties of the algorithm proposed in this paper need also
be addressed.
3. Extensions of the algorithm to deal with perturbations in submatrices of A
are of interest and will be studied elsewhere.
We can also extend the approach of this paper to other variations that include
uncertainties in a weighting matrix, multiplicatives uncertainties, etc (see, e.g., [15]).



--R

Robust solutions to least-squares problems with uncertain data
Some modified matrix eigenvalue problems
An analysis of the total least squares problem

Generalized cross-validation for large scale problems
Linear estimation in Krein spaces - Part I: Theory
The Total Least Squares Problem: Computational Aspects and Analysis
Fundamentals of
Filtering and smoothing in an H 1
Society for Industrial and Applied Mathematics

Fundamental Inertia Conditions for the Minimization of Quadratic Forms in Indefinite Metric Spaces


"Parameter estimation in the presence of bounded modeling errors,"
--TR

--CTR
Arvind Nayak , Emanuele Trucco , Neil A. Thacker, When are Simple LS Estimators Enough? An Empirical Study of LS, TLS, and GTLS, International Journal of Computer Vision, v.68 n.2, p.203-216, June 2006
Mohit Kumar , Regina Stoll , Norbert Stoll, Robust Solution to Fuzzy Identification Problem with Uncertain Data by Regularization, Fuzzy Optimization and Decision Making, v.3 n.1, p.63-82, March 2004
Pannagadatta K. Shivaswamy , Chiranjib Bhattacharyya , Alexander J. Smola, Second Order Cone Programming Approaches for Handling Missing and Uncertain Data, The Journal of Machine Learning Research, 7, p.1283-1314, 12/1/2006
Ivan Markovsky , Sabine Van Huffel, Overview of total least-squares methods, Signal Processing, v.87 n.10, p.2283-2302, October, 2007
