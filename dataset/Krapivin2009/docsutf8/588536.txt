--T
Integral Operators on Sparse Grids.
--A
In this paper we are concerned with the construction and use of wavelet approximation spaces for the fast evaluation of integral expressions. The spaces are based on biorthogonal anisotropic tensor product wavelets.  We introduce sparse grid (hyperbolic cross) approximation spaces which are adapted not only to the smoothness of the kernel but also to the norm in which the error is measured. Furthermore, we introduce compression schemes for the corresponding discretizations. Numerical examples for the Laplace equation with Dirichlet boundary conditions and an additional integral term with a smooth kernel demonstrate the validity of our theoretical results.
--B
Introduction
. A naive Galerkin discretization of an integral operator
Z
with global kernel K leads to a dense stiness matrix. Hence, on a uniform full grid
with O(2 nJ ) unknowns (n dimension, J maximal level in a multiscale discretization),
the discrete operator has O(2 2nJ ) entries. This makes matrix vector multiplications,
as they are needed in iterative methods, prohibitively expensive for large n or large J .
A standard strategy to reduce this cost is to exploit the decay properties of the kernel
that, for singular kernels, are typically of the form
@
x @
y K(x; y)
C
x y
f(;)
Such decay properties do hold for pseudo-dierential operators [24]. Together with
(isotropic) biorthogonal wavelets with a su-cient number of vanishing moments, most
entries in the corresponding stiness matrices are then close to zero and can be replaced
by zero without destroying the order of approximation [4, 9, 12].
Another approach to reduce the cost of the integral evaluation is to replace the full
grid approximation space by the so-called sparse grid space [30]. The idea is to use
an anisotropic tensor-product basis together with a specic subset of the full grid
approximation space. It has been shown that under some assumptions on the approximation
and smoothness properties of the underlying basis functions and provided
that certain additional regularity assumptions are fullled, the resulting approximation
spaces exhibit the same order of approximation as the full grid space, while having
less dimension, see for example [32].
The sparse grid approach (that also appeared under the names hyperbolic cross approximation
or boolean blending schemes) is well established in approximation and
interpolation theory, see e.g. [1, 11, 13, 30, 33, 34]. First approaches to use sparse
grids for integral operators can be found in [16, 18, 20, 27]. In [20] it has been observed
that a discretization with adaptive sparse grid spaces leads to good approximation
rates for the single layer potential equation on a square. In [18] theoretical results
on the discretization of elliptic operators of arbitrary order with sparse grids and
biorthogonal wavelets have been presented. In both papers sparse grids are used as
test- and ansatz-spaces in a Galerkin discretization. Hence, without further com-
pression, the resulting discrete systems are dense. This rises the question, whether
additional compression schemes along the lines of [10, 29] are applicable for this type
of tensor-product discretizations.
For integral operators with smooth kernel, sparse grids may be used directly for the
approximation of the kernel. The theoretical background to this approach is that the
smoothness of the kernel translates into decay properties of its multiscale coe-cients.
Then one can construct (modications of) sparse grid spaces such that the evaluation
of integral expressions of the type (1.1) can be performed with much lower cost but
the same accuracy as with the full grid approximation space. This approach has
already been used for complexity estimates of Fredholm integral equations of the
second kind with smooth kernels for the special case with isotropic Sobolev smoothness
or dominating mixed smoothness for K und u [15, 16, 27].
In this paper we develop sparse grid type approximation spaces for the approximate
evaluation of (1.1). We start with smoothness classes including isotropic Sobolev
spaces as well as spaces of dominating mixed derivative, and construct sparse grid
type approximation spaces for the kernel that are adapted to the smoothness of K
and u from (1.1), as well as the norm in which the error of the integral evaluation is
measured. Moreover we develop compression schemes for anisotropic tensor-product
discretizations for kernels that obey (1.2). Note that although these two types of
compression are based on dierent properties of the kernel, it is instructive to present
both approaches in a somewhat unied framework.
The remainder of the paper is as follows. Section 2 introduces the necessary notation
and summarizes the basic facts about biorthogonal wavelet bases, tensor-product
spaces, norm equivalences and the smoothness classes we consider. They are certain
intersections of classes of functions with dominating mixed derivatives, see (2.1) below.
In Section 3 we investigate strategies for the fast evaluation of (1.1). We introduce our
new approximation spaces and give estimates on the order of approximation obtained
with these spaces and discuss compression schemes for anisotropic tensor-product
discretizations for kernels that obey (1.2). In Section 4 we present numerical examples
for the integral evaluation and for the numerical solution of the Laplace equation with
Dirichlet boundary conditions and an additional integral term with smooth kernel.
2. Preliminaries. Multi-indices (vectors) are written boldface, for example
(j Inequalities like l  t or l  0 are to be understood componentwise. We
exist independent of any parameters x or y may depend on
such that C 1  y  x  C 2  y: In the rest of the paper C denotes a generic constant
which may depend on the smoothness assumptions and on the dimension n of the
problem. Moreover, for
By dist(x; y) we denote the
euclidian distance between x and y.
2.1. Sobolev spaces. Let us denote by H t (I n ); t 2 IR, a scale of standard
Sobolev spaces on I the space of L 2 -integrable functions
on I n .
Now, we dene the Sobolev spaces H t;l
mix . They x the smoothness assumptions we
consider.
Definition 1. Let t 2 IR; l 2
the
i-th unit-vector in IR n . Then we dene
mix
mix
mix
mix
H kn (I) for k 2 IR n . For l < 0 we dene H t;l
mix
as dual of H t; l
mix
This denition includes the class of functions of dominating mixed derivative H t;0
mix
as well as the standard isotropic Sobolev spaces H t
mix additional
information on these spaces, specically on the denition via Fourier transform, the
treatment of boundary conditions and the connections between spaces of bounded
mixed derivative and these anisotropic tensor-product spaces see [18, 22] and [21].
When the meaning is clear from the context, we will sometimes drop I n in H t;l
mix
and write H t;l
mix .
2.2. Biorthogonal wavelet bases. The approximation spaces considered here
stem from anisotropic tensor-products of univariate function spaces. We start from
a one-dimensional multi-resolution analysis
and we assume that the
complement spaces W are spanned by some multiscale
basis functions such that we have W
set dened from the subdivision rate of successive renement levels. We will consider
dyadic renement throughout the paper. Moreover, we assume that f jk
forms a Riesz-basis of W j and that there exists a dual system f ~
such that
holds. We assume k jk k L . In the following let
jk and ~
jk have N and ~
vanishing moments, respectively. Moreover we write
~
for the complement spaces spanned by the dual wavelets.
Under these conditions every u 2 L 2 has unique expansions
To simplify things we assume in the following the validity of the norm equivalences
where
0 and an analogous relation for the dual
wavelet (with t 2 (
r)). Such two-sided estimates can be inferred from the validity
of direct estimates (estimates of Jackson type) and inverse estimates (Bernstein
inequalities) for the primal and the dual wavelets as a consequence of approximation
theory in Sobolev spaces together with interpolation and duality arguments, see e.g.
[8, 25].
For the higher-dimensional case n > 1, let j 2 IN n be given, and consider the tensor-product
partition with uniform step size 2 j i into the i-th coordinate direction. By
we denote the corresponding tensor-product function spaces W j := W
W jn and ~
~
basis of W j is then given by [k2 j f jk
Under the assumption of the validity of (2.3) and an analogous relation for the dual
wavelet, it is then straightforward to prove the following norm equivalences
mix
r) and
mix
~
r), see [18]. For we regain the (standard) norm equivalences
for the isotropic Sobolev space H t and the Sobolev space with dominating mixed
derivative H t;0
mix . The dierent factors 2 2tjjj 1 and 2 2ljjj 1 in these equivalences re
ect
the dierent smoothness requirements.
One of the merits of the validity of a norm equivalence for H s is the fact that it leads
directly to optimal preconditioning for H s -elliptic problems and hence to fast iterative
methods with convergence rates independent of the number of unknowns, see [8].
Moreover, (2.4) and (2.5) may be used to analyse the approximation of the embedding
and to construct optimized sparse grid type Finite Element approximation
spaces for elliptic variational problems including integral operators [18]. However,
for integral operators the resulting stiness matrices are dense. In the next Section
we develop schemes to avoid the additional cost due to the denseness of the matrices.
3. Tensor-product approximation and compression of integral opera-
tors. The aim of this Section is to e-ciently approximate the coe-cients h jk of
To this end, we represent K and u from (1.1) in the dual and primal basis, respectively,
(j;k)2IN 2n
(l;m)2 jk
a jklm ~
jl (x) ~
where a
jk :=  j   k . Here we assume that eventual boundary conditions are implemented
into the denition of the primal and dual wavelets, compare Section 4.
Given an index set I  IN 2n we consider the approximation space
~
I :=
(j;k)2I
~
~
Typical examples are the full grid spaces
~
~
~
and the sparse grid (or hyperbolic cross) spaces
~
~
~
(3.
Figure

3.1. Index sets corresponding to the full (left) and the sparse grid space (right) with
maximal level in one dimension
from [30]. These are associated with rectangular index sets and with the index sets
respectively, compare Figure 3.1. The dimensions of the full grid and
the sparse grid approximation space are O(2 2nJ ) and O(2 J J 2n 1 ), respectively, see
[14, 30]. Note that the dimension of the sparse grid space compares favourably with
the dimension of the full grid space.
Given an additional index set  comp
jk   jk we dene the following two approximations
of the kernel K,
K I (x; y) :=
(j;k)2I
(l;m)2 jk
a jklm ~
jl (x) ~
km (y)
and
I (x; y) :=
(j;k)2I
(l;m)2 comp
a jklm ~
jl (x) ~
In (3.5) the kernel K is approximated by tensor-product approximation and by setting
all coe-cients a jklm with (j; I to zero. The second approximation step (3.6)
consists of (additionally) setting some of the coe-cients a jklm to zero.
The corresponding integral operators are the uncompressed operator A from (1.1) and
the two operators
A I u(x) :=
Z
K I (x; y)  u(y)dy and A comp
I u(x) :=
Z
I (x; y)  u(y)dy:
Now, to obtain schemes for an e-cient evaluation of the integral expression Au, we
have to choose the index sets I and  comp
jk in such a way, that on the one hand a
good approximation of the integral is obtained, and on the other hand, the cost of the
integral evaluation remains as small as possible. The rest of this Section is devoted
to this task.
3.1. Tensor product approximation. The following Theorem estimates the
approximation error k(A A I )ukH s for a given index set I.
Theorem 1. Let
mix . We assume that the norm
equivalences (2.4) and (2.5) hold. Moreover let the parameters ~
r from (2.4) and
(2.5) be such that ~
holds. Then,
(j;k)62I
mix
mix
Proof: We use the biorthogonality between the dual and the primal wavelets and the
validity of (2.4) and (2.5). It holds
(j;k)62I
(l;m)2 jk
a jklm ~
jl (x) ~
km (y)A
p2o
uop op (y)
(j;k)62I
(l;m)2 jk
a jklm ukm ~
k:(j;k)62I
a jklm ukmA ~
k:(j;k)62I
a jklm ukmA max
(j;k)62I
k:(j;k)62I
(j;k)62I
jklm
km
(j;k)62I
mix
mix
:Now, for a given bound  of the error k(A A I )ukH s we determine minimal index sets
I. To simplify things, we assume in the rest of this Section l; t;
we use in (3.7) for I the index set f(j;
which corresponds to the standard full grid approximation space (3.3) for the kernel
K, we obtain
mix
mix
Note that the order of approximation in (3.8) does not explicitely depend on the
smoothness of u (p and q only restrict the range of l and t). For example for a
kernel with product structure K(x; the smoothness of Au is only
dependent on the smoothness of k 1 .
A closer look on (3.7) and (3.8) reveals that one could discard indices from the full grid
index set without destroying the order of approximation from (3.8). This motivates
the following denition of index sets.
Figure

3.2. Index sets I100 (0; 0;
from left to right.
Definition 2. We introduce the parametrized index sets
I J (s; t; l; p; q) := f(j;
According to (3.2) the related approximation spaces are
~
(j;k)2IJ (s;t;l;p;q)
~
~
When the meaning is clear from the context, we sometimes drop the parameters
(s; t; l; p; q) and write I J instead of I J (s; t; l; p; q). For
we regain the index sets corresponding to the sparse grid
spaces (3.4) and the standard full grid spaces (3.3), respectively, compare Figure 3.2
(rst and second) and Figure 3.1. Additional smoothness in u, that is p > 0 or q > 0,
further reduces the number of entries in the index sets I J . Figure 3.2 (third and
fourth) gives two examples for the one-dimensional case.
Due to the biorthogonality of the primal and dual wavelets, not all coe-cients of the
wavelet representation of u are required for the evaluation of A I u or A comp
I u. For
example, the coe-cients of h IJ := A IJ u read
a
Therefore, we need the coe-cients ukm only if 9k 2 IN I J . In this way, I J
also induces approximation spaces for u,
VUI J :=
k2UI J
where
Similarly, for Au the denition of I J implies the index set
Note that this index set is optimal for the approximation of functions from H t;l
mix if
the error is measured in the H s -norm, compare [18]. Moreover, it depends only on
s l
t .
The following Lemma summarizes the approximation properties of our new approximation
spaces. It is a direct consequence of (3.7) and the denition of I J in (3.9).
Lemma 1. Let under the assumptions of Theorem 1 it holds
mix
Inequality (3.12) shows that the use of the index set I J (s; t; l; p; q) leads to the same
rate of convergence as the use of the full grid index set, compare (3.8), although in
most cases the number of elements in I J (s; t; l; p; q) is much smaller.
Note that the validity of (3.7) and (3.12) may be extended to the maximal range
because of the wider range of the -estimate in
(2.3), see [29] (with eventual additional logarithmic terms in the extremal case and
special cases of the index sets).
If the optimal approximation order is bounded by the approximation order of the
wavelets and not by the smoothness of K or u (i.e. for example for the case t > ~
then the optimal index set I J is obtained by setting
Then, for example for the L 2 -norm, i.e. the optimal index set is I J (0; ~
Together with an estimate of the number of elements in I J (s; t; l; p; q) (which may
be obtained from general results in [14]), it is straightforward to obtain estimates
for the cost of approximating Au. For example in the case t > 0, i.e. for a kernel
with dominating mixed smoothness, the number of elements N needed to obtain an
approximation with bounded by
O( 1
O( 1
O
s l
for s l < 0:
In the case of isotropic smoothness, i.e. the number of elements needed
to obtain an approximation A IJ u with k(A A IJ )ukH s   is bounded by
O( n
O( n
O
2n
s l q
for
The above estimates have to be compared with the corresponding number of elements
when using the full grid approximation spaces.
Hence, for a kernel with dominating mixed smoothness and s > l, an -approximation
may be obtained with a cost that is asymptotically independent of the dimension n,
compare (3.13). For s  l, there appears a slight n-dependence. This is similar to
the approximation of functions from H t;l
mix in H s . Indeed, in this case, the integral
evaluation is asymptotically not more expensive than the approximation of a function
from H t;l
mix , compare [18].
For K and u with isotropic smoothness, the cost is dependent on n, compare (3.14).
But in any case, the integral evaluation with the above scheme is less costly than the
evaluation with the use of full grid approximation spaces. See also [26, 27], where
special cases of this phenomenon have been described previously.
Note however, that without further compression the above sparse grid approach still
leads to dense discrete operators.
3.2. Additional compression. Additional compression may be obtained via a
special choice of the index sets  comp
jk from (3.6). The idea is to drop small entries
in the stiness matrix without destroying the order of approximation. Here, simple
thresholding, i.e. dropping those entries in the stiness matrix that are under a certain
threshold, is in general not feasible, as this approach requires the computation of all
entries. Moreover, this approach does not take into consideration the smoothness of
u and the norm in which the error is measured.
Special compression schemes for isotropic discretizations that take these considerations
into account can be found in [10, 29]. Then for isotropic tensor-product wavelets
on a full grid the number of entries in the stiness matrix after compression is only
of order O(J k 2 Jn ) with some k 2 IR + or even O(2 Jn ). Corresponding investigations
concerning anisotropic tensor-product discretizations seem still to be missing, especially
together with sparse grid type discretizations. Along the lines of [10] we obtain
the following results.
Decay properties of the kernel such as (1.2) introduce decay properties of the stiness
matrix entries. Specically, if the decay property (1.2) holds for all ;   N
Taylor expansion of the kernel together with the cancellation properties of the primal
wavelets shows
for That is, the size of the entries depends
on the distance of the supports of the basis functions lk , l 0 k 0 and on their scales.
The rate of the decay of the entries is governed by the number of vanishing moments.
The following Lemma estimates the error of setting entries in the stiness matrix to
zero.
Lemma 2. Let
I
mix . Moreover let the norm equivalences
(2.4) and (2.5) hold. The parameters ~
r from (2.4) and (2.5) shall full
~
I A comp
I )uk 2
(j;k)2I
(l;m)2 jk n comp
mix
The proof of Lemma 2 is similar to the proof of Theorem 1. It uses again the biorthogonality
of the primal and the dual wavelet and the stability of the dual wavelets in
H s , see (2.5), and of the primal wavelets in H p;q
mix , see (2.4).
Now, we need to dene  comp
jk in such a way that the order of approximation does not
deteriorate because of this compression. Assume for a moment that we have
A I )ukH s ' O(2 RJ ) with some R 2
. Then we require that k(A I A comp
I )ukH s
is of the same order, i.e. k(A I A comp
I )ukH s ' O(2 RJ ). According to Lemma 2 it
is su-cient to require
(j;k)2I
(l;m)2 jk n comp
jklm  O 2 2RJ
The following Theorem tells us how to dene the index sets  comp
jk . Note that the choice
of the  comp
jk has to balance the overall complexity and the error after compression in
an optimal way.
Theorem 2. For (j;
where J := maxfjj; Ig. We dene
comp
Under the above assumptions ((3.15) and Lemma 2) we then have
I A comp
I )ukH s  C  2 RJ  kuk H p;q
mix
Proof: Combination of (3.15), Lemma 2 and the Denition of  comp
jk shows (for
shortness we use the abbreviation
I A comp
I )uk 2
(j;k)2I
mix
C
(j;k)2I
(l;m)2 jk
mix
Now we apply
(l;m)2 jk
I A comp
I )uk 2
(j;k)2I
mix
(j;k)2I1
A  kuk 2
mix
mix
:The resulting stiness matrix will in general be non-symmetric. A symmetric operator
may be derived by taking the maximum of B jk and B kj in the denition of  comp
jk .
At this point we need an estimate of the number of remaining non-zero elements in the
compressed stiness matrix. A full analysis of this problem is presently missing. Note
however, it must be possible to obtain a compression with anisotropic wavelets which
is optimal up to a logarithmic factor: Each compactly supported univariate scaling
function on level j has a representation by O(j) compactly supported univariate
wavelets. Hence, each isotropic tensor product wavelet on level j has a representation
by at most O(j product wavelets. Therefore, the compression
schemes of [10, 29] translate into compression schemes for anisotropic wavelets which
result in stiness matrices with O(J 2n 1 J k 2 Jn ) entries.
4. Numerical examples. In this Section we present two numerical examples
in 2D which show the e-ciency of our scheme for the approximate integral
evaluation and for the solution of integro-dierential equations. Here we use orthogonal
Daubechies wavelets on the interval with
e.g. [7, 19]. Now, there
is a certain coarsest level such that the approximation space V j0 cannot be
further decomposed into V j0 1 and W j0 . In the multivariate setting of our numerical
experiments, this leads to some modications in the denition of the index sets I J ,
U IJ for the approximation of K and u. Here, we use
I J := f(j;
for the optimized scheme and
I f
for the classical full grid scheme. Except for the correction terms 6j 0 and 2j 0 , the
index sets I J and U J correspond to the index sets I J (0; N; 0; N; 0); U I2J (0;N;0;N;0)
which are optimized for smooth K and u (where the order of approximation is limited
only by N) and for k:k L 2-norm estimates of the error. In our experiments K and u
are arbitrarily smooth, see subsections 4.1 and 4.2. Table 4.1 shows the dimension
of the approximation spaces ~
I for the dierent index sets. It holds dim( ~
and dim( ~

Table
Dimension of approximation spaces for and the index sets from (4.1) and (4.2).
In the previous Sections the exact projections K I and
were used for the analysis. However, in numerical applications K I and u I are usually
not explicitly known and have to be approximated by, say, K 0
I and u 0
I . This is
an essential part in applications and needs to be considered also for accuracy and
complexity estimates. For full grid approximation spaces one usually employs tensor
products of one-dimensional quadrature schemes, see [31], to approximate the scalar
products of K and u with the scaling functions on the nest level of renement. In
all our experiments we used a quadrature scheme of order Then, by the
wavelet transform one obtains the coe-cients of K 0
I and u 0
I . For the sparse grid
spaces ~
IJ we use Smolyak's blending scheme [2, 11, 17, 30]. Then, for example, for
mix and U I2J Jg, one can show
I2J
mix
For
mix and the index set I J (0; N; 0; N;
obtain
mix
Note, that this error is of order O(J 2n 1 2 (M+1=2)J=2 ). However, since the
overall error is dominated by k(A A IJ )ukH s and not by the approximation of K IJ
and u I2J . Similar estimates (without the logarithmic terms) hold for the full grid
case.
For the calculation of errors the nodal values of the numerical solution h I and the exact
solution h are computed on a
grid
with mesh size 2 12 for the x- and y-direction.
Then, the exact (semi-) norms kh h I k L 2 and jh h I j H 1 are approximated using the
discrete (semi-) norms
These discrete norms yield accurate estimates of the true norms of the error.
4.1. Integral evaluation. We consider the integral evaluation (1.1) with
K(x; y) := sin
Y
and
The exact result is
0:577::: is Euler's constant. Furthermore, Si and
Ci denote the Sinus and Cosinus Integralis. See Figure 4.5 for a plot of u and h.

Figure

4.1 shows the (discrete) L 2 - and H 1 -norms of the error. The new scheme
with index sets (4.1) yields approximately the same accuracy as the classical full
grid scheme { with a much smaller work count, of course. Furthermore, the rate
of error reduction agrees very well with the rate predicted by Theorem 1. Plots of
error vs. number of degrees of freedom are given in Figure 4.2. The L 2 -optimized
scheme leads to superior convergence rates in both norms. In all experiments the
calculation of a 0
jklm was by far the most expensive part of the algorithm. Table 4.2
lists cpu-times for the numerical quadrature, the wavelet transforms involved in the
blending scheme and the matrix-vector product (3.10) for the L 2 -optimized scheme.
The measurements were carried out on a SGI O2000 with a 195 MHz R10k processor.
Due to the blending scheme the numerical work count and, therefore, the cpu-times
are only asymptotically proportional to dim( ~
This explains the growth of the
normalized cpu-time shown in the fth column of Table 4.2.
error
full grid I J
-optimized J
error
full grid I J
-optimized J

Figure

4.1. Discrete L 2 - and H 1 -errors against the maximal level J for the integral evaluation.
degrees of freedom
error
full grid I J
-optimized I J
degrees of freedom
error
full grid I J
-optimized I J

Figure

4.2. Discrete L 2 - and H 1 -errors against dim( ~
I ) for the integral evaluation.

Table
-optimized scheme: cpu-times in [s] for the calculation of the coe-cients a 0
jklm (quadratures
(Q), wavelet transforms (WT)), the multiplications (M) according to (3.10) and cpu-times for Q+
WT and M normalized with the dimensions of ~
I J .
4.2. Solution of integro-dierential equations. We consider the numerical
solution of the integro-dierential equation
Z
Here, K is given by (4.3) and the right hand side f is chosen such that the exact
solution is (4.4), see Figure 4.5. The trial and test functions for the Galerkin discretization
are Daubechies wavelets with homogeneous Dirichlet boundary conditions
and vanishing moments. A BiCGStab(2) iterative solver was used for the
solution of the resulting linear system. The number of iterations required to reduce
the residual to machine precision was between 40 and 50 (essentially independent of
the maximal level J).
As shown in Figure 4.3 the optimized scheme yields approximately the same accuracy
as the classical scheme and the theoretical convergence rates are matched well. Again
the new scheme leads to dramatically reduced errors when compared to the full grid
scheme, see Figure 4.4.
error
full grid I J
-optimized J
error
full grid I J
-optimized J

Figure

4.3. Discrete L 2 - and H 1 -errors against the maximal level J for the solution of (4.6).
degrees of freedom
error
full grid f
-optimized I J
degrees of freedom
error
full grid f
-optimized I J

Figure

4.4. Discrete L 2 - and H 1 -errors against dim( ~
I ) for the solution of (4.6).0.5010.010.030.50126
Figure

4.5. Left: u, (4.4). Middle: h, (4.5). Right: f , (4.6)

Acknowledgements

: The authors would like to thank the referees for their useful
comments. The second author has been supported by the Deutsche Forschungsge-
meinschaft, GR 1144/7-2.



--R

Approximation by trigonometric polynomials in a certain class of periodic functions of several variables

On the adaptive computation of integrals of wavelets
Fast wavelet transforms and numerical algorithms I


Wavelets on the interval and fast wavelet transforms
Stability of Multiscale Transformations
Wavelet and Multiscale Methods for Operator equations

Theory 34
Compression of Wavelet decompositions
Constructive Approximation 14
Number of integral points in a certain set and the approximation of functions of several variables
Complexity of local solution of multivariate integral equations
Information complexity of multivariate Fredholm equations in Sobolev classes
Numerical Integration using Sparse Grids
Optimized tensor-product approximation spaces
Orthogonal Wavelets on the Interval


Hyperbolic cross approximation of integral operators with smooth kernel
Approximation und Kompression mit Tensorprodukt-Multiskalen-Raumen
Wavelets: Calderon-Zymund and multilinear operators
On discrete norm estimates related to multilevel preconditioners in the
On the complexity of
On the complexity of
analysis of the combination technique

Quadrature and interpolation formulas for tensor products of certain classes of functions
Quadrature formulae and asymptotic error expansions for wavelet approximations of smooth functions
Approximation of periodic functions

Parallel Algorithms for Partial Di
--TR
