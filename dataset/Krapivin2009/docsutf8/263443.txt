--T
Bias in Robust Estimation Caused by Discontinuities and Multiple Structures.
--A
AbstractWhen fitting models to data containing multiple structures, such as when fitting surface patches to data taken from a neighborhood that includes a range discontinuity, robust estimators must tolerate both gross outliers and pseudo outliers. Pseudo outliers are outliers to the structure of interest, but inliers to a different structure. They differ from gross outliers because of their coherence. Such data occurs frequently in computer vision problems, including motion estimation, model fitting, and range data analysis. The focus in this paper is the problem of fitting surfaces near discontinuities in range data.To characterize the performance of least median of the squares, least trimmed squares, M-estimators, Hough transforms, RANSAC, and MINPRAN on this type of data, the "pseudo outlier bias" metric is developed using techniques from the robust statistics literature, and it is used to study the error in robust fits caused by distributions modeling various types of discontinuities. The results show each robust estimator to be biased at small, but substantial, discontinuities. They also show the circumstances under which different estimators are most effective. Most importantly, the results imply present estimators should be used with care, and new estimators should be developed.
--B
Introduction
Robust estimation techniques have been used with increasing frequency in computer vision
applications because they have proven effective in tolerating the gross errors (outliers) characteristic
of both sensors and low-level vision algorithms. Most often, robust estimators are
used when fitting model parameters - e.g. the coefficients of either a polynomial surface, an
affine motion model, a pose estimate, or a fundamental matrix - to a data set. For these
applications, robust estimators work reliably when the data contain measurements from a
single structure, such as a single surface, plus gross errors.
Sometimes, however, the data are more complicated than this, presenting a challenge to
robust estimators not anticipated in the robust statistics literature. This complication occurs
when the data are measurements from multiple structures while still being corrupted by
gross outliers. These structures may be different surfaces in depth measurements or multiple
moving objects in motion estimation. Here, the difficulty arises because robust estimators
are designed to extract a single fit. Thus, to estimate accurate parameters modeling one of
the structures - which one is not important - they must treat the points from all other
structures as outliers. After successfully estimating the fit parameters of one structure, the
robust estimator may be re-applied, if desired, to estimate subsequent fits after removing
the first fit's inliers from the data.
An example using synthetic range data illustrates the potential problems caused by multiple
structures. Figure 1 shows (non-robust) linear least-squares fits to data from a single
surface and to data from a pair of surfaces forming a step discontinuity. In the single surface
example, the least-squares fit is skewed slightly by the gross outliers, but the points from
the surface are still generally closer to the fit than the outliers. Thus, the fit estimated by
a robust version of least-squares will not be significantly corrupted by these outliers. In the
multiple surface example, the least-squares fit is skewed so much that it crosses (or "bridges")
the point sets from both surfaces, placing the fit in close proximity to both point sets. Since
robust estimators use fit proximity to distinguish inliers and outliers and downgrade the
influence of outliers, this raises two concerns about the accuracy of robust fits. First, an
estimator that iteratively refines an initial least squares fit will have a local, and potentially
a b

Figure

1: Examples demonstrating the effects of (a) gross outliers and (b) both gross outliers
and data from multiple structures on linear least-squares fits.
global, minimum fit that is not far from the initial, skewed fit. This is because points from
both surfaces will have both small and large residuals, making it difficult for the estimator to
"pull away" from one of the surfaces. Second, and more important, for the robust estimate
be the correct fit, thereby treating the points from one surface as inliers and points from the
other as outliers, the estimator's objective function must be lower for the smaller inlier set
of the correct fit than the larger inlier set of the bridging fit. By varying both the proximity
of the two surfaces and the relative sizes of their point sets, all robust estimators studied
here can be made to "fail" on this data, producing fits that are heavily skewed.
Motivated by the foregoing discussion, the goal of this paper is to study how effectively
robust estimators can estimate fit parameters given a mixture of data from multiple struc-
tures. Stating this "pseudo outliers problem" abstractly, to obtain an accurate fit a robust
technique must tolerate two different types of outliers: gross outliers and pseudo outliers.
Gross outliers are bad measurements, which may arise from specularities, boundary effects,
physical imperfections in sensors, or errors in low-level vision computations such as edge
detection or matching algorithms. Pseudo outliers are measurements from one or more additional
structures. (Without losing generality, inliers and pseudo outliers are distinguished by
assuming the inliers are points from the structure contributing the most points and pseudo
outliers are points from the other structures.) The coherence of pseudo outliers distinguishes
them from gross outliers. Because data from multiple structures are common in vision ap-
plications, robust estimators' performance on this type of data must be understood to use
them effectively. Where they prove ineffective, new and perhaps more complicated robust
techniques will be needed. 1
To study the pseudo outliers problem, this paper develops a measure of "pseudo outlier
bias" using tools from the robust statistics literature [10, pages 81-95] [12, page 11]. Pseudo
outlier bias will measure the distance between a robust estimator's fit to a ``target'' distribution
and its fit to an outlier corrupted distribution. The target distribution will model the
distribution of points drawn from a single structure without outliers, and the outlier corrupted
mixture distribution [27] will combine distributions modeling the different structures
and a gross outlier distribution. The optimal fit is found by applying the functional form of
an estimator to these distributions, rather than by applying the estimator's standard form
to particular sets of points generated from these distributions. This gives a theoretical mea-
sure, avoids the need for extensive simulations, and, most importantly, shows the inherent
limitations of robust estimators by studying their objective functions independent of their
search techniques. The bias of a number of estimators - M-estimators [12, Chapter 7], least
median of squares (LMS) [16, 21], least trimmed squares (LTS) [21], Hough transforms [13],
RANSAC [7], and MINPRAN [26] - will be studied as the target and mixture distributions
vary.
The application for studying the pseudo outliers problem is fitting surfaces to range data
taken from the neighborhood of a surface discontinuity. While this is a simple application for
studying the pseudo outliers problem, the problem certainly arises in other applications as
well - essentially any application where the data could contain multiple structures - and
the results obtained here should be used as qualitative predictions of potential difficulties in
these applications. In the context of the range data application, three idealized discontinuity
models are used to develop mixture distributions: step edges, crease edges and parallel
surfaces. Step edges model depth discontinuities, where points from the upper surface of
the step are pseudo outliers to the lower surface. Crease edges model surface orientation
discontinuities, where points from one side of the crease are pseudo outliers to the other.
versions of these techniques actually exist for fitting surfaces to range data. Their effectiveness,
however, depends in part on the accuracy of an initial set of robust fits.
Finally, parallel surfaces model transparent or semi-transparent surfaces, where a background
surface appears through breaks in the foreground surface, and data from the background are
pseudo outliers to the foreground.
A final introductory comment is important to assist in reading this paper. The paper defines
the notion of "pseudo outlier bias" using techniques common in mathematical statistics
but not in computer vision, most importantly, the "functional form" of a robust estimator.
The intuitive meaning of functional forms and their use in pseudo outlier bias are discussed
at the start of Section 4, which then proceeds with the main derivations. Readers uninterested
in the mathematical details should be able to skip Sections 4.2 through 4.6 and still
follow the analysis results.
Robust Estimators
This section defines the robust estimators studied. These definitions are converted to functional
forms suitable for analysis in Section 4. Because the goal of the paper is to expose
inherent limitations of robust estimators, the focus in defining the estimators is their objective
functions rather than their optimization techniques. Special cases of iterative optimization
techniques where local minima are potentially problematic will be discussed where
appropriate.
The data are (~x is an image coordinate vector - the independent vari-
able(s) - and z i is a range value - the dependent variable. Each fit is a function z = '(~x), often
restricted to the class of linear or quadratic polynomials. The notation -
'(~x) indicates the
fit that minimizes an estimator's objective function, with -
' called the "estimate". Each esti-
mator's objective function evaluates hypothesized fits, '(~x), via the residuals, r
2.1 M-Estimators
A regression M-estimate [12, Chapter 7] is
ae(r i;' =-oe); (1)
oe is an estimate of the true scale (noise) term, oe, and ae(u) is a robust "loss" function
which grows subquadratically for large juj to reduce the effect of outliers. (Often, as discussed
' and -
oe are estimated jointly.) M-estimators are categorized into three types [11] by
the behavior of one estimator of each type is studied. Monotone M-estimators
(Figure 2a), such as Huber's [12, Chapter 7], have non-decreasing, bounded /(u) functions.
Hard redescenders (Figure 2b), such as Hampel's [9] [10, page 150], force
hence c is a rejection point, beyond which a residual has no influence. Soft redescenders
(Figure 2c), such as the maximum likelihood estimator of Student's t-distribution [5], do not
have a finite rejection point, but force /(u) ! 0 as juj !1. The three robust loss functions
are shown in Figure 2 and in order they are
(2)
ae h
and
ae s
The ae functions' constants are usually set to optimize asymptotic efficiency relative to a
given target distribution [11] (e.g. Gaussian residuals).
M-estimators typically minimize
using iterative techniques [11] [12, Chapter
7]. The objective functions of hard and soft redescending M-estimators are non-convex and
may have multiple local minima.
In general, - oe must be estimated from the data. Hard-redescending M-estimators often
use the median absolute deviation (MAD) [11] computed from the residuals to an initial fit,
Monotone Hard Soft
ae(u)
(a) (b) (c)

Figure

2: ae(u) and /(u) functions for three M-estimators.
for consistency at the normal distribution and
at Student's t-distribution (when Other M-estimators jointly estimate -
oe and -
' as
';oe
ae(r
In particular, Huber [12, Chapter 7] uses
=oe) is from equation 2 and a is a tuning parameter; Mirza and Boyer [5] use
ae s (r
=oe) is from equation 4.
When fitting surfaces to range data, a different option for obtaining - oe is often used [3].
If oe depends only on the properties of the sensor then - oe may be estimated once and fixed for
all data sets. Theoretically, when - oe is fixed, the M-estimators described by equation 1 are
no longer true M-estimators since they are not scale equivariant [10, page 259]. To reflect
this, when - oe is fixed a priori, they are called "fixed-scale M-estimators." Both standard
M-estimators and fixed-scale M-estimators are studied here.
2.2 Fixed-Band Techniques: Hough Transforms and RANSAC
Hough transforms [13], RANSAC [4, 7], and Roth's primitive extractor [20] are examples of
"fixed-band" techniques [20]. For these techniques, - ' is the fit maximizing the number of
points within ' \Sigma r b , where r b is an inlier bound which generally depends on - oe (i.e. r
some constant c). Equivalently, viewing fixed-band techniques as minimizing the number of
outliers, they become a special case of fixed-scale M-estimators with a simple, discontinuous
loss function
ae f
Fixed-band techniques search for -
using either random sampling or voting techniques.
2.3 LMS and LTS
Least median of squares (LMS), introduced by Rousseeuw [21], finds the fit minimizing the
median of squared residuals. (See [16] for a review.) Specifically, the LMS estimate is
fmedian i
Most implementations of LMS use random sampling techniques to find an approximate
minimum.
Related to LMS and also introduced by Rousseeuw [21] is the least trimmed squares
estimator (LTS). The LTS estimate is
where the (r 2
are the (non-decreasing) ordered squared residuals of fit '. Usually
implementations also use random sampling.
2.4 MINPRAN
MINPRAN searches for the fit minimizing the probability that a fit and a collection of inliers
to the fit could be due to gross outliers [24, 26]. It is derived by assuming that relative to
any hypothesized fit '(x) the residuals of gross outliers are uniformly distributed 2 in the
range \SigmaZ 0 . Based on this assumption, the probability that a particular gross outlier could
be within '(~x i Furthermore, if all n points are gross outliers,
the probability k or more of them could be within '(~x) \Sigma r is
Given n data points containing an unknown number of gross outliers, MINPRAN evaluates
hypothesized fits '(~x) by finding the inlier bound, r, and the associated number of points
(inliers), k r;' , within \Sigmar of '(~x), minimizing the probability that the inliers could actually
be gross outliers. Thus MINPRAN's objective function in evaluating a particular fit is
min r
and MINPRAN's estimate is
[min r
MINPRAN is implemented using random sampling techniques (see [26]).
Modeling Discontinuities
The important first step in developing the pseudo outlier bias analysis technique is to model
the data taken from near a discontinuity as a probability distribution. Attention here is
restricted to discontinuities in one-dimensional structures, since this will be sufficient to
demonstrate the limitations of robust estimators.
3.1 Outlier Distributions
To set the context for developing the distributions modeling discontinuities, consider the one-
dimensional, outlier corrupted distributions used in the statistics literature to study robust
location estimators [10, page 97] [12, page 11]:
2 MINPRAN has been generalized to any known outlier distribution [26].
z
x
x d

Figure

3: Example data set for points near a step discontinuity.
Here, F 1 is an inlier distribution (also called a "target distribution"), such as a unit variance
Gaussian, and G is an outlier distribution, such as a large variance Gaussian or an uniform
distribution over a large interval. The parameter " is the outlier proportion. A set A of N
points sampled from this distribution will contain on average "N outliers. Robust location
estimators are analyzed using distribution F rather than using a series of point sets sampled
from F .
3.2 Mixture Distributions Modeling Discontinuities
The present paper analyzes robust regression estimators by examining their behavior on
distributions modeling discontinuities. These mixture distributions [27] will be of the form
will be inlier, pseudo outlier and gross outlier distributions, respectively, and
control the proportion of points drawn from the three distributions.
To and to set " s and " of data points taken
from the vicinity of a discontinuity. For example, S might be the points in Figure 3 whose
x coordinate falls in the interval [x modeled as a two-dimensional distribution
of points (x; z) with x values in an interval [x losing generality,
more points are from the left side of the discontinuity location than the right. (Using a two-dimensional
distribution could be counterintuitive since the x values, which may be thought
of as image positions at which depth measurements are made, are usually fixed.) Here, x is
treated as uniform in the interval [x modeling the uniform spacing of image positions. 3
The depth measurement for an inlier is z = fi 1 (x)+e, where e is independent noise controlled
by the Gaussian density g(e; oe 2 ) with mean 0 and variance oe 2 . fi 1 (x) models the ideal curve
from which the inliers are measured. The pseudo outlier distribution, H 2 , can be defined
similarly, with x values uniform in [x d
for both distributions H 1 and H 2 , the densities of x and z can be combined to give the joint
density
0; otherwise.
bound the uniform distribution on the x interval.
For the distribution of gross outliers in S, again x values are uniformly distributed,
but this time over the entire interval [x z values are governed by density g o (z),
which will be uniform over a large range. This gives the joint density for a gross outlier:
0; otherwise.
The mixture proportions " s and " in (14) are easily specified. " just the fraction
of gross outliers. " s is the "relative fraction" of inliers, i.e. the fraction points that are not
gross outliers and that are from the inlier side of the discontinuity. Assuming the density of
x values does not change across the discontinuity, " s is determined by x d :
Equivalently, inliers and pseudo outliers,
Notice that the "actual fraction" of inliers is " Depending
on which estimator is being analyzed, either the relative or the actual fraction or both will
be important.
3 For any point set sampled from this distribution, the x values will not be uniformly spaced, in general,
but their expected values are. This expected behavior is captured when using the distribution itself in the
analysis rather than points sets sampled from the distribution.
Using these mixture proportions, the above densities can be combined into a single,
mixed, two-dimensional density:
Observe that the "target density" is just h 1 (x; z) and the "target distribution" is H 1 (x; z).
The mixture distribution H(x; z) and the target distribution H 1 (x; z) can be calculated from
h(x; z) and h 1 (x; z) respectively.
Using mixture density h(x; z), data can be generated to form step edges and crease edges.
The appropriate model is determined by the two curve functions fi 1 and fi 2 . For example, a
step edge of height \Deltaz is modeled by setting fi 1
c. A crease edge is modeled when fi 1 and fi 2 are linear functions and
lines with overlapping x domains can be created by using fi 1 and fi 2 from step edges, but
setting x proportion of points
from the lower line. In this case, the mixture proportions are divorced from the location
of the discontinuity, which has no meaning. Thus, all three desired discontinuities can be
modeled.
4 Functional Forms and Mixture Models
To analyze estimators on distributions H, each estimator must be rewritten as a functional,
a mapping from the space of probability distributions to the space of possible estimates.
This section derives functional forms of the robust estimators defined in Section 2. It
starts, in Section 4.1 by giving intuitive insight. Then, Section 4.2 introduces functional
forms and empirical distributions on a technical level, using univariate least-squares location
estimates as an example. Next, Section 4.3 derives several important distributions needed in
the functionals. The remaining sections derive the required functionals. Readers uninterested
in the technical details should read only Section 4.1 and then skip ahead to Section 5.
4.1 Intuition
To illustrate what it means for a functional T to be applied to a distribution H, consider
least-squares regression. When applied to a set containing points
objective function is
i;' , which is proportional to the second moment
of the residuals conditioned on ', and the least squares estimate is the fit -
minimizing
this conditional second moment. A similar second moment, conditioned on ', may be calculated
for distribution H(x; z), and the fit -
' minimizing this conditional second moment
may be found. This is the least-squares regression functional. The functional form of an
M-estimator, by analogy, returns the fit minimizing a robust version of the second moment
of the conditional residual distribution calculated from H. Intuitions about the functional
forms of other estimators are similar.
The estimate T (H) can be used to represent or characterize the estimator's performance
on point sets sampled from H. Although the robust fit to any particular point set may differ
from T (H), if T (H) is skewed by the pseudo and gross outliers, then the fit to the point
set will likely be skewed as well. Indeed, when an estimator's minimization technique is an
iterative search, the skew may be worse than that of T (H) because it may stop at a local
minimum.
4.2 One-Dimensional Location Estimators
To introduce functional forms on a more technical level, this section examines the least-squares
location estimate for univariate data. For a finite sample fx g, the location
estimate is
'n
which is the sample mean or expected value. The functional form of this is the location
estimate of the distribution F from which the x i 's are drawn:
Z
Z
Z
the population mean or expected value.
The functional form of the location estimate is derived from the sample location estimate
by writing the latter in terms of the "empirical distribution" of the data, denoted by F n , and
then replacing F n with F , the actual distribution. The empirical density of fx is
where ffi(\Delta) is the Dirac delta function, and the empirical distribution is
where u(\Delta) is the unit step function. When the x i 's are independent and identically dis-
converges to F as n ! 1. The least squares location estimate is written in
terms of the empirical density by using the sifting property of the delta function [8, page 56]:
argmin
'n
'n
Z
Z
Z
Replacing f n with the population density yields the functional form of the
location estimate as desired (20).
4.3 Residual Distributions and Empirical Distributions
Before deriving functional forms for the robust regression estimators, the mixture distribution
H(x; z) must be rewritten in terms of the distribution of residuals relative to a hypothesized
fit, '. This is because the estimators' objective functions depend directly on residuals r
and only indirectly on points (x; z). In addition, several empirical versions of this "residual
distribution" are needed.
Two different residual distributions are required: one for signed residuals and one for
their absolute values. Let the distribution and density of signed residuals be F s (rj'; H) and
(including H in the notation to make explicit the dependence on the mixture
distribution). These are easily seen to be (Figure 4a)
F s
Z '(x)+r
a b

Figure

4: The cumulative distribution of residual r relative to fit '(x) is the integral of the
point densities, h 1 and h 2 , from the curves and from the gross outlier density, h over the
region bounded above by '(x) bounded on the sides by
(a) unbounded below for signed residuals or (b) bounded below by '(x) \Gamma r for absolute
residuals. Both figures show the region of integration for functions fi i and x boundaries
modeling a step edge.
and
Let the distribution and density of absolute residuals be F a (rj'; H) and f a (rj'; H), where
r - 0. These are (Figure 4b)
F a
Z '(x)+r
and
f a


Appendix

A evaluates these integrals. Replacing h with h 1 in the above equations yields the
residual distributions and densities for the target (inlier) distribution.
several empirical distributions are needed below. First, given n points
sampled from h(x; z), the empirical density of the data is simply
should not be confused with h i from equation 15). Next, the empirical density of the
signed residuals follows from h n (x; z) using the sifting property of the ffi function [8, page 56]:
\Gamma1n
=n
Finally, the empirical distribution of the absolute residuals is
F a
Z r
\Gammar
4.4 M-Estimators and Fixed-Band Techniques
The functionals for the robust regression estimators can now be derived, starting with that
of fixed-scale M-estimators. The first step is to write equation 1 in a slightly modified form,
which does not change the estimate:
'n
ae(r i;' =-oe);
Next, writing this in terms of the empirical distribution produces
argmin
'n
ae(r i;'
'n
'n
ZZ
ZZ
Replacing the empirical density h n (x; z) with the mixture density h(x; z) yields
T ae
ZZ
The change of variables simplifies things further,
T ae
Z
ae(r=-oe)
Z
Z
This is the fixed-scale M-estimator functional. Substituting equations 2, 3 and 4 gives
respectively for the M-estimators studied here.
For the M-estimators that jointly estimate -
' and - oe (see equations 7 and 8), the functional
is obtained by replacing ae(r=-oe) with ae(r; oe) in equation 27, producing
T ae;s
Z
Finally, recalling that fixed-band techniques are special cases of fixed-scale M-estimators,
their functional is obtained by substituting equation 9 into equation 27, yielding
T b
Observe that [1 \Gamma F a (r b j'; H)] is the expected fraction of outliers.
4.5 LMS and LTS
Deriving the functional equivalent to LMS requires first deriving the cumulative distribution
of the squared residuals and then writing the median in terms of the inverse of this
distribution. Defining the empirical distribution of squared residuals is
F n;y
since it is simply the percentage of points whose absolute residuals relative to fit ' are less
than
y. Now,
n;y (1=2j'; h); (30)
In other words, the median is the inverse of the cumulative, evaluated at 1=2. 4 This is the
standard functional form of the median [10, page 89]. Substituting equation
4 When LMS is implemented using random sampling where p points are chosen to instantiate a fit, the
median residual is taken from among the remaining points. To reflect this, the 1=2 in equation
could be replaced by (n
replacing the empirical distribution F n;y with F y
produces the LMS
y
Turning now to LTS, normalizing its objective function and writing it in terms of the
empirical density of residuals yieldsn
Z rm
n;y (1=2j'; H n ) is the empirical median square residual. The functional form of
LTS then is easily written as
T T
\GammaF
4.6 MINPRAN
MINPRAN's functional is derived by first re-writing MINPRAN's objective function, replacing
the binomial distribution with the incomplete beta function [19, page 229]:
min r
where
\Gamma(v)\Gamma(w)
Z pt
and \Gamma(\Delta) is the gamma function. This is done because I(v; w; p) only requires v; w 2
the binomial distribution requires integer values for k r;' and n. Now, since
F a
is the empirical distribution of the absolute residuals (see equation 26), k
objective function can be re-written equivalently
as
min r
I(n \Delta F a
Replacing F a
n by F a and substituting equation 13 gives the functional
min r
Observe that n, the number of points, is still required here, but TM (H) is considered a
functional [10, page 40].
5 Pseudo Outlier Bias
Now that the functional forms of the robust estimators have been derived, the pseudo outlier
bias metric can be defined. Given a particular mixture distribution H(x; z), target distribution
These fits are assumed to minimize the estimator's objective functional globally. Then,
pseudo outlier bias is defined as the normalized L 2 distance between the fits:
oe 1=2
As is easily shown, this metric is invariant to translation and independent scaling of both x
and z. (For fixed-scale M-estimators, - oe, which is provided a priori, must be scaled as well.
For MINPRAN, the outlier distribution must be scaled appropriately.)
When the set of the possible curves '(x) includes fi 1 (x), it can be shown that for each
of the functionals derived in Section 4, T
In other words, the estimator's
objective function is minimized by fi 1 . 5 When T the pseudo outlier bias metric
becomes
oe 1=2
Intuitively, pseudo outlier bias measures the L 2 norm distance between the two estimates,
T (H) and T (H 1 ), normalized by the length of the x interval over which H(x; z) is non-zero
and by the standard deviation of the noise in the z values. Since T (H 1 for the cases
studied here, a metric value of 0 implies that T is not at all corrupted by the presence of
either gross or pseudo outliers, and a metric value of 1 implies that on average over the x
domain T (H) is one standard deviation away from fi 1 .
5 In the analysis results given in Section 6, the set of curves will be linear functions of the form
will also be linear. These curves are continuous and have infinite extent in x, unlike
the densities modeling data drawn from them.
z
d
s
a a
z
d
Step Creaseb (x)
zx
Parallel

Figure

5: Parameters controlling the curve models for step edges, crease edges, and parallel
lines. In each case, fi 1 (x) is the desired correct fit and points from fi 2 (x) are pseudo outliers.
\Deltaz=oe is the scaled discontinuity magnitude, and " s controls the percentage of points from
6 Bias Caused by Surface Discontinuities
Pseudo outlier bias (or "bias" for short) can now be used to analyze robust estimators' accuracy
in fitting surfaces to data from three different types of discontinuities: step edges, crease
edges, and parallel lines with overlapping x domains. To do this, Section 6.1 parameterizes
the mixture density, outlines the technique to find T (H), and discusses the relationship between
results presented here and results for higher dimensions. Then, analysis results for
specific estimators are presented: fixed-scale M-estimators and fixed-band techniques (Sec-
tion 6.2) which require a prior estimate of -
oe, standard M-estimators (Section 6.3) which
estimate -
oe, and LMS, LTS and MINPRAN (Section 6.4) which are independent of -
oe. In
each case, the bias is examined as both the discontinuity magnitude and mixture of inliers,
pseudo outliers and gross outliers vary.
-5
-T

Figure

Surface plot of the objective functional of T ae h (H), i.e.
R
the hard redescending, fixed-scale M-estimator on a step edge with "
when fits have the form b. (The plot shows the negation of the objective
functional, so local minima in the functional appear as local maxima in the plot.) There
are three local optimal: one at the second at and the third at a
heavily biased fit, 1:91. The biased fit is the global optimum.
6.1 Discontinuity Models and Search

Figure

5 shows the models of step edges, crease edges, and parallel lines. The translation and
scale invariance of both the estimators and pseudo outlier bias, along with several realistic
assumptions, allow these discontinuities to be described with just a few parameters. (Refer
back to Section 3 for the exact parameter definitions.) For all models, and the x
interval is [0; 1]. For step edges, fi 1 retaining the oe parameter
to make clear the scale invariance - and x
these values, " To move from step to crease edges, only the curves fi 1 (x) and fi 2 (x)
must be changed. Referring to Figure 5b, these functions are
no explicit role because it is not scale invariant. For
parallel lines (Figure 5c), fi 1 (x) and fi 2 (x) are the same as for step edges, x
and the parameter x d plays no role. Finally, the outlier distribution g o (z) is
uniform for z within \Sigmaz 0 =2 of \Deltaz=2 and 0 otherwise.
The foregoing shows that the parameters " s , " \Deltaz=oe, and z 0 completely specify a two
surface discontinuity model, the resulting mixture density, h(x; z), and therefore, the distri-
bution, H(x; z). Hence, after specifying the class of functions (linear, here) for hypothesized
fits, a given robust estimator's pseudo outlier bias can be calculated as a function of these
parameters. This calculation requires an iterative, numerical search to minimize T (H), and
may require several starting points to avoid local minima. (See Figure 6 for an example plot
of T ae h
's objective functional.) Thus, for a particular type of discontinuity and for a particular
robust estimator, the parameters may be varied to study their effect on the estimator's
pseudo outlier bias, thereby characterizing how accurately the estimator can fit surfaces near
discontinuities.
As a final observation, although the results are presented for one-dimensional image
domains, they have immediate extension to two dimensions. For example, a two-dimensional
analog of the step edge presented here is fi 1 (x;
It is straightforward to show that this model
results in exactly the same pseudo outlier bias as a one-dimensional step model having
the same mixture parameters and gross outlier distribution. Similar results are obtained
for natural extensions of the crease edge and parallel lines models. Thus, one-dimensional
discontinuities are sufficient to establish limitations in the effectiveness of robust estimators.
6.2 Fixed-Scale M-Estimators and Fixed-Band Techniques
The first analysis results are for fixed-band techniques and fixed-scale M-estimators. These
techniques represent an ideal case where the noise parameter - oe = oe is known and fixed in
advance. Figure 7 shows the bias of fixed-band techniques (T F ) and three fixed-scale M-estimators
) as a function \Deltaz=oe when " 0:8. The
bias of the least-squares estimator, calculated by substituting
is included for comparison. The ae function tuning parameters values are directly from the
literature page 167], and
. Interestingly, the proportion of gross outliers, "
has no effect on the results. This is because the fraction of the outlier distribution within
r of a fit is the same for all fits ' and for all r except when '(x) \Sigma r is extreme enough to
cross outside the bounds of the gross outlier distribution.
The sharp drops in bias shown in Figure 7 (a) and (b) for fixed-band techniques and the
hard redescending M-estimator (and to some extent for the soft redescending M-estimator in
(b)) correspond to -
shifting from the local minimum associated with a heavily
biased fit to the local minimum near fi 1 (x), the optimum fit to the target distribution.
Plotting the step height at which this drop occurs as a function of " s gives a good summary
of these estimators' bias on step edges. Figure 8 does this, referring to this height as the
"small bias height" and quantifying it as the step height at which the bias drops below 1.0.
The plots in Figures 7 and 8(a) show that fixed-band techniques and fixed-scale M-estimators
are biased nearly as much as least-squares for significant step edge and crease
edge discontinuity magnitudes. The estimators fare much better on parallel lines (Figure 7(e)
and (f)); apparently, asymmetric positioning of pseudo outliers causes the most bias. To give
an intuitive feel for the significance of the bias, Figure 9 shows step edge data generated using
model parameters for which the robust estimators are strongly
biased.
Overall, the hard redescending, fixed-scale M-estimator is the least biased of the techniques
studied thus far. Compared to other fixed-scale M-estimators, its finite rejection
point - the point at which outliers no longer influence the fit - makes it less biased by
pseudo outliers than monotone and soft redescending fixed-scale M-estimators. On the other
hand, it is less biased than fixed-band techniques because it retains the statistical efficiency
of least-squares for small residuals.
The hard redescending, fixed-scale M-estimator can be made less biased by reducing
the values of its tuning parameters, as shown in Figure 8(b), effectively narrowing ae h and
reducing its finite rejection point. (The parameter set a = 1:0, 2:0 comes from
[2]; the set a = 1:0, chosen as an intermediate set of values.) Using
small parameter values has two disadvantages, however: the optimum statistical efficiency of
the standard parameters is lost, giving less accurate fits to the target distribution, and some
good data may be rejected as outliers. Despite these disadvantages, lower tuning parameters
should be used since avoiding heavily biased fits is the most important objective.
Finally, in practice, the non-convex objective functions of hard and soft redescending
Bias
Step Height
Least-squares
Fixed-band
Bias
Step Height
Least-squares
Monotone
Fixed-band
Hard
a b
Crease
Bias
Crease Height
Least-squares
Monotone
Fixed-band
Hard0.20.611.41.8
Bias
Crease Height
Least-squares
Monotone
Fixed-band
Hard
c d
Parallel
Bias
Relative Height
Least-squares
Monotone
Fixed-band
Bias
Relative Height
Least-squares
Monotone
Fixed-band
Hard

Figure

7: Bias of fixed-band techniques, fixed-scale M-estimators and least-squares on step
edges, (a) and (b), crease edges, (c) and (d), and parallel lines, (e) and (f), as a function
of height when " 0:8. The horizontal axis is the relative discontinuity
magnitude (height), \Deltaz=oe, and the vertical axis is the bias (see equation 35). Plots not
shown in (a) are essentially equivalent to the least-squares plots.
Small
Bias
Cut-off
Height
Fraction of Points on Lower Half of Step
Fixed-band
Small
Bias
Cut-off
Height
Fraction of Points on Lower Half of Step
1.31, 2.04, 4.0
1.0, 2.0, 3.0
1.0, 1.0, 2.0
a b

Figure

8: Small bias cut-off heights as a function of " s , the relative fraction of points on
the lower half of the step. Plots in (a) show the heights for fixed-band techniques and two
fixed-scale M-estimators. Plots in (b) show the heights for different tuning parameters of
the hard redescending fixed-scale M-estimator. Heights not plotted for small " s are above
When height is not plotted for large " s , bias is never greater than 1.0.
fixed-scale M-estimators can lead to more biased results than indicated here. Iterative search
techniques, especially when started from a non-robust fit, may stop at a local minimum
corresponding to a biased fit when the fit to the target distribution is the global minimum
of the objective function. Therefore, to avoid local minima, fixed-scale M-estimators should
use either a random sampling search technique or a Hough transform.
6.3 M-Estimators
Next, consider standard M-estimators, which estimate - oe from the data. To calculate T (H)
for the monotone and soft redescending M-estimators, simply calculate -
any mixture distribution using equation 7 or 8 as the objective functional. For the hard
redescending M-estimator, which estimates -
oe from an initial fit, the optimum fit to the
mixture distribution is found in three stages: first find the optimum LMS fit, then calculate
the median absolute deviation (MAD) [10, page 107] to this fit, scaling it to estimate -
oe,
and finally calculate -
oe fixed. Two different scale factors for estimating -
oe
are considered: the first, 1:4826, ensures consistency at the normal distribution; the second,
1:14601, ensures consistency at Student's t-distribution (with 1:5). Using the latter
allows accurate comparison between the hard and soft redescending M-estimators since the

Figure

9: Example step edge data generated when "
where each the objective function of each robust estimator (except LTS) is minimized by a
biased fit. The example fit shown is -
'(x) for the hard redescending, fixed-scale M-estimator.
latter is the maximum likelihood estimate for Student's t distribution [5].

Figure

shows bias plots for the soft redescending M-estimator and for the hard re-
descending M-estimator using the two different scale factors (plot "Hard-N" for the normal
distribution and plot "Hard-t" for the t-distribution). Results for the monotone M-estimator
are not shown since its bias matches that of least-squares almost exactly. Overall, the results
are substantially worse than for fixed-scale M-estimators, especially for " This is a
direct result of -
oe being a substantial over-estimate of oe: for example, when "
oe=oe - 2:4 for all estimates. (See [22] for analysis of bias in estimating -
oe.)
These over-estimates allow a large portion of the residual distribution to fall in the region
where ae is quadratic, causing the estimator to act more like least-squares. Because of this,
M-estimators are heavily biased by discontinuities when they must estimate - oe from the data.
6.4 LMS, LTS and MINPRAN
The last estimators examined are LMS, LTS, and MINPRAN, methods which neither require
oe a priori nor need to estimate it while finding -
'(x).

Figure

shows bias plots for these
estimators on step edges, crease edges and parallel lines, using "

Figure

shows small bias cut-off heights on step edges for LMS, LTS and MINPRAN, and
it demonstrates the effects of changes in the mixture proportions on LMS and LTS.
LMS and LTS work as well as any technique studied as long as the actual of fraction
Bias
Step Height
Least-squares
Hard-N
Bias
Step Height
Least-squares
Hard-N
Hard-t
a b
Crease
Bias
Crease Height
Least-squares
Hard-N
Hard-t0.20.611.41.8
Bias
Crease Height
Least-squares
Hard-N
Hard-t
c d
Parallel
Bias
Relative Height
Least-squares
Hard-N
Bias
Relative Height
Least-squares
Hard-N
Hard-t

Figure

10: Bias of M-estimators and least-squares on step edges, (a) and (b), crease edges,
Bias
Step Height
Least-squares
MINPRAN
LMS
Bias
Step Height
Least-squares
MINPRAN
LMS
a b
Crease
Bias
Crease Height
Least-squares
MINPRAN
LMS
Bias
Crease Height
Least-squares
MINPRAN
LMS
c d
Parallel
Bias
Relative Height
Least-squares
MINPRAN
LMS
Bias
Relative Height
Least-squares
MINPRAN
LMS

Figure

11: Bias of MINPRAN, LMS, LTS and least-squares on step edges, (a) and (b), crease
Small
Bias
Cut-off
Height
Relative Fraction from Lower Half of Step
MINPRAN
LMS
Small
Bias
Cut-off
Height
Relative Fraction from Lower Half of Step0.1a b261014
Small
Bias
Cut-off
Height
Actual Fraction from Lower Half of Step0.1048120.5 0.55 0.6 0.65 0.7 0.75
Small
Bias
Cut-off
Height
Actual Fraction from Lower Half of Step0.1c d

Figure

12: Small bias cut-off heights. Plot (a) shows these for LMS, LTS, MINPRAN, and
the modified MINPRAN optimization criteria (MINPRAN2) as a function of " s , the relative
fraction of inliers. Plot (b) shows these for LTS as a function of " s for different gross outlier
percentages " Plots (c) and (d) show these for LTS and LMS respectively as a function of
, the actual fraction of inliers. Heights not plotted for small " s or are
above When height is not plotted for large " s or never greater
than oe.
inliers - data from fi 1 (x) - is above 0.5. Since this fraction is the bias of LMS and
LTS, unlike that of M-estimators, depends heavily on both " sampling
implementations of LMS and LTS, where p points instantiate a hypothesized fit and the
objective function is evaluated on the remaining points, the bias curves in Figure 11
and the steep drop in cut-off heights in Figure 12 will shift to the right, but only marginally
since usually n AE p.) Figures 12b and c demonstrate this dependence in two ways for LTS.

Figure

shows small bias cutoffs as a function of " s , the relative fraction of inliers -
points on the lower half of the step. The bias cutoffs are lower for lower " simply because
fewer gross outliers imply more actual inliers when " s remains fixed. Figure 12c shows
small bias cutoffs as a function of the actual fraction of inliers. In this context, varying "
while fixed changes the fraction of gross outliers versus pseudo outliers. As the
plot shows, the coherent structure of the pseudo outliers causes more bias than the random
structure of gross outliers. This same effect is shown for LMS in Figure 12d. Finally, the
magnitude of z 0 , which controls the gross outlier distribution, has little effect on the bias
results, except in the unrealistic case where it approaches the discontinuity magnitude.
LTS is less biased than LMS, especially when the actual fraction of inliers is only slightly
above 0.5. This can be seen most easily by comparing the low bias cutoff plots in Figure 12c
and d. Like the advantage of hard redescending M-estimators over fixed-band techniques
(Section 6.2), this occurs because LTS is more statistically efficient than LMS [21] - its
objective function depends on the smallest 50% of the residuals rather than just on the
median residual. It is important to note that although LMS's efficiency can be improved by
application of a one-step M-estimator starting from the LMS estimate, this will not improve
substantially a heavily biased fit, since a local minimum of the M-estimator objective function
will be near this fit.
With a minor modification to its optimization criteria, MINPRAN can be made much
less sensitive to pseudo outliers, improving dramatically on the poor performance shown in

Figures

11 and 12. The idea is to find two disjoint fits (no shared inliers), -
' a and -
inlier bounds -
r a and - r b and inlier counts k - 'a ;-r a
and k -
, minimizing F(-r a +-r b ; k - ' 1 ;-r a
[23, 26]. If -
' is the single fit minimizing the criterion function, with inlier bound - r and inlier
count k -r , then the two fits -
' a and -
' b are chosen instead of the single fit -
'a ;-r a
Thus, the modified optimization criteria tests whether one or two inlier distributions are
more likely in the data [27]. Figure 12 shows the step edge small bias cut-off heights for this
new objective function, denoted by MINPRAN2. These are substantially lower than those of
the other techniques, including LTS. Further, these results, unlike those of MINPRAN, are
only marginally affected by the parameters " . Unfortunately, the search for -
' a and
' b is computationally expensive, and so the present implementation of MINPRAN2 uses a
simple search heuristic that yields [23, 26] more biased results than the optimum shown here.
It is, however, as effective as the fixed-scale, hard redescending M-estimator and, unlike LMS
and LTS, it does not fail dramatically when there are too few inliers.
6.5 Discussion and Recommendations
Overall, the results show that all the robust estimators studied estimate biased fits at small
but substantial discontinuity magnitudes. This bias, which relative to the bias of least-squares
is greater for crease and step edges and less for parallel lines, occurs even if -
oe or the
distribution of gross outliers or both are known a priori. Further, it must be emphasized
that this bias is not an artifact of the search process: the functional form of each estimator
returns the fit corresponding to the global minimum of the estimator's objective function.
The reason for the bias can be seen by examining the cumulative distribution functions
(cdfs) of absolute residuals. Figure 13 plots this cdf, F a (rj'; H), when ' is the target fit
is the least-squares fit to H, for H modeling crease and step discontinuities.
For the cdf of the biased fit is almost always greater that of the target fit,
meaning that in a discrete set of samples, the biased fit, which crosses through both point
sets, will on average yield smaller magnitude residuals than the target fit, which is close to
only the target point set. (The situation is somewhat better when \Deltaz=oe = 9:0.) Therefore,
robust estimators, such as the ones studied, whose objective functions are based solely on
residuals, are unlikely to estimate unbiased fits at small magnitude discontinuities.
CDF
Absolute Residual
Biased Fit
Target Fit0.20.61
CDF
Absolute Residual
Biased Fit
Target Fit
a b
Crease
CDF
Absolute Residual
Biased Fit
Target Fit0.20.61
CDF
Absolute Residual
Biased Fit
Target Fit
c d

Figure

13: Each figure plots the cumulative distribution functions (cdf) of absolute residuals
for the target fit and for a biased (least-squares) fit: (a) and (b) are relative to a step
discontinuity, and (c) and (d) are relative to a crease discontinuity. For all plots, the mixture
fractions are fixed at " robust estimators are substantially biased
at both step and crease discontinuities.
While none of the estimators works as well as desired, the following recommendations for
choosing among them are based on the results presented above:
oe is known a priori , one should use a hard redescending M-estimator objective
function such as Hampel's with reduced tuning parameter values and either a
random-sampling search technique or a weighted Hough transform. To ensure all inliers
are found and to obtain greater statistical efficiency, an one-step M-estimator with
larger tuning parameters should be run from the initial optimum fit. This technique is
preferable to LTS and LMS because it is less sensitive to the number of gross outliers.
oe is not known a priori, but the distribution of gross outliers is known, one
should use the modified MINPRAN algorithm, MINPRAN2 [23, 26].
ffl When neither - oe nor the distribution of gross outliers is known, LTS should be used,
although its performance degrades quickly when there are too few inliers. LTS is
preferable to LMS because of its statistical efficiency.
7 Summary and Conclusions
This paper has developed the pseudo outlier bias metric using techniques from mathematical
statistics to study the fitting accuracy of robust estimators on data taken from multiple
structures - surface discontinuities, in particular. Pseudo outlier bias measures the distance
between a robust estimator's optimum fit to a target distribution and its optimum fit to an
outlier corrupted mixture distribution. Here, the target distribution models the points from
a single surface and the mixture distribution models points from multiple surfaces plus
gross outliers. Each estimator's optimum fit is found by applying its functional form to
one of these model distributions. Thus, like other analysis tools from the robust statistics
literature, pseudo outlier bias depends on point distributions rather than on particular point
sets drawn from these distributions. While this has some limitations - the actual fitting
error for particular points sets may be more or less than the pseudo outlier bias and it ignores
problems that may arise from multiple local minima in an objective function - it represents
a simple, efficient, and elegant method of analyzing robust estimators.
Pseudo outlier bias was used to analyze the performance of M-estimators, fixed-band
techniques (Hough transforms and RANSAC), least median of squares (LMS), least trimmed
squares (LTS) and MINPRAN in fitting surfaces to three different discontinuity models: step
edges, crease edges and parallel lines. For each of these discontinuities, two surfaces generate
data, with the larger set of surface data forming the inliers and the smaller set forming
the pseudo outliers. By characterizing these discontinuity models using a small number of
parameters, formulating the models as mixture distributions, and studying the bias of the
robust estimators as the parameters varied, it was shown that each robust estimator is biased
for substantial discontinuity magnitudes. This effect, which relative to that of least-squares
is strongest for step edges and crease edges, persists even when the noise in the data or the
gross outlier distribution or both are known in advance. It is disappointing because in vision
data - not just in range data - multiple structures (pseudo outliers) are more prevalent
than gross outliers. In spite of the disappointment, however, specific recommendations,
which depend on what is known about the data, were made for choosing between current
techniques. 6
These negative results indicate that care should be used when robustly estimating surface
parameters in range data, either to obtain local low-order surface approximations or to
initialize fits for surface growing algorithms [3, 5, 6, 15]. (Similar problems may occur
for the "layers" techniques that have been applied to motion analysis [1, 6, 28].) Robust
estimates will be accurate for large scale depth discontinuties and sharp corners, but will
be skewed at small magnitude discontinuites, such as near the boundary of a slightly raised
or depressed area of a surface. Obtaining accurate estimates near these discontinuities will
require new and perhaps more sophisticated robust estimators.

Acknowledgements

The author would like to acknowledge the financial support of the National Science Foundation
under grants IRI-9217195 and IRI-9408700, the assistance of James Miller in various
aspects of this work, and the insight offered by the anonymous reviewers which led to sub-
6 See [14, 17] for new, related techniques.
stantial improvements in the presentation.


Appendix

A: Evaluating F s
This appendix shows how to evaluate the conditional cumulative distribution and conditional
density of signed residuals, F s 22). The
distribution and density of the absolute residuals are obtained easily from these.
Expanding the expression in equation 21 for F s (rj'; H), using equation h, gives
F s
Here,
Z '(x)+r
is the cumulative distribution of the gross outliers, and for
Z '(x)+r
To simplify evaluating F s
variables and then change the order of integration.
Starting with the change of variables, make the substitutions
(intuitively, v is the fit residual at x), define
Then, the integral becomes
Z OE(x)+r
Since the integrand is now independent of x, rewriting the integral to integrate over strips
parallel to the x axis will produce a single integral. Consider a strip bounded by v and v+ \Deltav

Figure

14). The integral over this strip is approximately - i g(v)w(v)\Deltav, where w(v) is the
width of the integration region at v. In the limit as \Deltav ! 0, this becomes exact and the
integral over the entire region becomes
is the maximum of OE(x)
r
f
x

Figure

14: Calculating F s
requires integrating the point density for curve
over strips of width \Deltav parallel to the x axis. The density g(v; oe 2 ) is constant over these
strips.
Evaluating w(v) depends on OE(x). This paper studies linear fits and linear curve models,
so OE(x) is linear. In this case, let
and Figure 14). Then,
using G to denote the cdf of the gaussian,
A similar result is obtained when m ! 0, and when
To compute the density f s (rj'; H), start from the mixture density in equation 22 and
integrate each component density separately. This is straightforward when the density g o is
uniform and, as above, '(x) and fi i (x) are linear.



--R

Layered representation of motion video using robust maximum likelihood estimation of mixture models and MDL encoding.
Robust window operators.
Segmentation through variable-order surface fitting
A Ransac-based approach to model fitting and its application to finding cylinders in range data
The Robust Sequential Estimator: A general approach and its application to surface organization in range data.
Cooperative robust estimation using layers of support.
Random Sample Consensus: A paradigm for model fitting with applications to image analysis and automated cartography.
Linear Systems
The change-of-variance curve and optimal redescending M-estimators
Robust Statistics: The Approach Based on Influence Functions.
Robust regression using iteratively reweighted least- squares
Robust Statistics.
A survey of the Hough transform.
Robust adaptive segmentation of range images.
Segmentation of range images as the search for geometric parametric models.
Robust regression methods for computer vision: A review.
MUSE: Robust surface fitting using unbiased scale esti- mates
Performance evaluation of a class of M-estimators for surface parameter estimation in noisy range data
Numerical Recipes in C: The Art of Scientific Computing.
Extracting geometric primitives.
Least median of squares regression.
Alternatives to the median absolute deviation.
A new robust operator for computer vision: Application to range images.
A new robust operator for computer vision: Theoretical analysis.
Expected performance of robust estimators near discontinuities.
MINPRAN: A new robust estimator for computer vision.
Statistical Analysis of Finite Mixture Distri- butions
Layered representation for motion analysis.
--TR

--CTR
Cesare Alippi, Randomized Algorithms: A System-Level, Poly-Time Analysis of Robust Computation, IEEE Transactions on Computers, v.51 n.7, p.740-749, July 2002
Alireza Bab-Hadiashar , David Suter, Robust segmentation of visual data using ranked unbiased scale estimate, Robotica, v.17 n.6, p.649-660, November 1999
Ulrich Hillenbrand, Consistent parameter clustering: Definition and analysis, Pattern Recognition Letters, v.28 n.9, p.1112-1122, July, 2007
Klaus Kster , Michael Spann, MIR: An Approach to Robust Clustering-Application to Range Image Segmentation, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.22 n.5, p.430-444, May 2000
Thomas Kmpke , Matthias Strobel, Polygonal Model Fitting, Journal of Intelligent and Robotic Systems, v.30 n.3, p.279-310, March 2001
Christine H. Mller , Tim Garlipp, Simple consistent cluster methods based on redescending M-estimators with an application to edge identification in images, Journal of Multivariate Analysis, v.92 n.2, p.359-385, February 2005
Hanzi Wang , David Suter, Robust Adaptive-Scale Parametric Model Estimation for Computer Vision, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.26 n.11, p.1459-1474, November 2004
Hanzi Wang , David Suter, MDPE: A Very Robust Estimator for Model Fitting and Range Image Segmentation, International Journal of Computer Vision, v.59 n.2, p.139-166, September 2004
Philip H. S. Torr , Colin Davidson, IMPSAC: Synthesis of Importance Sampling and Random Sample Consensus, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.25 n.3, p.354-364, March
T. Vieville , D. Lingrand , F. Gaspard, Implementing a Multi-Model Estimation Method, International Journal of Computer Vision, v.44 n.1, p.41-64, August 2001
P. H. S. Torr, Bayesian Model Estimation and Selection for Epipolar Geometry and Generic Manifold Fitting, International Journal of Computer Vision, v.50 n.1, p.35-61, October 2002
Myron Z. Brown , Darius Burschka , Gregory D. Hager, Advances in Computational Stereo, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.25 n.8, p.993-1008, August
