--T
Tensor product multiresolution analysis with error control for compact image representation.
--A
A class of multiresolution representations based on nonlinear prediction is studied in the multivariate context based on tensor product strategies. In contrast to standard linear wavelet transforms, these representations cannot be thought of as a change of basis, and the error induced by thresholding or quantizing the coefficients requires a different analysis. We propose specific error control algorithms which ensure a prescribed accuracy in various norms when performing such operations on the coefficients. These algorithms are compared with standard thresholding, for synthetic and real images.
--B
Introduction
Multiresolution representations of data, such as wavelet basis decompositions, are a powerful
tool in several areas of application like numerical simulation, statistical estimation and data
compression. In such applications, one typically exploits the ability of these representations
to describe the involved functions with high accuracy by a very small set of coe-cients.
A pivotal concept for a rigorous analysis of their performance is nonlinear approximation: a
function f is \compressed" by its partial expansion f N in the wavelet basis, which retains only
the N largest contributions in some prescribed metric X. The most commonly used metric for
thresholding images is other norms can be considered. For a given function
f , the rate of best N-term approximation is the largest r such that N
behaves as O(N r ) as N tends to +1. In many instances a given rate is equivalent to
some smoothness property on the function f (see e.g. [11] for a survey on such results).
Several recent works have demonstrated that such a rate is also re
ected in the performance
of adaptive methods in the above mentioned applications: see [6] for numerical simulation,
[13, 12] for statistical estimation and [14, 7, 9] for data compression, as well as the celebrated
image coding algorithms proposed in [22, 21].
In the case of applications involving images, such as compression and denoising, the main
limitation to a high rate of best N-term approximation is caused by the presence of edges,
since the numerically signicant coe-cients at ne scales are essentially those for which the
wavelet support is intersected by such discontinuities. In particular, this limits the e-ciency
of high order wavelets due to their large supports. From a mathematical point of view, this
is re
ected by the poor decay - in O(N 1=2 ) - of the best wavelet N-term approximation L 2
error for a \sketchy image
where
is a bounded domain with a smooth
boundary. This re
ects the fact that this type of approximation essentially provides local
isotropic renement near the edges. Improving on this rate through a better choice of the
representation has motivated the recent development of ridgelets and curvelets in [4], which
are bases and frames having some anisotropic features, resulting in the better rate O(N 1 ).
Another possible track for such improvements is oered by multiresolution representations
which incorporate a specic adaptive treatment of edges. Tools of this type have been
introduced in the early 90's by Ami Harten (see e.g. [16, 19, 17]) as a combination of wavelet
analysis and of numerical procedures introduced by the same author in the context of shock
computations, the so-called essentially non-oscillatory (ENO) and subcell resolution (ENO-
reconstruction techniques (see e.g. [18, 20]). Formally speaking, the multiresolution
representations proposed by Harten have the same structure as the standard wavelet trans-
forms: a sequence v L of data sampled at resolution 2 L is transformed into (v
where the v 0 corresponds to a sampling at the coarsest resolution and each sequence d k represents
the intermediate details which are necessary to recover v k from v k 1 . However, the main
dierence is that the basic interscale decomposition/reconstruction process which connects v k
with (v k 1 ; d k ) is allowed to be nonlinear.
As we shall see in image examples, the nonlinear process allows a better adapted treatment
of singularities, in the sense that they do not generate so many large detail coe-cients as in
standard wavelet transforms. On the other hand, it raises new problems in terms of stability
which need to be addressed in order to take full advantage of this gain in sparsity. Let us
explain this point in more detail. In most practical applications, the multiscale representation
(v processed into a new (^v
d L ) which is close to the original one,
in the sense that for some prescribed discrete norm k  k,
where the accuracy parameters  are chosen according to some criteria specied
by the user. Such processing corresponds for example to quantization in the context of data
compression or simple thresholding in the context of statistical estimation, adaptive numerical
simulation or nonlinear approximation. Applying the inverse transform to the processed
representation, we obtain a modied sequence ^ v L which is expected to be close to the original
discrete set v L . In order for this to be true, some form of stability is needed, i.e., we must
require that
lim
l
In the case of linear multiresolution representations, the stability properties can be precisely
analyzed in terms of the underlying wavelet system: for example if this system constitutes
a Riesz basis for L 2 , stability can be re-expressed as a norm equivalence between v L and
(v constants that do not depend on L. However, these techniques are no
more applicable in the setting of non-linear representations which cannot anymore be thought
as a change of basis. In the nonlinear case, stability can be ensured by modifying the algorithm
for the direct transform in such a way that the error accumulated in processing the values of
the multi-scale representation remains under a prescribed value. This idea was introduced by
Harten for one dimensional algorithms in [16, 3, 1].
The aim of this paper is to introduce and analyze two-dimensional multiresolution processing
algorithms that ensure stability in the above sense, and to test them on image data. We
consider here the tensor product approach, which is also used in most wavelet compression
algorithms. While this approach limitates the compactness of the representation for edges
which are not horizontal or vertical, it inherits the simplicity of the one dimensional techniques
in any dimension. This allows us to develop our algorithms in the same spirit as in
the one dimensional case, yet pointing out new ways of truncating the data and proving explicit
a-priori error bounds in the L 1 , L 2 and L 1 metric, in terms of the quantization and
thresholding parameters. Let us mention that closely related algorithms have been recently
introduced in [5] based on a non-linear extension of the so-called lifting scheme.
The paper is organized as follows: we recall in x2 the discrete framework for multiresolution
introduced by Harten, and we focus on two specic cases corresponding to point-value and
cell-average discretizations for which we recall the linear, ENO and ENO-SR reconstruction
techniques. The tensor-product error control algorithms are discussed in x3, where we also
discuss the sharpness of the error bounds and the connexions with standard wavelet thresholding
algorithms. These strategies are then practically tested on images in x4. In a rst
set of experiments, we test the reliability and sharpness of our a-priori error bounds for a
prescribed target accuracy. In a second set of experiments, we apply an alternate processing
strategy based on a-posteriori error bounds in order to further reduce the number of preseved
coe-cients while remaining within this prescribed accuracy. In the last set of experiments,
we compare the compression performances of linear and nonlinear multiresolution decomposi-
tions, using either error control or standard thresholding, on geometric and real images. The
results can roughly be summarized as follows: nonlinear decompositions clearly outperform
standard linear wavelet decompositions for geometric images, with an inherent limitation due
to the use of a tensor product strategy, but brings less improvement for real images due to
the presence of additional texture. This raises both perspectives of developping appropriate
non-tensor product representations and of separating the geometric and textural information
in the image in order to take more benet from these new representations.
Acknowledgment: we are grateful to the anonymous reviewers for their constructive suggestions
in improving the paper.
2. Harten's framework for Multiresolution
2.1. The general framework
The discrete multiresolution framework introduced by Harten essentially relies on two pro-
cedures: decimation and prediction. From a purely algebraic point of view, decimation and
prediction can be considered simply as interscale operators connecting linear vector spaces,
that represent in some way the dierent resolution levels (k increasing implies more res-
olution), i.e.,
(a)
(b)
While the decimation D k 1
k is always assumed to be linear, we do not x this constraint on
the prediction P k
. The basic consistency property that these two operators have to satisfy
is
is the identity operator in V k 1 , which in particular implies that D k 1
k has full
rank. If v the vectors derived by iterative
decimation
the prediction error. Clearly v k
and (v contain the same information, but e k expressed as a vector of V k contains some
redundancy since the consistency relation (2) implies that e k is an element of the null space,
(D k 1
Keeping the algebraic viewpoint, we can remove this redundancy by introducing
an operator G k which computes the coordinates of e k in a basis of N (D k 1
k ), and another
that recovers the original redundant description of e k from its non-redundant
part, i.e., such that e These ingredients allow us to write a purely algebraic
description of the direct and inverse multiresolution transform as follows:
These algorithms which connect v L with its multiscale representation (v
can be viewed as a simple change of basis in the case where the prediction operator is linear.
F
F
@
@
@R
@
@
@I


Figure

1: Denition of transfer operators
In practice, the construction of the operators D k 1
k 1 is based on two fundamental
tools: discretization and reconstruction. The discretization operator D k acts from a non-discrete
function space F onto the space V k and yields discrete information at the resolution
level specied by a grid X k . It is required that D k be a linear operator. The reconstruction
operator R k , on the other hand acts from V k to F and produces an approximation to a function
F from its discretized values: the function R k D k f . A basic consistency requirement of
the framework is that
Given sequences of discretization and reconstruction operators satisfying (5), it is then possible
to dene the decimation and prediction operators according to
This denition is schematically described in gure 1. Observe that there seems to be an
explicit dependence of D k 1
k on R k but it is easy to prove that the decimation operator is in
fact totally independent of the reconstruction process whenever the sequence of discretization
is nested, i.e., it satises
This property implies, in essence, that all the information contained in the discretized data
at a given resolution level is also included in the next (higher) one. For a nested sequence
of discretization, the description of D k 1
k as R k D k 1 , is just a formal one (useful in some
contexts), and in practice one does not resort to the reconstruction sequence to decimate
discrete values.
The description of the prediction step as P k
opens up a tremendous number
of possibilities in designing multiresolution schemes. The reconstruction process is the
step. The subband ltering algorithms associated to biorthogonal wavelet decompositions
correspond to particular cases where this process is linear. In contrast, nonlinear reconstruction
operators will lead naturally to nonlinear multiresolution representations which cannot
anymore be thought as a change of basis. For the sake of completeness and ease of reference,
in the remainder of this section we give a very brief description in one dimension of more
restricted frameworks associated to point-value and cell-average discretizations, together with
the corresponding linear, ENO and ENO-SR reconstruction operators. The left-out details in
the entire section (and much more) can be found in [1, 17, 2, 3].
multiresolution representations have also recently been proposed in
[5] based on a non-linear extension of the lifting scheme. In particular, the nonlinear ENO
and ENO-SR representations that will be used in this paper could be introduced with the
lifting terminology in place of the above general framework in which they were originally in-
troduced. On the other hand the error control strategies that we develop in x3 are specically
adapted to the restricted frameworks of point-value and cell-average discretizations (and different
from the general \synchronization" strategy of [5]). These restricted frameworks have
a particularly simple interpretation within Harten's general framework.
2.2. Point-value multiresolution analysis in 1D
Let us consider a set of nested grids:
where N 0 is some xed integer. Consider the point-value discretization
is the space of sequences of dimension
we obtain
(D
Notice that N (D k 1
can be dened as follows:
A reconstruction procedure for the discretization operator dened by (8) is given by any
operator R k such that
which means that
(R k
therefore, (R k
should be a continuous function that interpolates the data
f k on X k .
Let us change slightly the notation and denote by I k (x;
such an interpolatory reconstruction
of the data
f k . The prediction operator can be computed as follows:
and the direct and inverse transforms (3) and (4) take the following simple form:
Notice that in the point-value framework, the detail coe-cients are simply interpolation errors
at the odd points of the grid that species the level of resolution.
A natural way of dening a linear interpolation operator is as follows: some integer m > 0
being xed, we consider the unique polynomial p i of degree 2m 1 such that p i
simply dene
I
This choice coincides with the so-called Lagrange interpolatory wavelet transform. Note that
as m increases, the interpolation process has higher order accuracy, i.e. the details d k
i will be
smaller if f is smooth on [x k 1
]. On the other hand, the intervals [x k 1
larger with m so that a singularity will aect more detail coe-cients.
Non-linear essentially non-oscillatory (ENO) interpolation techniques, which were rstly
introduced in [20], circumvent this drawback: the idea is to replace in (15) the polynomial
by a polynomial p
selected among fp in order to avoid the in
uence
of the singularity. The selection process is usually made by picking the \least oscillatory"
polynomial using numerical information on the divided dierences of f at the points x k 1
.
In the present paper we have been using the so called hierachical selection process which is
detailed in [1]. Once this selection is made, we thus dene
I
Such a process still produces large details d k
when a singularity is contained in the interval
In order to reduce further the interpolation error, subcell resolution methods
were introduced in [18] as an elaboration of ENO interpolation. The idea is rst
to detect the possible presence of singularities by
agging those i such that p
i.e. such that the selection process tends to escape the interval [x k 1
such
i+1 intersect at a single point a 2 [x k 1
we identify this point as
the singularity and replace in (16) the polynomial p
i by the piecewise polynomial function
which coincides with p
Note that such a process is
better tted to localize the singularities of the rst order, i.e. jumps in f 0 , rather than the
discontinuities of f . Such discontinuities, which correspond to edges in image processing, are
better treated in the cell-average framework that we now describe.
2.3. Cell-average multiresolution analysis in 1D
With the same nested grids structure of last section, we dene the discretization
is the space of absolutely integrable functions in [0; 1]. It is su-cient to consider
weighted averages
since these contain information on f over [0; 1]. Thus, V k
is the space of sequences with N k components. Additivity of the integral leads to the following
decimation step:
thus, the prediction error satises
and the operators G k and E k can be dened as follows
A reconstruction operator for the discretization in (17) is any operator R k
satisfying
(D
(R k
That is, R k
k (x) has to be a function in L 1 ([0; 1]) whose mean value on the ith cell coincides
with
In one dimension, the simplest way to construct R k is via the \primitive function".
Dene the sequence fF k
i g on the k-th grid as
The function F (x) is a primitive of f(x), and the sequence fF k
corresponds to a discretization
by point-values of F (x) on the k-th grid. Let us denote by I k (x; F k ) an interpolatory
reconstruction of F (x), and dene
(R k
dx I k (x; F k
It is easy to see that D k R (R k
With these denitions the direct transform (3) and its inverse (4) can be described as
follows:
where
d
dx
The linear, ENO and ENO-SR techniques for cell-averages are simply derived from the corresponding
techniques in the point-value framework, using the above primitivation. Note that
jumps in f get transformed into jumps in F 0 , so that the ENO-SR process is now well tted
to localize the discontinuities.
Remark 2.1 In practice, the primitive function is used as a design tool and it is never
computed explicitly: All calculations are done directly on the discrete cell-values ([16, 1]). In
particular, for linear techniques, we obtain the biorthogonal wavelets decompositions corresponding
to the case where the dual scaling function ~
' is the box function  [0;1] (if in addition
we have for the accuracy parameter, we obtain nothing but the Haar system).
3. MR-based compression schemes with error-control
Multiresolution representations lead naturally to data-compression algorithms. Probably the
simplest data compression procedure is truncation by thresholding, which amounts to setting
to zero all detail coe-cients which fall below a prescribed, possibly level dependent, threshold,
Thresholding is used primarily to reduce the \dimensionality" of the data. A more elaborate
procedure, which is used to reduce the digital representation of the data is quantization, which
can be modeled by
where round [] denotes the integer obtained by rounding. For example, if jd k
then we can represent
i by an integer which is not larger than 32 and commit a
maximal error of 4. Observe that jd k
and that in both cases
While the thresholding procedure is usually applied only to the scale coe-cients, the quantization
process is also applied to the coarse level representation.
After the application of a particular compression strategy, such as truncation or quantiza-
tion, we obtain a compressed multiresolution representation M
d L g, where
represents the compression parameters, i.e.,  applying truncation
by thresholding). Obviously, M
f is close to M
g. Applying the inverse
multiresolution transform to the compressed representation, we obtain ^
f , an
approximation to the original signal
f L . The fundamental question is that of estimating, and
thus being able to control, the error k
f L k in some prescribed norm. It often happens
that we are given a target accuracy, i.e., a maximum allowed deviation , thus our goal is to
obtain a compressed representation M
f such that
We can formulate this goal in various ways :
f L k from the errors kd k
f L k from the thresholds  k .
For linear multiscale representations corresponding to wavelet decompositions, error estimates
of the type we are looking for are typically derived by using the stability properties of
the underlying wavelet system. In the nonlinear framework, an error-control strategy was rst
proposed by Harten in [16] to directly accomplish (29), namely modify the direct transform in
such a way that the modication allows us to keep track of the cumulative error and truncate
accordingly.
Let us explain in a nutshell the idea of the error-control algorithm (EC henceforth). As
a rst step, the sequences
f 0 are computed from
f L by iterative application of the
decimation operator. Then, we start at the coarsest level and dene ^
f 0 by applying some
perturbation process (thresholding or quantization) on
is now to dene the processed details ^
d k and processed kth scale representation ^
f k in an
intertwined manner from coarse to ne scales: for L, the processed details ^
represent a perturbation of the prediction error
involving the processed data
at the coarser scale ^
while the sequence of processed data is simultaneously computed
according to ^
d k .
In the following, we shall focus on the point-value and cell-average frameworks for which
there is a rather natural specic way to dene the processed details ^
d k . In one dimension, the
strategy can be schematically described as follows:
d L g
Here
given by the one-dimensional decimation operator correspnding
to the chosen framework, i.e.
2i for point-values and
cell-averages. On the other hand [
stands for the kth step of
the 1D error control algorithms presented in [1], which we recall here for the sake of clarity:
~
~
(a) (b)

Figure

2: algorithms in 1d. (a) Point-values. (b) Cell-averages
A key point is that the values ^
f k are precisely the values computed by the corresponding
inverse multiresolution transform M 1 at each resolution level, i.e. (14) in the point-value
framework and (25) in the cell-average framework. In other words:
A second key point is that, in both algorithms, the coe-cients ~
coincide with the true
prediction errors at odd points e k
(i.e., the scale coe-cients in the
direct encoding) only when
In the EC algorithm, the details need to be dened in such a way that, after compression
takes place, the error accumulated at each renement step, i.e., jj
can be controlled.
To get a direct control on jj
f k jj, the details ~
must contain relevant information on the
error committed in predicting the true values at the kth level, i.e.
k , from the computed
In the point-value framework, there is no error at even points, and only the prediction
errors at the odd points needs to be controlled. It is not hard to deduce from gure 2-(a) (see
also [1]) that one has
In the cell average setting, the compressed details ^
d k are dened by applying the processing
strategy on the half-dierences ~
adjacent points. This is su-cient to ensure control on the prediction errors at each location
on a given resolution level, because from gure 2-(b) one can easily deduce that [1]
Relations (31) in the point-value framework, and (32) in the cell-average framework, express
the compression error at the kth level in terms of the compression error at the previous level
plus a quantity that is directly related to the thresholds  k . These basic relations lead to the
one-dimensional error-bounds in [1].
product EC algorithms in two dimensions
Needs a small paragraph on why we choose tensor product (simple, everybody
else does that etc,,,)
The modied algorithms for the direct and inverse multiresolution transforms we shall
describe below can be viewed as two-dimensional tensor product extensions of the one dimensional
algorithms in [16, 1]. For each resolution level, one acts with the one-dimensional
decimation operator on each row of the 2d array
ij , and computes intermediate values, say
2 . This array, which has N k rows but only N k 1 columns, is decimated again column
by column with the one-dimensional operator to obtain
These values are stored until
the bottom level is reached. Then computed values ^
2 and processed details ^
d k are
computed in an intertwined manner using only the one-dimensional algortihms described in
the previous section. In this context, and similarly to the tensor-product wavelet transform,
we obtain three types of details ^
d k (3).
To be more explicit, we give next a schematic description of the typical 2D algorithm for
the EC-direct transform,
Algorithm 1: Modied Direct transform
The intermediate values
f k are discarded once the algorithm concludes, and the outcome of
the EC-direct transform is
d
d
d L (3)g
The inverse multiresolution transform can be described using the 1d operator inver1d, which
denotes the k step of algorithms (14) in the point-value framework and (25) in the cell average
framework.
Algorithm 2: Inverse transform
As in the 1D case, the inverse transform (decoding) satises
It should be remarked that, due to the intertwined structure of the error control algorithm,
the details, ~
will be dierent if one acts rst on the rows or rst on the columns. In any
case, and even though the computed details might be dierent, they are of the same order of
magnitud.
1.1 Explicit Error Bounds for the EC strategies in two dimensions
We shall consider the following norms:



In what follows we shall see that it is possible to estimate the error between the original signal
f L and the signal obtained from decoding its compressed representation, i.e. ^
f L
from either the dierences jj ^
or the threshold parameters  k .
Proposition 3.2 Given a discrete sequence
f L , with the modied direct transform for the
pointvalue framework in 2-d (Algorithm 1) we obtain a multiresolution representation M M
f L
such that if we apply the inverse transform (Algorithm 2) we obtain ^
f L satisfying:
(jjj ~
where
Proof: From the 1d relations (31) we obtain
hence
i;j (2)
and
i;j (2)
Then
(j
and we obtain (34). Since N
(j
which proves (35) and (36).
Corollary 3.3 Consider the error control multiresolution scheme described in proposition
3.2, and a processing strategy for the detail coe-cients such that
we obtain that
In particular, if we assume that jj
we obtain
Proposition 3.5 Given a discrete sequence
f L , with the modied direct transform for the
cell average framework in 2-d (Algorithm 1) we obtain a multiresolution representation M M
f L
such that if we apply the inverse transform (Algorithm 2) we obtain ^
f L satisfying:
Proof: As a consequence of the relations in (32) we obtain
and
i;j (2)
i;j (2)
i;j (2)
i;j (2)
i;j (2)
and
i;j (2)
Hence
max(j ~
i;j (2)
i;j (2)
i;j (2)
thus
and we deduce (40).
Also from (44), we deduce
4j
and
hence we have proved (41).
To prove (42), we obtain from (43)
and
d
hence
2and the pronof is concluded.
Corollary 3.6 Consider the error control algorithm for the cell average framework described
in proposition 3.5 and a processing strategy for the detail coe-cients such that
3:
Then
In particular, if we assume that jj
we obtain
3.4 Remarks and comparison with standard thresholding
The results of Propositions 3.2 and 3.5 can be viewed as a-posteriori bounds on the compression
error, since they involve the ~
d k which themselves depend on the processing strategies applied
at the coarser levels. In practice, this becomes important; the a-posteriori bound can be
evaluated at the same time the compression process is taking place, and the a-posteriori
bound coincides in some cases with the exact compression error ((34), (35), (36) and (42)).
On the other hand, Corollary 3.3 and 3.6 provide a-priori bounds on the compression error.
Consider for example the L 2 -error in the cell-average framework and assume, for simplicity,
that
f 0 . According to Corollary 3.6, we can ensure an error of order  by choosing a
sequence
and requiring that the processed details ^
3:
The truncation strategies given by (26) and (27) both satisfy (37). Note, however, that with
either one of these two strategies, the error bound jj ~
is already not sharp
since it corresponds to the worst case scenario, where all the dierences j( ~
are close to  k , while in practice these dierences are often zero, or much smaller than  k .
Therefore, we expect in this case that the a-priori bound in  is over-estimated and much
larger than the a-posteriori bound given by Proposition 3.5, which in this particular case is
precisely the compression error k
This fact is examined in detail in the next section,
where it is used as the starting point for the design of new truncation strategies.
Another important issue is how to choose the dependence in k of the truncation parameter
k in order to optimize the compression process. By \optimize", we mean here to minimize
the number of resulting parameters for a given prescribed error. This number corresponds to
the number of non-discarded coe-cients in the case of thresholding, and to the total number
of bits in the case of quantization.
In the case of linear multiresolution representations associated to wavelet decompositions,
the answer to this question is now well understood: the threshold or truncation parameters
should be normalized in accordance with the error norm that one is targeting. More precisely,
if the detail coe-cients d  represent the expansion of a function in a wavelet basis
and if one is interested in controlling the L p norm with a minimal number of parameters, then
these coe-cients should be perturbed according to
d   d   k L
d  d  jk  k L p
where  is xed independently of . We refer to [11] and [7] for such type of results. It is
easily seen that in the case of two-dimensional cell-average multiresolution, this corresponds
to taking
in Corollary 3.6. This suggests to use similar normalizations in the setting of
nonlinear multiresolution algorithms with error control, although there is no guarantee now
that such a strategy will be optimal in the above sense. This issue will be revisited in the next
section through numerical testing.
As an example consider again the L 2 -error in the cell-average discretization. In the linear
case, an optimal choice for  k is of the form
According to Corollary 3.6, we can take ensure a global L 2 error less than  by
taking
. However, as already remarked, we may expect that the L 2 error is
actually much less than , so that one can still lower the number of compression parameters
by raising  while the a-posteriori L 2 -error bound, which in this case is precisely the exact
remains below the tolerance .
The fact that the error-control strategy permits to monitor the compression error at each
resolution level, through the a-posteriori bounds, allows us to design new processing strategies
that aim at reducing as much as possible the number of resulting parameters, while keeping,
at the same time, the total compression error below a specied tolerance. An example of such
strategy is given in the next section.



--R




A Surprisingly E
Nonlinear Wavelet Transforms for Image Coding via Lifting Scheme
Adaptive wavelet algorithms for elliptic operator equations - Convergence rate to appear in Math
Tree approximation and optimal encoding.
Biorthogonal bases of compactly supported wavelets.
On the importance of combining wavelet-based nonlinear approximation with coding strategies
Ten Lectures on Wavelets.
Nonlinear approximation.
Unconditional Bases are Optimal Bases for Data Compression and for Statistical Estimation
Wavelet shrinkage: asymptotia
Analysis of low bit rate image coding.
Analyses Multir
Discrete multiresolution analysis and generalized wavelets.
Multiresolution representation of data II: General framework.
ENO schemes with subcell resolution.
Multiresolution representation of cell-averaged data
Uniformly high order accurate essentially non-oscillatory schemes III
An image multiresolution representation for lossless and lossy compressio.
Embedded image coding using zerotrees of wavelet coe-cient
--TR
Uniformly high order accurate essentially non-oscillatory schemes, 111
ENO schemes with subcell resolution
Multiresolution representation of data
Multiresolution Based on Weighted Averages of the Hat Function I
Multiresolution Based on Weighted Averages of the Hat Function II
Adaptive wavelet methods for elliptic operator equations

--CTR
F. Arndiga , R. Donat , P. Mulet, Adaptive interpolation of images, Signal Processing, v.83 n.2, p.459-464, February
S. Amat , J. Ruiz , J. C. Trillo, Compression of color image using nonlinear multiresolutions, Proceedings of the 5th WSEAS International Conference on Signal Processing, Robotics and Automation, p.11-15, February 15-17, 2006, Madrid, Spain
S. Amat , J. C. Trillo , P. Viala, classical multiresolution algorithms for image compression, Proceedings of the 5th WSEAS International Conference on Signal Processing, Robotics and Automation, p.1-5, February 15-17, 2006, Madrid, Spain
Sergio Amat , S. Busquier , J. C. Trillo, Nonlinear Harten's multiresolution on the quincunx pyramid, Journal of Computational and Applied Mathematics, v.189 n.1, p.555-567, 1 May 2006
