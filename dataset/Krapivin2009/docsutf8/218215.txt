--T
Active Camera Calibration for a Head-Eye Platform Using the Variable State-Dimension Filter.
--A
AbstractThis correspondence presents a new technique for calibrating a camera mounted on a controllable head/eye platform. It uses the trajectories of an arbitrary number of tracked corner features to improve the calibration parameter estimates over time, utilizing a novel variable state dimension form of recursive filter. No special visual stimuli are required and no assumptions are made about the structure of the scene, other than that it is stationary relative to the head. The algorithm runs at 4 frames per second on a single Inmos T805 transputer, and is fully integrated into a real-time active vision system. Updated calibration parameters are regularly passed to the vision modules that require them. Although the algorithm requires an initial estimate of camera focal length, results are presented from real experiments demonstrating that convergence is achieved for initial errors up to 50%.
--B
Introduction
Scene reconstruction and object recognition are areas of computer
vision which have been plagued by the need for accurate
camera calibration [19]. Calibration typically requires objects
made to high precision to be placed in front of the cameras
[13], [8] and requires considerable experimental care, making
the methods impractical for an autonomous robot acting in an
unstructured world.
For these reasons much emphasis has been placed recently
in the fields of structure from motion [12], stereo [7], object
recognition [21] and active vision [5] on algorithms that obviate
camera calibration. However, when vision is to control
robotic systems, there seems a limit to how far these ideas can
be pushed. In order to make controlled motions, an active system
must convert image quantities into angles and distances,
requiring calibration of some form. For example, consider 1D
fixation on a target point P which corresponds under perspective
projection to a point x in the image. If the focal length is f ,
and igonoring any image distortions, the angle that the camera
must turn through to fixate P is first sight,
one of the merits of an active system exploiting visual feedback
is that it can redirect gaze without knowing f . However, if
one underestimates f , the value for ' is overestimated, and vice
versa, so that the choice of f affects the damping of the fixation
control system.
In this correspondence we describe a calibration method
which exploits our head/eye platform's ability to make precisely
known head movements while robustly tracking stationary
points in an unstructured world. The approach is novel in several
ways. First it utilises all measurements of image features
tracked through multiple images in a computationally efficient
and statistically principled manner. Secondly, the process is
fully integrated into our real-time reactive vision system for gaze
control [18], [17] and provides continual updates of calibration
parameters to the other vision modules. Thirdly, the algorithm
uses a novel form of recursive filter, which allows observations
from an arbitrary number of tracked features to be incorpo-
rated. The variable state-dimension filter (VSDF) is a general
This work was supported by grants from the UK EPSRC (grant
GR/G30003), and from the EC's Esprit Programme (EP 5390).
The authors are with the Department of Engineering Science, University
of Oxford, Parks Road, Oxford, OX1 3PJ, UK; email pm or
dwm@robots.ox.ac.uk
solution for static estimation problems involving a global state
and a variable number of local states, coupled to the global state
but not to each other. In this work these are, respectively, the
calibration parameters and the visual directions of the tracked
features in the world. We show in Section III that the filter
can effectively deal with discarded states (corresponding to lost
features) and has time complexity linear with the number of
current local states (tracked features).
Our method has aspects in common with previously reported
work. Like Thacker & Courtney [23], we aim to improve estimates
over time by incorporating new image measurements as
they arrive. Like Hartley [10] we calibrate using rotational camera
motions only, and like Du & Brady [6] we use image features
tracked over multiple frames. All these researchers together
with Maybank & Faugeras [14] and Brooks et al. [4] emphasize
the benefits of self-calibration, a paradigm - which well describes
the present work - in which no special objects are used
for visual stimuli, in which calibration parameters are updated
over time and react to external disturbances, and in which calibration
proceeds automatically in the background while other
tasks are in progress.
In the following Section we discuss the calibration parameters
for a single camera directed by two rotation axes. In Sections
III and IV we give a detailed description of the algorithm, which
runs in parallel with the motion algorithms implemented on our
real-time vision system [17]. Results are presented in Section V
for two versions of the algorithm, demonstrating the generalizable
nature of our approach.
II. Geometrical parameters recovered
We consider the calibration parameters for a single camera
with two attached rotation axes, elevation and azimuth. The
camera and head geometry is sketched in Figure 1. The X;Y; Z
frame is the stationary or resting head frame, defined by setting
the X axis to coincide with the elevation axis, and the Y axis
to coincide with the azimuth axis in its arbitrary intial position.
After elevation by angle 'e and azimuth by angle 'a the gaze
frame is transformed to the (X fixed but
unknown transformation takes the head frame (X
the camera frame (x; z). The z-axis is the optic axis and
is by definition perpendicular to the x and y axes, which are
parallel to the image plane z = \Gammaf .
Image frame
Camera frame
optical centre
y
x
z
f
(X',Y',Z') frame to
frame
Transformation from
elevation axis
Y
Z
Z'
Head frames
azimuth axis
Fig. 1. The model used for a single camera mounted on a 2 degree
of freedom (elevation and azimuth) head. The configuration of
Yorick means that the azimuth axis corresponds to Y 0 , the Y
axis after elevation. Note the sense of the head angles: positive
elevation means the camera is looking up, positive azimuth means
that the camera is gazing to the left.
Ideally the transformation between (X;Y; Z) and (X
would be a pure rotation, and that between (X
would be an identity. Some head designs strive for this
ideal using vernier adjustable camera mounts. Although important
for applications in metrology, such effort seems mis-placed
for vision. Such devices cannot take account of internal misalignments
in the camera which change as, for example, zoom
is changed, which then must be determined by calibration. We
prefer to use less precise mounts, and then calibrate for the
misalignment.
The overall transformation from (X; Y; Z) to (x; y; z) can be
written a priori as the sum of a pure rotation and pure trans-
lation. The rotation comprises a part which can be changed
very precisely using the rotation axes, and a fixed unknown
part which arises from misalignment of the camera mount, camera
body, and optic axis (or effectively the CCD chip) within
the body. The translation, which will change as the rotation
changes, arises from small offsets between the rotation axes, a
relatively large displacement of the optic centre from the rotation
centre, and a small displacement of the principal point
from the notional image centre, (x; Because the
translation couples to the rotation it is theoretically possible to
determine it. However, the effect on visual data of the translational
component is also proportional to the inverse depth.
In practice, on the present platform, the effect is negligble for
There are two further matters relating to the camera. First,
and of minor significance as data for calibration is obtained close
to the image centre, is that of radial distortion: this is routinely
corrected for using the method described in [3]. Secondly, because
the aspect ratio of the image frame is imperfectly known
(because of lack of precise knowledge of hardware parameters
such as camera A/D rates), two independent focal lengths are
calculated for the x and y directions in the image.
To summarise then, first we assume that when points at sufficient
depth are viewed (in our case greater than 2m distant)
the transformation betweene (X; Y; Z) and (X
rotation which can be changed precisely. The mechanism of the
camera platform used, Yorick [22], provides information about
relative head angles to within 0:01 ffi , corresponding to 0.1 pixels
in the image. Importantly, proper account is taken of timing
delays in image capture and early processing so that head odometry
is mapped precisely to the corresponding image data [22].
Secondly, we assume the transformation between (X
and (x; frames is again a pure rotation. The three small
angles associated with it (each typically less than 3 ffi ) are recovered
by calibration, as are the two focal lengths of camera
fx and fy in the x and y directions which take account of the
aspect ratio.
A final assumption is that 3D points can be viewed which
are stationary relative to frames' origin. This simplifies the development
of a calibration algorithm since both intrinsic and
extrinsic estimated parameters will be constant. If the world
moves, one has to recover the scene motion as well as the cali-
bration, complicating matters considerably [14], [1], [11].
Consider a stationary scene point
in the stationary gaze frame (X; Y; Z) (thus P is con-
stant). We shall first determine which point (x; y) on the image
plane it projects to, given the current elevation and azimuth
angles 'e and 'a .
Elevation is a rotation by angle 'e about the X axis and
azimuth is a rotation by angle 'a about the elevated Y axis Y 0 ,
thus they are represented by the matrices
ce se
ce
\Gammas a 0 ca
e
where we have used the abbreviations
etc. Now define -
y and - z to be the unit vectors in the direction
of the camera (x; measured in the fixed (X;Y; Z)
frame. Let the small rotations from the (x; axes to the
. Then the rotation from the
camera frame to the head frame can be described by the matrix
\GammaOE y OE x 1
Hence combining all the rotations together we have (- x -
RaReR OE .
Now consider the fixed point P. Its 3D coordinates relative
to the camera are
The image coordinates (x; y) are related to the above by the
standard projection equations
We now set expressing the fact that we can only measure
the visual direction of the 3D point P. This will be a valid
simplification so long as the head angles 'e and 'a are kept well
away from the \Sigma90 ffi point, which is where PZ approaches zero.
Equations (2) provide the basis for an algorithm to estimate the
head parameters from observations of stationary points in the
world. This algorithm is decribed in Section IV, and uses the
variable state dimension filter described now.
III. The variable state dimension filter (VSDF)
We wish to recover calibration parameters from the trajectory
of tracked features. In essence this problem should not be
very difficult: combine the measurements each feature provides
about the state over time using (typically) a Kalman filter [2].
However there are complicating issues. First, each feature, as
well as providing information about the global state, i.e. the calibration
parameters, also has its own local state, the 3D visual
direction of the point in the world. Secondly, the visible features
in the world are constantly changing as the head rotates. The
combination of these implies that the full state vector, comprising
the local and global state vectors, is constantly changing
in size. A similar problem arose in the structure from motion
system of Harris et al. [9], which estimates egomotion from
tracked corner features. The solution chosen there was to have
separate Kalman filters running on local state vectors attached
to each corner, updating the global state vectors at the end
of each iteration. A good reason for using Harris's approach is
that the time complexity of the Kalman filter is a cubic function
of state vector size (because it involves matrix multiplications
and inversions), so implementing a full Kalman filter would be
prohibitive. However, the approach is sub-optimal because it
does not fully impose the rigidity constraint upon which the
egomotion problem was formulated.
One of the contributions of this correspondence is to show
that a certain class of estimation problems involving a single
global state and multiple local states can be formulated in such a
way that the time-complexity is linear with full state vector size,
thus making them computationally tractable. The observation
required to formulate this new method is that each local state is
coupled with its local measurement (feature position) and the
global state, but not to the other local states.
Our approach is exactly equivalent to the Kalman filter, but
uses a different formulation that brings out important aspects
of the chosen problem set. We take advantage of the observation
that the equations relating parameters to features do not
include relations between features. This means that the inverse
covariance matrix of the full state vector will contain mostly
zeroes, as is shown below. It turns out that this property of
the inverse covariance matrix means that it can be calculated
in time linear with the number of features using an updating
method as in the normal Kalman filter, and from this the state
vector can be determined, also in linear time. Because we use
inverse covariance, the filter is related to the information filter
[15]. The other benefit of our formulation is that it allows features
to be removed from the state vector, by simply removing
the corresponding row and column from the inverse covariance
matrix. Subsequent updates of inverse covariance and state vector
are exactly as if the features had been retained in the state
vector. In other words we can fit the state vector dimension to
the number of current features.
A. The filter in detail
We first discuss the operation of the filter in general terms,
and describe its use for calibration in Section IV. Let us write
the set of global parameters as a vector x, corresponding to the
focal lengths and mechanical alignment errors for the camera
calibration problem. Let the features being tracked have parameters
to be estimated y i , world co-
ordinates) and call these parameters the local parameters. The
whole parameter set x, is termed the full state
vector and is related to observations z i (j) at time j by the measurement
equation
z
where w i (j) is a Gaussian distributed vector with mean 0 and
covariance R i (j). Let the prior estimate of x be - x0 with covariance
P0 . We first consider the whole history of features up to
time k and define d i (j) as follows:
ae
is tracked at time step j
k. The maximum likelihood solution for
is the innovation. The first
term measures the disagreement between the full state vector
and the initial estimate -
x0 , while the second term includes all
measurements up to time k. Note that no state dynamics are
involved: x and y i are assumed to be constant vectors.
We digress at this point to analyse the simplified problem
of recursively estimating a single constant state vector x from
measurements z(j), and subsequently apply the results to the
complete system presented above. Given an estimate - x(k) of x
at time step k with covariance P (k) and a new measurement
1), the new estimate - x(k
should minimise
where R(k+1) is the covariance of the zero-mean measurement
1). Linearizing around the previous estimate - x(k)
and dropping the time step index for simplicity, we obtain the
update rules
is the Jacobian matrix @h=@x.
Both H and h (and hence also -) are evaluated at the old value
of -
x. The update of P is done before calculating xd . Since we are
assuming x to be constant, estimates of x should vary slowly, so
that the error introduced by the linearization should be small,
although it is an important source of error in the equivalent part
of the Extended Kalman Filter [2].
We can now apply these results to the case of estimating
multiple state vectors x and y i . Consider first bundling all the
into a single vector y. Thus the complete state vector is
We accordingly partition the inverse covariance
. The update rules for A,
and C given a new measurement z = h(x; y) +w are
A
where D and E are the individual Jacobians @h=@x and @h=@y.
To use Equation (5) we require P itself, which can be written
as:
which means that the changes in the state estimates are
The reason for using this form of the state update formulae
rather than directly using Eq. (5) is that when applied to the
case of multiple y's not coupled by the measurement process,
the above formulae have lower computational complexity. This
is the crux of the VSDF approach.
With multiple y i 's, each with measurements z
and with the noise vectors w i having covariances R i , We
can write z, R, D and E as "stacked" vectors and matrices:
R =B @
Because E and R are now block-diagonal, we can also write the
inverse covariance matrix as
The update rules for the terms of P \Gamma1 are
A / A+
Having updated P \Gamma1 , the state estimates are updated using the
increments
(D ?
Note that if there is no measurement for a local state i, corresponding
to d i in Eq. (3) being zero, B i and C i remain unchanged
and the i th term in the update of A is ignored. The
increment applied to - y i in this case is \GammaC
The above update formulae constitute the VSDF algorithm
for achieving the minimization of J in Eq. (4). By inspection it
is clear that the computational complexity of a complete update
for a single time-step is linear with n, as opposed to cubic if
Eq. (5) were to be used directly. It remains to describe how
the VSDF is initialized, and how the local state vectors y i are
added and removed.
A.1 Initialization
The filter is given an initial estimated value -
x0 of the global
state vector x and a covariance P0 . The following matrix and
scalar are initialized:
A.2 Introducing a new local state
A local state vector yn+1 is introduced with initial estimate
(determined from the initial measurement). New matrices
Bn+1 and Cn+1 are initialized to zero. Finally n is in-
cremented: n / n + 1. This procedure effectively adds a new
column and row block to the inverse covariance matrix P \Gamma1 .
A.3 Removing local states
A local state, y1 say, is removed from the state vector, when
no more data is available for it. We therefore remove row and
column block l in the inverse covariance matrix P \Gamma1 . However
we must ensure that this does not affect subsequent calculations
of the reduced state when new observations are made.
This is accomplished by subtracting from A the following term:
A
l . B l and C l are discarded and state n is
shifted down to position l in the remaining states to fill the gap.
Finally n is decremented: n / n \Gamma 1. Note that no extra storage
is required to maintain the information about discarded states.
IV. Implementing the Calibration Algorithm
The pair of equations (2) can be written as
with the definitions fy OE x OE y OE z
. The measurement function h is defined as h(x;
are the
rows of the rotation matrix R, defined in terms of 'e , 'a , OE x ,
OE y and OE z . x constitutes the global set of parameters to be
estimated and z the measured position of the projection of
y. We drop the subscripts i in this section and consider a single
tracked feature. To implement the VSDF recursive update we
need to calculate the Jacobian matrices of h with respect to x
and y, which are readily obtained by differentiation. We now
fill in details concerning implementation of camera calibration
in the VSDF framework.
A. Initialization
Initial values fx0 and fy0 of the focal lengths fx and fy in
pixels are estimated (where empirically the tolerable error can
be as high as - 50%). The angles OE x , OE y and OE z are likely to be
small so their initial values are set to zero. The initial covariance
P0 is set diagonal, with entries oe 2
f for the focal lengths and oe 2
OE
for angles, where oe 30pixel and oe 0:1radians - large to
indicate lack of confidence in these initial estimates. The effect
of the initial covariance declines over time of course.
B. Corner matching
Corners are detected [25] at frame-rate (25Hz) in a 64 \Theta 32
pixel central image window. The assumption that tracked feature
i corresponds to a stationary scene point means that the
position of a feature in a new image can be predicted from the
previous estimates -
x of the visual direction and calibration
parameters, and the new head angles 'e , 'a , using Eqs.
2. The feature is searched for in a 3 \Theta 3 pixel window centred
on the predicted position. If a corner is found the match
is accepted if the pixels around it are similar to those around
the feature detected in the previous image. This is tested by
summing the absolute differences in pixel grey-level value over
a 7 \Theta 7 window around each feature and dividing by the mean
pixel value of both, and if this ratio is less than 0.1 the match
is accepted. If more than one possible match is found, all are
rejected. This stringency reflects the need to to eliminate false
matches at the expense of losing some true matches. If a unique
match is found, the new feature position x incorporated
as a new observation (see next Section). If a match cannot be
found for a feature (or if there are ambiguous matches) it may
still reappear in subsequent frames. A feature is allowed to remain
"invisible" for three consecutive frames before removing
its local state vector P i .
Fig. 2 shows several frames taken during a run of the calibration
algorithm, with the tracked corners superimposed.
Fig. 2. Consecutive frames from the calibration algorithm. Matched,
unmatched and newly appeared corners are indicated respectively
by black, white and dotted black circles.
C. Incorporating new observations
The VSDF update procedure allows an arbitrary subset of
the current local states to have new observations
incorporated. This subset corresponds to those features i with
unique matches in the latest frame at image positions x
We have - z take the covariance R i of z i to be
diagonal with entries oe 2
xy , where oe corresponding
to the estimated random error in feature positions.
D. Adding a new local state
When a new corner feature appears with initial image
position the initial estimate of the 3D world
direction Pn+1 is determined from the current parameters by
rearranging Eqs. (2) into two linear equations in Px and Py .
The rotation matrix R is calculated using the latest estimate -
x
of x and for image position xn+1
'a at the time the image was taken. A new local state vector
is added using the procedure in Section III-A.2 and the first
observation is then passed to the VSDF.
General
Two versions of the algorithm have been implemented. The
first calibrates only for focal length, the other is the full calibration
algorithm described above. In both cases corner data
from every sixth frame of the 25Hz image stream is used, so the
algorithm runs at 4.25Hz. The focal length only algorithm can
easily be constructed by redefining x as (fx fy ) ? and removing
all terms in OE x , OE y and OE z from the equations in Section IV,
effectively forcing them to zero.
In order to run the calibration algorithm (either version) the
camera is first aligned on the head by hand. Initial values for
the focal lengths are provided and the algorithm is started, simultaneously
commencing head motion. The exact trajectory
does not matter so long as it is chosen to cover a large enough
range of the space of head angles. (E.g., if a purely horizontal
trajectory were chosen, it would be impossible to measure the
vertical focal length fy .) In our implementation, the head follows
a trajectory with constant angular speed of 6
elevation and azimuth until one of the angular range limits of
20 ffi for elevation and are reached, whereupon
the head reverses the relevant velocity, appearing to "bounce"
off the limit.
V. Results
A. Results for focal length algorithm
Fig. 3 shows results for five runs of the focal length algo-
rithm. The horizontal axis essentially measures time, but because
the amount of data in the form of corner matches varies
with time (there is not much to match on the ceiling, for exam-
ple) we instead use the cumulative number of corner matches
as the abscissa. The five runs, each lasting around 200 seconds,
used different starting values of (fx ; fy
(300,250), (350,650) and (650,275). Thus for initial errors up to
50% the algorithm still converges.
In each case the algorithm converged to the same values. The
mean value of fx taken at the end of each run is 523.3 with a
standard deviation of 1.7 pixels, i.e. 0.3% of the mean value.
The mean and standard deviation of fy are 391.8 and 1.46 (0.4%
of the mean). Also plotted is the prediction error. This is the
only absolute error measure available (equivalent to epipolar
error in a stereo calibration algorithm [24]). It measures the
root-mean-square (RMS) error between the predicted position
of a feature and its actual position in the image. Since the
tracking search window is limited to one pixel either side of
the predicted position, and corner pixel positions are integers,
the RMS error is in practice restricted to the range (0.5,1),
with occasional dips below 0.5. As can be seen, the RMS error
quickly drops to near 0.5, showing that the tracking error is
small.
The lack of a "ground-truth" for the calibration parameters
is not important for our current purposes. We wish to be able to
track moving objects using the general motion tracker [20], and
for this we must make predictions about image motion based
Parameter fx fy OE x OE y OE z
Unit pixels radians
Mean 528.3 387.8 -0.028 0.054 -0.0058
Std. dev. 0.9 1.0 0.007 0.011 0.0011
Ratio as % 0.2 0.3 25 20 20


I
Means and standard deviations of the calibration
parameters at the end of the runs shown in figure 4.
on known head motion. The RMS prediction error in the calibration
is exactly the measure that tells us how well this can be
done.
B. Results for the full algorithm
Eight runs of the full calibration algorithm are shown in
Fig. 4. As before the focal lengths quickly converge to the same
value, slightly different from the values in the previous experiment
because of the different calibration method and change in
focussing adjustment between the runs. The angular alignment
offsets take longer to converge, because of the smaller effect
they have. The runs each took about 10 minutes, at which
point the parameters were still converging. The results at the
end of the runs are summarized in Table I. As expected the
percentage errors in the alignment offsets are much larger than
for the focal lengths, because they are small and have less ef-
fect. Perhaps slightly surprising is a measurable cyclotorsion OE z
of about \Gamma0:3 ffi , indicating either a misalignment of the camera
mount or of the CCD within the camera. Utilising the coupling
between the image centre and the alignment offsets, we can see
that the "effective" image centre is located at (fx OE y ; \Gammaf y OE x ),
i.e. at (28; 11) for the results in table I. At present our general
motion detection and tracking algorithms [17], [20] use only the
focal lengths. These are sufficient to perform predictions from
image to image.
VI. Discussion and conclusion
We have applied the variable state-dimension filter to the
problem of calibrating a single camera mounted on a robot head
by tracking corner features and utilising accurate knowledge of
the camera rotation. The approach can be extended in several
ways. Firstly we could use any image features, for instance
line segments, as we have done for 3D structure and motion
recovery. We would like to explore the issue of robustness and
outlier detection more thoroughly than we have thus far. The
feature matching algorithm we employ is very restrictive and
does not allow many false matches, but there are situations
when the model can break down. If the calibration changes,
due for instance to manual refocussing, the VSDF will have to
be reset. The breakdown can theoretically be detected by testing
the residual J using a - 2 test on DOF degrees of freedom.
Mismatches for a given feature can also be tested for by measuring
the change in J that would occur if the measurements for
that feature were removed. The residual change is a - 2 random
variable. We have implemented this test, but space restrictions
prevent us giving details.
We believe that the variable state dimension filter is the first
algorithm that possesses all the essential properties for real-time
vision of recursiveness, low computational complexity with
repect to image data size, and statistically rigorous derivation.
It has already been applied to structure-from-motion recovery
where similar problems of varying state size also occur [16].
error
(pixels) x-focal length
time (matches)
(pixels) y-focal length
time (matches)
error
(pixels) prediction error
time (matches)
Fig. 3. Results for five runs of the focal length calibration algorithm, with different initial values of focal length. The horizontal axis measures
the cumulative number of matches. The time taken for each run was approximately 200 seconds.
error
(pixels) x-focal length
time (matches)
(pixels) y-focal length
time (matches) 2000 4000 6000 8000
-0.06
-0.020.020.06error
x-angle offset
2000 4000 6000 8000
-0.06
-0.020.020.06error
time (matches)
y-angle offset
2000 4000 6000 8000
error
z-angle offset
error
(pixels) prediction error
time (matches)
Fig. 4. Results for eight runs of the full calibration algorithm, including camera alignment offset angles. The horizontal axis measures the
cumulative number of matches.



--R

Euclidean structure from uncalibrated images.
Tracking and Data Associa- tion
Applications of projective geometry to robot vision.
Self calibration of motion and stereo vision for mobile robots.
Qualitative surface shape from deformation of image curves.

What can be seen in three dimensions with an uncalibrated stereo rig?
The calibration problem for stereo.
3D positional integration from image sequences.


Affine structure from motion.
Techniques for calibration of the scale factor and image center for high accuracy 3-d machine vision metrology
A theory of self-calibration of a moving camera
Stochastic Models
Recursive affine structure and motion from image sequences.
Driving saccade to pursuit using image motion.
Reactions to peripheral image motion using a head/eye platform.
Geometrical modelling from multiple stereo views.
Active tracking of foveated feature clusters using affine structure.
Using projective invariants for constant time library indexing in model based vision.
A modular head/eye platform for real-time reactive vision
Online calibration of a 4 dof stereo head.
Estimation of stereo and motion parameters using a variational principle.
Corner detection for 3D vision using array processors.
--TR

--CTR
Chung-Yi Lin , Sheng-Wen Shih , Yi-Ping Hung , Gregory Y. Tang, A new approach to automatic reconstruction of a 3-D world using active stereo vision, Computer Vision and Image Understanding, v.85 n.2, p.117-143, February 2002
Joss Knight , Ian Reid, Automated Alignment of Robotic Pan-Tilt Camera Units Using Vision, International Journal of Computer Vision, v.68 n.3, p.219-237, July      2006
