--T
Bayesian online classifiers for text classification and filtering.
--A
This paper explores the use of Bayesian online classifiers to classify text documents. Empirical results indicate that these classifiers are comparable with the best text classification systems. Furthermore, the online approach offers the advantage of continuous learning in the batch-adaptive text filtering task.
--B
INTRODUCTION
Faced with massive information everyday, we need automated
means for classifying text documents. Since hand-crafting
text classifiers is a tedious process, machine learning
methods can assist in solving this problem[15, 7, 27].
Yang & Liu[27] provides a comprehensive comparison of
supervised machine learning methods for text classification.
In this paper we will show that certain Bayesian classifiers
are comparable with Support Vector Machines[23], one
of the best methods reported in [27]. In particular, we
will evaluate the Bayesian online perceptron[17, 20] and the
Bayesian online Gaussian process[3].
For text classification and filtering, where the initial training
set is large, online approaches are useful because they
allow continuous learning without storing all the previously
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
SIGIR'02, August 11-15, 2002, Tampere, Finland.
seen data. This continuous learning allows the utilization
of information obtained from subsequent data after the initial
training. Bayes' rule allows online learning to be performed
in a principled way[16, 20, 17]. We will evaluate the
Bayesian online perceptron, together with information gain
considerations, on the batch-adaptive filtering task[18].
2. CLASSIFICATION AND FILTERING
For the text classification task defined by Lewis[9], we
have a set of predefined categories and a set of documents.
For each category, the document set is partitioned into two
mutually exclusive sets of relevant and irrelevant documents.
The goal of a text classification system is to determine whether
a given document belongs to any of the predefined cate-
gories. Since the document can belong to zero, one, or more
categories, the system can be a collection of binary classi-
fiers, in which one classifier classifies for one category.
In Text REtrieval Conference (TREC), the above task is
known as batch filtering. We will consider a variant of batch
filtering called the batch-adaptive filtering[18]. In this task,
during testing, if a document is retrieved by the classifier,
the relevance judgement is fed back to the classifier. This
feedback can be used to improve the classifier.
2.1 Corpora and Data
For text classification, we use the ModApte version of
the Reuters-21578 corpus 1 , where unlabelled documents are
removed. This version has 9,603 training documents and
test documents. Following [7, 27], only categories that
have at least one document in the training and test set are
retained. This reduces the number of categories to 90.
For batch-adaptive filtering, we attempt the task of TREC-
9[18], where the OHSUMED collection[6] is used. We will
evaluate on the OHSU topic-set, which consists of 63 topics.
The training and test material consist of 54,710 and 293,856
documents respectively. In addition, there is a topic statement
for each topic. For our purpose, this is treated as an
additional training document for that topic. We will only
use the title, abstract, author, and source sections of the
documents for training and testing.
2.2 Representation
There are various ways to transform a document into a
representation convenient for classification. We will use the
Available via http://www.daviddlewis.com/resources/
testcollections/reuters21578.
bag-of-words approach, where we only retain frequencies
of words after tokenisation, stemming, and stop-words re-
moval. These frequencies can be normalized using various
schemes[19, 6]; we use the ltc normalization:
l
l i,d  t i
j#{terms in d} (l j,d
where the subscripts i and d denote the ith term and the
dth document respectively, TF i,d is the frequency of the ith
term in the dth document, n i is the document-frequency of
the ith term, and N is the total number of documents.
2.3 Feature Selection Metric
Given a set of candidate terms, we select features from
the set using the likelihood ratio for binomial distribution
advocated by Dunning[5]:
is the number of relevant (non-relevant) training
documents which contain the term, R t (N t ) is the number
of relevant (non-relevant) training documents which do
not, and N is the total number of training documents.
Asymptotically, -2 ln # is # 2 distributed with 1 degree of
freedom. We choose terms with -2 ln # more than 12.13,
i.e. at 0.05% significance level. More details on the feature
selection procedures will be given in section 4.
2.4 Performance Measures
To evaluate a text classification system, we use the F1
measure introduced by van Rijsbergen[22]. This measure
combines recall and precision in the following way:
number of correct positive predictions
number of positive examples
number of correct positive predictions
number of positive predictions
Recall
For ease of comparison, we summarize the F1 scores over
the di#erent categories using the micro- and macro-averages
of F1 scores[11, 27]:
categories and documents
average of within-category F1 values.
The micro- and macro-average F1 emphasize the performance
of the system on common and rare categories re-
spectively. Using these averages, we can observe the e#ect
of di#erent kinds of data on a text classification system.
In addition, for comparing two text classification systems,
we use the micro sign-test (s-test) and the macro sign-test
(S-test), which are two significance tests first used for comparing
text classification systems in [27]. The s-test compares
all the binary decisions made by the systems, while
the S-test compares the within-category F1 values. Similar
to the F1 averages, the s-test and S-test compare the
performance of two systems on common and rare categories
respectively.
To evaluate a batch-adaptive filtering system, we use the
T9P measure of TREC-9[18]:
number of correct positive predictions
Max(50, number of positive predictions)
which is precision, with a penalty for not retrieving 50 documents

3. BAYESIAN ONLINE LEARNING
Most of this section is based on work by Opper[17], Solla
Suppose that each document is described by a vector x,
and that the relevance indicator of x for a category is given
by label y # {-1, 1}, where -1 and 1 indicates irrelevant
and relevant respectively. Given m instances of past data
the predictive probability of the
relevance of a document described by x is
Z
da p(y|x, a)p(a|Dm ),
where we have introduced the classifier a to assist us in the
prediction. In the Bayesian approach, a is a random variable
with probability density p(a|Dm ), and we integrate over all
the possible values of a to obtain the prediction.
Our aim is to obtain a reasonable description of a. In
the Bayesian online learning framework[16, 20, 17], we begin
with a prior p(a|D0 ), and perform incremental Bayes'
updates to obtain the posterior as data arrives:
p(y t+1 |x t+1 , a)p(a|D t )
R da p(y t+1 |x t+1 , a)p(a|D t )
To make the learning online, the explicit dependence of
the posterior p(a|D t+1) on the past data is removed by approximating
it with a distribution p(a|A t+1 ), where A t+1
characterizes the distribution of a at time t + 1. For exam-
ple, if p(a|A t+1) is a Gaussian, then A t+1 refers to its mean
and covariance.
Hence, starting from the prior
from a new example (y t+1 , x t+1) comprises two steps:
Update the posterior using Bayes rule
Approximate the updated posterior by parameterisation
where the approximation step is done by minimizing the
Kullback-Leibler distance between the the approximating and
approximated distributions.
The amount of information gained about a after learning
from a new example can be expressed as the Kullback-Leibler
distance between the posterior and prior distribu-
IG(y t+1 , x t+1|D t
Z
da p(a|D t+1) log 2
Z
da p(a|A t+1) log 2
where instances of the data D are replaced by the summaries
A in the approximation.
To simplify notation henceforth, we use p t (a) and # t to
denote p(a|A t ) and averages taken over p(a|A t ) respectively.
For example, the predictive probability can be rewritten as
Z
da p(y|x, a)p t (a) = #p(y|x, a)# t .
In the following sections, the scalar field
be used to simplify notation and calculation.
3.1 Bayesian Online Perceptron
Consider the case where a describes a perceptron. We then
define the likelihood as a probit model
ya  x
0 is a fixed noise variance, and # is the cumulative
Gaussian distribution
Z u
-#
If p0 (a) is the spherical unit Gaussian, and p t (a) is the
Gaussian approximation, Opper[16, 17] and Solla &
obtain the following updates by equating the means and covariances
of p(a|A t+1) and p(a|A t , (y t+1 , x t+1 )):
where
#p(y t+1 |h)#
y t+1 #h# t
3.1.1 Algorithm
Training the Bayesian online perceptron on m data involves
successive calculation of the means #a# t and covariances
C t of the posteriors, for t # {1, ., m}:
1. Initialize #a# 0 to be 0 and C0 to be 1 (identity matrix),
i.e. a spherical unit Gaussian centred at origin.
2. For
3. y t+1 is the relevance indicator for document x t+1
4. Calculate s t+1 , # t+1 , #h# t and #p(y t+1 |h)# t
5. Calculate
and
6. Calculate #
#p(y t+1 |h)# t
7. Calculate # 2
#p(y t+1 |h)# t
#p(y t+1 |h)# t
8. Calculate #a# t+1 and C t+1
The prediction for datum (y, x) simply involves the calculation
of #p(y|x, a)#
3.2 Bayesian Online Gaussian Process
Gaussian process (GP) has been constrained to problems
with small data sets until recently when Csato & Opper[3]
and Williams & Seeger[24] introduced e#cient and e#ective
approximations to the full GP formulation. This section will
outline the approach in [3].
In the GP framework, a describes a function consisting of
function values {a(x)}. Using the probit model, the likelihood
can be expressed as
where #0 and # are described in section 3.1.
In addition, p0(a) is a GP prior which specifies a Gaussian
distribution with zero mean function and covariance/kernel
function K0 (x, x # ) over a function space. If p t (a) is also a
Gaussian process, then Csato & Opper obtain the following
updates by equating the means and covariances of p(a|A t+1)
and p(a|A t , (y t+1 , x t+1 )):
where
#p(y t+1 |h)#
y t+1 #h# t
Notice the similarities to the updates in section 3.1. The
main di#erence is the 'kernel trick' introduced into the equations
through
New inputs x t+1 are added sequentially to the system via
the 1)th unit vector e t+1 . This results in a quadratic
increase in matrix size, and is a drawback for large data
sets, such as those for text classification. Csato & Opper
overcome this by introducing sparseness into the GP. The
idea is to replace e t+1 by the projection
where
This approximation introduces an error
which is used to decide when to employ the approximation.
Hence, at any time the algorithm holds a set of basis vec-
tors. It is usually desirable to limit the size of this set. To
accommodate this, Csato & Opper describe a procedure for
removing a basis vector from the set by reversing the process
of adding new inputs.
For lack of space, the algorithm for the Bayesian Online
Gaussian Process will not be given here. The reader is referred
to [3] for more information.
4. EVALUATION
4.1 Classification on Reuters-21578
In this evaluation, we will compare Bayesian online per-
ceptron, Bayesian online Gaussian process, and Support Vector
Machines (SVM)[23]. SVM is one of the best performing
learning algorithms on the Reuters-21578 corpus[7, 27].
The Bayesian methods are as described in section 3, while
for SVM we will use the SV M light package by Joachims[8].
Since SVM is a batch method, to have a fair comparison,
the online methods are iterated through the training data 3
times before testing. 2
4.1.1 Feature Selection
For the Reuters-21578 corpus, we select as features for
each category the set of all words for which -2 ln # > 12.13.
We further prune these by using only the top 300 features.
This reduces the computation time required for the calculation
of the covariances of the Bayesian classifiers.
Since SVM is known to perform well for many features,
for the SVM classifiers we also use the set of words which
occur in at least 3 training documents[7]. This gives us 8,362
words. Note that these words are non-category specific.
4.1.2 Thresholding
The probabilistic outputs from the Bayesian classifiers can
be used in various ways. The most direct way is to use the
Bayes decision rule, to determine
the relevance of the document described by x. 3 However,
as discussed in [10, 26], this is not optimal for the chosen
evaluation measure.
Therefore, in addition to 0.5 thresholding, we also empirically
optimise the threshold for each category for the F1
measure on the training documents. This scheme, which we
shall call MaxF1, has also been employed in [27] for thresholding
kNN and LLSF classifiers. The di#erence from our
approach is that the threshold in [27] is calculated over a
validation set. We do not use a validation set because we
feel that, for very rare categories, it is hard to obtain a reasonable
validation set from the training documents.
For the Bayesian classifiers, we also perform an analytical
threshold optimisation suggested by Lewis[10]. In this
scheme, which we shall call ExpectedF1, the threshold for
each category is selected to optimise the expected F1 :
|D +|+
otherwise,
where # is the threshold, p i is the probability assigned to
document i by the classifier, D is the set of all test docu-
ments, and D+ is the set of test documents with probabilities
higher than the threshold #.
Note that ExpectedF1 can only be applied after the probabilities
for all the test documents are assigned. Hence the
classification can only be done in batch. This is unlike the
first two schemes, where classification can be done online.
4.1.3 Results and Discussion
section A.2 for discussion on the number of passes.
3 For SVM, to minimise structural risks, we would classify
the document as relevant if w  x is the
hyperplane, and b is the bias.
section A.3 for discussion on the jitter terms # ij .

Table

1: Description of Methods
Description 4
Perceptron fixed feature (for bias)

Table

2: Micro-/Macro-average F1
Perceptron 85.12 / 45.23 86.69 / 52.16 86.44 / 53.08

Table

1 lists the parameters for the algorithms used in our
evaluation, while Table 2 and 3 tabulate the results. There
are two sets of results for SVM, and they are labeled SVMa
and SVM b . The latter uses the same set of features as the
Bayesian classifiers (i.e. using the -2 ln # measure), while
the former uses the set of 8,362 words as features.

Table

2 summarizes the results using F1 averages. Table
3 compares the classifiers using s-test and S-test. Here, the
MaxF1 thresholds are used for the classification decisions.
Each row in these tables compares the method listed in the
first column with the other methods. The significance levels
from [27] are used.
Several observations can be made:
. Generally, MaxF1 thresholding increases the performance
of all the systems, especially for rare categories.
. For the Bayesian classifiers, ExpectedF1 thresholding
improves the performance of the systems on rare categories

. Perceptron implicitly implements the kernel used by
GP-1, hence their similar results.
. With MaxF1 thresholding, feature selection impedes
the performance of SVM.
. In

Table

2, SVM with 8,362 features have slightly lower
micro-average F1 to the Bayesian classifiers. However,
the s-tests in Table 3 show that Bayesian classifiers
outperform SVM for significantly many common cat-
egories. Hence, in addition to computing average F1
measures, it is useful to perform sign tests.
. As shown in Table 3, for limited features, Bayesian
classifiers outperform SVM for both common and rare
categories.
. Based on the sign tests, the Bayesian classifiers outperform
(using 8,362 words) for common categories,
and vice versa for rare categories.

Table

3: s-test/S-test using MaxF1 thresholding
Perceptron #
"#" or "#" means P-value # 0.01; ">" or "<" means 0.01 < P-value # 0.05; "#" means P-value > 0.05.
The last observation suggests that one can use Bayesian
classifiers for common categories, and SVM for rare ones.
4.2 Filtering on OHSUMED
In this section, only the Bayesian online perceptron will
be considered. In order to avoid numerical integration of
the information gain measure, instead of the probit model
of section 3.1, here we use a simpler likelihood model in
which the outputs are flipped with fixed probability #:
where
The update equations will also change accordingly, e.g.
#p(y t+1 |h)#
y t+1 #h# t
Using this likelihood measure, we can express the information
gained from datum (y t+1 , x t+1) as
log

c
log 2
where
c
We use evaluation. The following sections
will describe the algorithm in detail. To simplify presen-
tation, we will divide the batch-adaptive filtering task into
batch and adaptive phases.
4.2.1 Feature Selection and Adaptation
During the batch phase, words for which -2 ln # > 12.13
are selected as features.
During the adaptive phase, when we obtain a feedback, we
update the features by adding any new words with -2
12.13. When a feature is added, the distribution of the perceptron
a is extended by one dimension:
4.2.2 Training the classifier
During the batch phase, the classifier is iterated through
the training documents 3 times. In addition, the relevant
documents are collected for use during the adaptive phase.
During the adaptive phase, retrieved relevant documents
are added to this collection. When a document is retrieved,
the classifier is trained on that document and its given relevance
judgement.
The classifier will be trained on irrelevant documents most
of the time. To prevent it from "forgetting" relevant documents
due to its limited capacity, whenever we train on an
irrelevant document, we would also train on a past relevant
document. This past relevant document is chosen successively
from the collection of relevant documents.
This is needed also because new features might have been
added since a relevant document was last trained on. Hence
the classifier would be able to gather new information from
the same document again due to the additional features.
Note that the past relevant document does not need to be
chosen in successive order. Instead, it can be chosen using
a probability distribution over the collection. This will be
desirable when handling topic-drifts.
We will evaluate the e#ectiveness of this strategy of re-training
on past retrieved relevant documents, and denote
its use by +rel. Though its use means that the algorithm
is no longer online, asymptotic e#ciency is una#ected, since
only one past document is used for training at any instance.
4.2.3 Information Gain
During testing, there are two reasons why we retrieve
a document. The first is that it is relevant, i.e.
represents the document. The second
is that, although the document is deemed irrelevant
by the classifier, the classifier would gain useful information
from the document. Using the measure IG(y, x|D t ), we calculate
the expected information gain
0.40.8N ret
q Target number of

Figure

1: # versus Nret tuned for T9P
A document is then deemed useful if its expected information
gain is at least #. Optimizing for the T9P measure
(i.e. targeting 50 documents), we choose # to be
where N ret is the total number of documents that the system
has retrieved. Figure 1 plots # against N ret . Note that this
is a kind of active learning, where the willingness to tradeo#
precision for learning decreases with N ret . The use of this
information gain criteria will be denoted by +ig.
We will test the e#ectiveness of the information gain strat-
egy, against an alternative one. The alternative, denoted by
+rnd, will randomly select documents to retrieve based on
the probability
50-N ret
293856 otherwise,
where 293,856 is the number of test documents.
4.2.4 Results and Discussion

Table

4 lists the results of seven systems. The first two are
of Microsoft Research Cambridge and Fudan University re-
spectively. These are the only runs in TREC-9 for the task.
The third is of the system as described in full, i.e. Bayesian
online perceptron, with retraining on past retrieved relevant
documents, and with the use of information gain. The rest
are of the Bayesian online perceptron with di#erent combinations
of strategies.
Besides the T9P measure, for the sake of completeness, Table
4 also lists the other measures used in TREC-9. Taken
together, the measures show that Bayesian online percep-
tron, together with the consideration for information gain,
is a very competitive method.
For the systems with +rel, the collection of past known
relevant documents is kept. Although Microsoft uses this
same collection for its query reformulation, another collection
of all previously seen documents is used for threshold
adaptation. Fudan maintains a collection of past retrieved
documents and uses this collection for query adaptation.
reports results from run ok9bfr2po, while we report
results from the slightly better run ok9bf2po.
Average number of relevant documents retrieved
Average
number
of
features
Pptron+rel+ig
Pptron+ig
Pptron+rnd
Pptron

Figure

2: Variation of the number of features as
relevant documents are retrieved. The plots for
Pptron+rel+ig and Pptron+ig are very close. So are
the plots for Pptron+rnd and Pptron.
In a typical operational system, retrieved relevant documents
are usually retained, while irrelevant documents are
usually discarded. Therefore +rel is a practical strategy to
adopt.

Figure

2 plots the average number of features during the
adaptive phase. We can see that features are constantly
added as relevant documents are seen. When the classifier
is retrained on past documents, the new features enable the
classifier to gain new information from these documents. If
we compare the results for Pptron+rel and Pptron in Table
4, we find that not training on past documents causes
the number of relevant documents retrieved to drop by 5%.
Similarly, for Pptron+rel+ig and Pptron+ig, the drop is
8%.

Table

5 breaks down the retrieved documents into those
that the classifier deems relevant and those that the classifier
is actually querying for information, for Pptron+ig
and Pptron+rnd. The table shows that none of the documents
randomly queried are relevant documents. This is
not surprising, since only an average of 0.017% of the test
documents are relevant. In contrast, the information gain
strategy is able to retrieve 313 relevant documents, which is
26.1% of the documents queried. This is a significant result.
Consider Pptron+ig. Table 4 shows that for Pptron, when
the information gain strategy is removed, only 731 relevant
documents will be retrieved. Hence, although most of the
documents queried are irrelevant, information gained from
these queries helps recall by the classifier (i.e. 815 documents
versus 731 documents), which is important for reaching
the target of 50 documents.
MacKay[13] has noted the phenomenon of querying for
irrelevant documents which are at the edges of the input
space, and suggested maximizing information in a defined
region of interest instead. Finding this region for batch-
adaptive filtering remains a subject for further research.
Comparing the four plots in Figure 2, we find that, on
average, the information gain strategy causes about 3% more
features to be discovered for the same number of relevant
documents retrieved. A consequence of this is better recall.

Table

4: Results for Batch-adaptive filtering optimized for T9P measure.
Microsoft 5 Fudan Pptron+rel+ig Pptron+ig Pptron+rnd Pptron+rel Pptron
Total retrieved 3562 3251 2716 2391 2533 1157 1057
Relevant retrieved 1095 1061 1227 1128 732 772 731
Macro-average recall 39.5 37.9 36.2 33.3 20.0 20.8 20.0
Macro-average precision 30.5 32.2 35.8 35.8 21.6 61.9 62.3
Mean T9P 30.5 31.7 31.3 29.8 19.2 21.5 20.8
Mean Utility -4.397 -1.079 15.318 15.762 -5.349 18.397 17.730
Mean T9U -4.397 -1.079 15.318 15.762 -5.349 18.397 17.730
Mean scaled utility -0.596 -0.461 -0.025 0.016 -0.397 0.141 0.138
Zero returns

Table

5: Breakdown of documents retrieved for Pptron+ig and Pptron+rnd. The numbers for the latter are in
brackets.
Relevant Not Relevant Total
docs retrieved by perceptron classifier proper 815 (732) 378 (345) 1193 (1077)
docs retrieved by information gain (or random strategy) 313 (0) 885 (1456) 1198 (1456)
Total 1128 (732) 1263 (1801) 2391 (2533)
5. CONCLUSIONS AND FURTHER WORK
We have implemented and tested Bayesian online perceptron
and Gaussian processes on the text classification prob-
lem, and have shown that their performance is comparable
to that of SVM, one of the best learning algorithms on
text classification in the published literature. We have also
demonstrated the e#ectiveness of online learning with information
gain on the TREC-9 batch-adaptive filtering task.
Our results on text classification suggest that one can use
Bayesian classifiers for common categories, and maximum
margin classifiers for rare categories. The partitioning of the
categories into common and rare ones in an optimal way is
an interesting problem.
SVM has been employed to use relevance feedback by
Drucker et al [4], where the retrieval is in groups of 10 doc-
uments. In essence, this is a form of adaptive routing. It
would be instructive to see how Bayesian classifiers perform
here, without storing too many previously seen documents.
It would also be interesting to compare the merits of incremental
SVM[21, 1] with the Bayesian online classifiers.

Acknowledgments

We would like to thank Lehel Csato for providing details
on the implementation of the Gaussian process, Wee Meng
Soon for assisting in the data preparation, Yiming Yang
for clarifying the representation used in [27], and Loo Nin
Teow for proof-reading the manuscript. We would also like
to thank the reviewers for their many helpful comments in
improving the paper.
6.



--R

Incremental and decremental support vector machine learning.
Analysis of Binary Data.

Relevance feedback using support vector machines.
Accurate methods for the statistics of surprise and coincidence.
OHSUMED: An interactive retrieval evaluation and new large test collection for research.
Text categorization with support vector machines: Learning with many relevant features.
Making large-scale SVM learning practical
Representation and Learning in Information Retrieval.
Evaluating and optimizing automomous text classification systems.
Training algorithms for linear text classifiers.
Bayesian interpolation.

Monte Carlo implementation of Gaussian process models for Bayesian regression and classification.
Feature selection
Online versus
A Bayesian approach to online learning.
The TREC-9 filtering track final report

Optimal perceptron learning: an online Bayesian approach.
Incremental learning with support vector machines.
Information Retrieval.
The Nature of Statistical Learning Theory.
Using the Nystrom method to speed up kernel machines.
Bayesian Mean Field Algorithms for Neural Networks and Gaussian Processes.
A study on thresholding strategies for text categorization.

--TR
Term-weighting approaches in automatic text retrieval
Representation and learning in information retrieval
Bayesian interpolation
Information-based objective functions for active data selection
OHSUMED
The nature of statistical learning theory
Evaluating and optimizing autonomous text classification systems
Training algorithms for linear text classifiers
Feature selection, perception learning, and a usability case study for text categorization
Making large-scale support vector machine learning practical
A Bayesian approach to on-line learning
Optimal perceptron learning
A re-examination of text categorization methods
A study of thresholding strategies for text categorization
Information Retrieval
Text Categorization with Suport Vector Machines
Relevance Feedback using Support Vector Machines

--CTR
Kian Ming Adam Chai, Expectation of f-measures: tractable exact computation and some empirical observations of its properties, Proceedings of the 28th annual international ACM SIGIR conference on Research and development in information retrieval, August 15-19, 2005, Salvador, Brazil
Hwanjo Yu , ChengXiang Zhai , Jiawei Han, Text classification from positive and unlabeled documents, Proceedings of the twelfth international conference on Information and knowledge management, November 03-08, 2003, New Orleans, LA, USA
Rey-Long Liu, Dynamic category profiling for text filtering and classification, Information Processing and Management: an International Journal, v.43 n.1, p.154-168, January 2007
Randa Kassab , Jean-Charles Lamirel, Towards a synthetic analysis of user's information need for more effective personalized filtering services, Proceedings of the 2007 ACM symposium on Applied computing, March 11-15, 2007, Seoul, Korea
Vaughan R. Shanks , Hugh E. Williams , Adam Cannane, Indexing for fast categorisation, Proceedings of the twenty-sixth Australasian conference on Computer science: research and practice in information technology, p.119-127, February 01, 2003, Adelaide, Australia
Rey-Long Liu , Wan-Jung Lin, Adaptive sampling for thresholding in document filtering and classification, Information Processing and Management: an International Journal, v.41 n.4, p.745-758, July 2005
Aynur Dayanik , David D. Lewis , David Madigan , Vladimir Menkov , Alexander Genkin, Constructing informative prior distributions from domain knowledge in text classification, Proceedings of the 29th annual international ACM SIGIR conference on Research and development in information retrieval, August 06-11, 2006, Seattle, Washington, USA
new topic identification using multiple linear regression, Information Processing and Management: an International Journal, v.42 n.4, p.934-950, July 2006
Franca Debole , Fabrizio Sebastiani, An analysis of the relative hardness of Reuters-21578 subsets: Research Articles, Journal of the American Society for Information Science and Technology, v.56 n.6, p.584-596, April 2005
