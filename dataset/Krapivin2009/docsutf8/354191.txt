--T
Self-Calibration of a 1D Projective Camera and Its Application to the Self-Calibration of a 2D Projective Camera.
--A
AbstractWe introduce the concept of self-calibration of a 1D projective camera from point correspondences, and describe a method for uniquely determining the two internal parameters of a 1D camera, based on the trifocal tensor of three 1D images. The method requires the estimation of the trifocal tensor which can be achieved linearly with no approximation unlike the trifocal tensor of 2D images and solving for the roots of a cubic polynomial in one variable. Interestingly enough, we prove that a 2D camera undergoing planar motion reduces to a 1D camera. From this observation, we deduce a new method for self-calibrating a 2D camera using planar motions. Both the self-calibration method for a 1D camera and its applications for 2D camera calibration are demonstrated on real image sequences.
--B
Introduction
A CCD camera is commonly modeled as a 2D projective device that projects a point in P 3 (the
projective space of dimension 3) to a point in P 2 . By analogy, we can consider what we call a 1D
projective camera which projects a point in P 2 to a point in P 1 . This 1D projective camera may seem
very abstract, but many imaging systems using laser beams, infra-red or ultra-sound acting only on a
source plane can be modeled this way. What is less obvious, but more interesting for our purpose, is
that in some situations, the usual 2D camera model is also closely related to this 1D camera model.
One first example might be the case of the 2D affine camera model operating on line segments: The
direction vectors of lines in 3D space and in the image correspond to each other via this 1D projective
camera model [21]. Other cases will be discussed later.
In this paper, we first introduce the concept of self-calibration of a 1D projective camera by analogy
to that of a 2D projective camera which is a very active topic [17, 12, 7, 13, 1, 29, 20] since the
pioneering work of [18]. It turns out that the theory of self-calibration of 1D camera is considerably
simpler than the corresponding one in 2D. It is essentially determined in a unique way by a linear
algorithm using the trifocal tensor of 1D cameras. After establishing this result, we further investigate
the relationship between the usual 2D camera and the 1D camera. It turns out that a 2D camera
undergoing a planar motion can be reduced to a 1D camera on the trifocal line of the 2D cameras.
This remarkable relationship allows us to calibrate a real 2D projective camera using the theory of
self-calibration of a 1D camera. The advantage of doing so is evident. Instead of solving complicated
Kruppa equations for 2D camera self-calibration, exact linear algorithm can be used for 1D camera
self-calibration. The only constraint is that the motion of the 2D camera should be restricted to planar
motions. The other applications, including 2D affine camera calibration, are also briefly discussed.
Part of this work was also presented in [10].
The paper is organised as follows. In Section 2, we review the 1D projective camera and its
trifocal tensor. Then, an efficient estimation of the trifocal tensor is discussed in Section 3. The
theory of self-calibration of a 1D camera is introduced and developed in Section 4. After pointing
out some direct applications of the theory in Section 5, we develop in Section 6 a new method of 2D
camera self-calibration by converting a 2D camera undergoing planar motions into a 1D camera. The
experimental results on both simulated and real image sequences are presented in Section 7. Finally,
some concluding remarks and future directions are given in Section 8.
Throughout the paper, vectors are denoted in lower case boldface, matrices and tensors in upper
case boldface. Some basic tensor notation is used: Covariant indices as subscripts, contravariant
indices as superscripts and the implicit summation convention.
projective camera and its trifocal tensor
We will first review the one-dimensional camera which was abstracted from the study of the geometry
of lines under affine cameras [21]. We can also introduce it directly by analogy to a 2D projective
camera.
A 1D projective camera projects a point (projective plane) to a point
(projective line). This projection may be described by a 2 \Theta 3 homogeneous
matrix M as
x:
We now examine the geometric constraints available for points seen in multiple views similar to
the 2D camera case [23, 24, 13, 28, 9]. There is a constraint only in the case of 3 views, as there is no
any constraint for 2 views (two projective lines always intersect in a point in a projective plane).
Let the three views of the same point x be given as
(1)
These can be rewritten in matrix form as
0: The vector
cannot be zero, so
0: (2)
The expansion of this determinant produces a trifocal constraint for the three views
where T ijk is a 2 \Theta 2 \Theta 2 homogeneous tensor whose components T ijk are 3 \Theta 3 minors (involving
all three views) of the following 6 \Theta 3 joint projection matrix:
The components of the tensor can be made explicit as T where the
bracket denotes the 3 \Theta 3 minor of i-th,j 0 -th and k 00 -th row vector of the above joint projection
matrix and bar "-" in - i, - j and - k denotes the mapping (1; 2) 7! (2; \Gamma1):
It can be easily seen that any constraint obtained by adding further views reduces to a trilinearity.
This proves the uniqueness of the trilinear constraint. Moreover, the 2 \Theta 2 \Theta 2 homogeneous tensor
has so it is a minimal parametrization of three views in the uncalibrated
setting since three views have exactly 3 \Theta (2 \Theta 3 \Gamma to a projective
transformation in P 2 .
This result for the one-dimensional projective camera is very interesting. The trifocal tensor
encapsulates exactly the information needed for projective reconstruction in P 2 . Namely, it is the
unique matching constraint, it minimally parametrizes the three views and it can be estimated linearly.
Contrast this to the 2D image case in which the multilinear constraints are algebraically redundant
and the linear estimation is only an approximation based on over-parametrization.
3 Estimation of the trifocal tensor of a 1D camera
Each point correspondence in 3 views u yields one homogeneous linear equation for the
8 tensor components T ijk for 2:
With at least 7 point correspondences, we can solve for the tensor components linearly.
A careful normalisation of the measurement matrix is nevertheless necessary just like that stressed
in [11] for the linear estimation of the fundamental matrix. The points at each image are first translated
so that the centroid of the points is the origin of the image coordinates, then scaled so that the average
distance of the points from the origin is 1. This is achieved by an affine transformation of the image
coordinates in each image: -
With these normalised image coordinates, the normalised tensor components -
T ijk are linearly
estimated by SVD from -
The original tensor components T ijk are recovered by de-scaling the normalised tensor -
T ijk as
a
c
4 Self-calibration of a 1D camera from 3 views
The concept of camera self-calibration using only point correspondences became popular in computer
vision community following Maybank and Faugeras [18] by solving the so-called Kruppa equations.
The basic assumption is that the internal parameters of the camera remain invariant. In the case of
the 2D projective camera, the internal calibration (the determination of the 5 internal parameters) is
equivalent to the determination of the image ! of the absolute conic in P 3 .
4.1 The internal parameters of a 1D camera and the circular points
For a 1D camera represented by a 2 \Theta 3 projection matrix M 2\Theta3
, this projection matrix can always be
decomposed into
R 2\Theta2
where K
ff
represents the two internal parameters: ff the focal length in pixels and u 0
the position of the principal point; the external parameters are represented by a 2 \Theta 2 rotation matrix
R 2\Theta2
cos ' sin '
sin ' cos '
and the translation vector t 2\Theta1
The object space for a 1D camera is a projective plane, and any rigid motion on the plane leaves
the two circular points I and J invariant (a pair of complex conjugate points on the line at infinity) of
the plane. Similar to the 2D camera case where the knowledge of the internal parameters is equivalent
to that of the image of the absolute conic, the knowledge of the internal parameters of a 1D camera is
equivalent to that of the image points i and j of the circular points in P 2 .
The relationship between the image of the circular points and the internal parameters of the 1D
camera follows directly by projecting one of the circular points I = (i;
\Gamma1, by
the camera M 2\Theta3
ff
R 2\Theta2
@
It clearly appears that the real part of the ratio of the projective coordinates of the image of the
circular point i is the position of the principal point u
and the imaginary part is the focal length ff.
4.2 Determination of the images of the circular points
Our next task is to locate the circular points in the images. Let us consider one of the circular points,
say I . This circular point is projected onto i, i 0 and i 00 in the three views. As they should be invariant
because of our assumption that the internal parameters of the camera are constant, we have:
where
The triplet of corresponding points i satisfies the trilinear constraint (3) as all corresponding
points do, therefore, T ijk 0: This yields the following cubic
equation in the unknown
A cubic polynomial in one unknown with real coefficients has in general either three real roots or one
real root and a pair of complex conjugate roots. The latter case of one real and a pair of complex
conjugates is obviously the case of interest here. In fact, Equation (4) characterizes all the points of
the projective plane which have the same coordinates in three views. This is reminiscent of the 3D
case where one is interested in the locus of all points in space that project onto the same point in
two views (see Section 6). The result that we have just obtained is that in the case where the internal
parameters of the camera are constant, there are in general three such points: the two circular points
which are complex conjugate, and a real point with the following geometric interpretation.
Consider first the case of two views and let us ask the question, what is the set of points such
that their images in the two views are the same? This set of points can be called the 2D horopter
(h) of the set of two 1D views. Since the two cameras have the same internal parameters we can
ignore them and assume that we work with the calibrated pixel coordinates. In that case, a camera
can be identified to an orthonormal system of coordinates centered at the optical center, one axis is
parallel to the retina, the other one is the optical axis. The two views correspond to each other via a
rotation followed by a translation. This can always be described in general as a pure rotation around a
point A whose coordinates can easily be computed from the cameras' projection matrices. A simple
computation then shows that the horopter (h) is the circle going through the two optical centers and
A, as illustrated in Figure 1.a. In fact it is the circle minus the two optical centers. Note that since all
circles go through the circular points (hence their name), they also belong to the horopter curve, as
expected.
In the case of three views, the real point, when it exists, must be at the intersection of the horopter
of the first two views and the horopter (h 23 ) of the last two views. The first one is a circle going
through the optical centers C 1
and C 2
, the second one is a circle going through the optical centers C 2
and C 3
. Those two circles intersect in general at a second point C which is the real point we were
discussing, and the third circle (h 13
corresponding to the first and third views must also go through
the real point C, see Figure 1.b.
a.
First camera
Second camera
center of rotation
b.C
Second camera
C2First camera
Third camera

Figure

1: a. The two dimensional horopter which is the set of points having the same coordinates in
the 2 views (see text). b. The geometric interpretation of the real point C which has the same images
in all three views (see text).
We have therefore established the interesting result that the internal parameters of a 1D camera can
be uniquely determined through at least 7 point correspondences in 3 views: the seven points yield
the trifocal tensor and Equation (4) yields the internal parameters.
Applications
The theory of self-calibration of 1D camera is considerably simpler than the corresponding one in
2D [18] and can be directly used whenever a 1D projective camera model occurs, for instance: self-calibration
of some active systems using laser beams, infra-red [3] or ultra-sound whose imaging
system is basically reduced to a 1D camera on the source plane; and partial/full self-calibration of 2D
projective camera using planar motions.
The first type of applications is straightforward. The interesting observation is that the 1D calibration
procedure can also be used for self-calibrating a real 2D projective camera if the camera motion
is restricted to planar motions. This is discussed in detail in the remaining of this paper.
6 Calibrating a 2D projective camera using planar motions
A planar motion consists of a translation in a plane and a rotation about an axis perpendicular to that
plane. Planar motion is often performed by a vehicle moving on the ground, and has been used for
camera self-calibration by Beardsley and Zisserman [4] and by Armstrong et al. [1].
Recall that the self-calibration of a 2D projective camera [8, 18] consists of determining the 5
unchanging internal parameters of a 2D camera, represented by a 3 \Theta 3 upper triangular matrix
This is mathematically equivalent to the determination of the image of the absolute conic
!, which is a plane conic described by x Given the image
of the absolute conic x T the calibration matrix K can be found from C using the Choleski
decomposition.
Converting 2D images into 1D images
For a given planar motion, the trifocal plane-the plane through the camera centers-of the camera
is coincident with the motion plane as the camera is moving on it. Therefore the image location
of the motion plane is the same as the trifocal line which could be determined from fundamental
matrices. The determination of the image location of the motion plane has been reported in [1, 4].
Obviously, if restricting the working space to the trifocal plane, we have a perfect 1D projective camera
model which projects the points of the trifocal plane onto the trifocal line in the 2D image plane,
as the trifocal line is the image of the trifocal plane. In practice, very few or no any points at all really
lie on the trifocal plane. However, we may virtually project any 3D points onto the trifocal plane,
therefore comes the central idea of our method: the 2D images of a camera undergoing any planar
motion reduce to 1D images by projecting the 2D image points onto the trifocal line. This can be
achieved in at least two ways.
First, if the vanishing point v of the rotation axis is well-defined. This vanishing point of the
rotation axis being the direction perpendicular to the common plane of motion can be determined
from fundamental matrices by noticing that the image of the horopter for planar motion degenerates
to two lines [1], one of which goes through the vanishing point of the rotation axis, we may refer to
[1] for more details.
Given a 3D point M with image m, we mentally project it to -
M in the plane of motion, the
projection being parallel to the direction of rotation. The image -
m of this virtual point can be obtained
in the image as the intersection of the line v \Theta m with the trifocal line t, i.e. -
(v \Theta m): Since
the vanishing point v of the rotation axis and the trifocal line t are well defined, this construction
illustrated in Figure 2 is a well-defined geometric operation.
Note this is also a projective projection from P 2 (image plane) to P 1 (trifocal line): m 7! -
m; as
is illustrated in Figure 3.
Alternatively, if the vanishing point is not available, we can nonetheless create the virtual points
in the trifocal plane. Given two points M and M 0 with images m and m 0 , the line (M; M 0 ) intersects
the plane of motion in -
M . The image -
m of this virtual point can be obtained in the image as the
intersection of the line (m; m 0 ) with the trifocal line t, see Figure 4.
Another important consequence of this construction is that 2D image line segments can also be
converted into 1D image points! The construction is even simpler, as the resulting 1D image point is
just the intersection of the line segment with the trifocal line.
Direction of the axis
of rotation

Figure

2: Creating a 1D image from a 2D image from the vanishing point of the rotation axis and the
trifocal line (see text).
trifocal line
vanishing point of the rotation axis
2D image point
1D image point

Figure

3: Converting 2D image points into 1D image points in the image plane is equivalent to a
projective projection from the image plane to the trifocal line with the vanishing point of the rotation
axis as the projection center.

Figure

4: Creating a 1D image from any pairs of points or any line segments (see text).
1D Self-calibration
At this point, we have obtained the interesting result that a 1D projective camera model is obtained
by considering only the re-projected points on the trifocal line for a planar motion. The 1D self-calibration
method just described in Section 4 will allow us to locate the image of the circular points
common to all planes parallel to the motion plane.
Estimation of the image of the absolute conic for the 2D camera
Each planar motion generally gives us two points on the absolute conic, together with the vanishing
point of the rotation axes as the pole of the trifocal line w.r.t. the absolute conic. The pole/polar
relation between the vanishing point of the rotation axes and the trifocal line was introduced in [1].
As a whole, this provides 4 constraints on the absolute conic. Since a conic has 5 d.o.f., at least
different planar motions, yielding 8 linear constraints on the absolute conic, will be sufficient to
determine the full set of 5 internal parameters of a general 2D camera by fitting a general conic of
If we assume a 4-parameter model for camera calibration with no image skew (i.e.
planar motion yielding 4 constraints is generally sufficient to determine the 4 internal parameters of
the 2D camera. However this is not true for the very common planar motions such as purely horizontal
or vertical motions with the image plane perpendicular to the motion plane It can be easily proven that
there are only 3 instead of 4 independent constraints on the absolute conic in these configurations. We
need at least 2 different planar motions for determining the 4 internal parameters.
This also suggests that even if the planar motion is not purely horizontal or vertical, but close, the
vanishing point of the rotation axes only constrains loosely the absolute conic. Using only the circular
points located on the absolute conic is preferable and numerically stable, but we may need at least
3 planar motions to determine the 5 internal parameters of the 2D camera. Note that the numerical
instability of the vanishing point for nearly horizontal trifocal line was already reported by Armstrong
in [2]. Obviously, if we work with a 3-parameter model with known aspect ratio and without skew,
one planar motion is sufficient [1].
As we have mentioned at the beginning of this section the method described in this section is
related to the work of Armstrong et al. [1], but there are some important differences which we explain
now.
ffl First, our approach gives an elegant insight of the intricate relationship between 2D and 1D
cameras for a special kind of motion called planar motion.
ffl Second, it allows us to use only the fundamental matrices of the 2D images and the trifocal
tensor of 1D images to self-calibrate the camera instead of the trifocal tensor of 2D images.
It is now well known that fundamental matrices can be very efficiently and robustly estimated
[31, 27]. The same is true of the estimation of the 1D trifocal tensor [21] which is a linear
process. Armstrong et al., on the other hand, use the trifocal tensor of 2D images which so
far has been hard to estimate due to complicated algebraic constraints to our knowledge. Also,
the trifocal tensor of 2D images takes a special form in the planar motion case [1] and the new
constraints have to be included in the estimation process.
It may be worth mentioning that in the case of interest here, planar motion of the cameras,
the Kruppa equations become degenerate [30] and the recover the internal parameters is impossible
from the Kruppa equations. Since it is known that the trifocal tensor of 2D images is
algebraically equivalent to the three fundamental matrices plus the restriction of the trifocal tensor
to the trifocal plane [14, 15, 9], our method can be seen as an inexpensive way of estimating
the full trifocal tensor of 2D images: first estimate the three fundamental matrices (nonlinear
but simple and well understood), then estimate the trifocal tensor in the trifocal plane (linear).
Although it looks superficially that both the 1D and 2D trifocal tensors can be estimated linearly
with at least 7 image correspondences, this is misleading since the estimation of the 1D trifocal
tensor is exactly linear for 7 d.o.f. whereas the linear estimation of the 2D trifocal tensor is only
a rough approximation based on a set of 26 auxiliary parameters for its d.o.f. and obtained
by neglecting 8 complicated algebraic constraints.
ffl Third, but this is a minor point, our method may not require the estimation of the vanishing
point of the rotation axes.
7 Experimental results
The theoretical results for 1D camera self-calibration and its applications to 2D camera calibration
have been implemented and experimented on synthetic and real images. Due to space limitation,
we are not to present the results on synthetic data, the algorithms generally perform very well. We
only show some real examples. Here we consider a scenario of a real camera mounted on a robot's
arm. Two sequences of images are acquired by the camera moving in two different planes. The first
sequence contains 7 (indexed from 16 to 22) images (cf. Figure 5) and the second contains 8 (indexed
from 8 to 15).
The calibration grid was used to have the ground truth for the internal camera parameters which
have been measured as ff using the standard
calibration method [6].

Figure

5: Three images of the first planar motion.
We take triplets of images from the first sequence and for each triplet we estimate the trifocal
line and the vanishing point of the rotation axes by the 3 fundamental matrices of the triplet. The
1D self-calibration is applied for estimating the images of the circular points along the trifocal lines.
To evaluate the accuracy of the estimation, the images of the circular points of the trifocal plane are
re-computed in the image plane from the known internal parameters by intersecting the image of the
absolute conic with the trifocal line. Table 1 shows the results for different triplets of images of the
first sequence.
Image triplet Fixed point Circular points by self-calibration Circular points by calibration

Table

1: Table of the estimated positions of the images of the circular points by self-calibration with
different triplets of images of the first sequence. The quantities are expressed in the first image pixel
coordinate system. The location of circular points by calibration vary as the trifocal line location
varies.
Since we have more than 3 images for the same planar motion of the camera, we could also
estimate the trifocal line and the vanishing point of the rotation axes by using all available fundamental
matrices of the 7 images of the sequence. The results using redundant images are presented for
different triplets in Table 2. We note the slight improvement of the results compared with those
presented in Table 1.
Image triplet Circular points Fixed point
known position by calibration 262:1 \Sigma i2590:6

Table

2: Table of the estimated positions of the image of circular points with different triplets of
images. These quantities vary because the 1D trifocal tensor varies. The trifocal line and the vanishing
point of the rotation axes are estimated using 7 images of the sequence instead of the minimum of 3
images.
The same experiment was carried out for the other sequence of images where the camera underwent
a different planar motion. Similar results to the first image sequence are obtained, we give only
the result for one triplet of images in Table 3 for this sequence.
Image triplet Fixed point Circular points by self-calibration Circular points by calibration

Table

3: Table of the estimated position of the image of circular points with one triplet of second
image sequence.
Now two sequences of images each corresponding to a different planar motion yield four distinct
imaginary points on the image plane which must be on the image ! of the absolute conic. Assuming
that there is no camera skew, we could fit to those four points an imaginary ellipse using standard
techniques and compute the resulting internal parameters. Note that we did not use the pole/polar
constraint of the vanishing point of the ratation axes on the absolute conic as it was discussed in
Section 6 that this constraint is not numerically reliable.
To have an intuitive idea of the planar motions, the two trifocal lines together with one image are
shown in Figure 6.
-1000 -500 500 1000 150050015002500

Figure

The image of the motion planes of the two planar motions.
The ultimate goal of self-calibration is to get 3D metric reconstruction. 3D reconstruction from
two images of the sequence is performed by using the estimated internal parameters as illustrated in

Figure

7. To evaluate the reconstruction quality, we did the same reconstruction using the known
internal parameters. Two such reconstructions differ merely by a 3D similarity transformation which
could be easily estimated. The resulting relative error for normalised 3D coordinates by similarity
between the reconstruction from self-calibration and off-line calibration is 3:4 percent.
-0.4
-0.3
-0.2
-0.10.10.3
-0.23.43.84.4

Figure

7: Two views of the resulting 3D reconstruction by self-calibration.
8 Conclusions and other applications
We have first established that the 2 internal parameters of 1D camera can be uniquely determined
through the trifocal tensor of three 1D images. Since the trifocal tensor can be estimated linearly
from at least 7 points in three 1D images, the method of the 1D self-calibration is a real linear method
(modulo the fact that we have to find the roots of a third degree polynomial in 1 variable), no over-
parameterisation was introduced.
Secondly, we have proven that if a 2D camera undergoes a planar motion, the 2D camera reduces
to a 1D camera in the plane of motion. The reduction of a 2D image to a 1D image can be efficiently
performed by using only the fundamental matrices of 2D images. Based on this relation between 2D
and 1D images, the self-calibration of 1D camera can be applied for self-calibrating a 2D camera.
Our experimental results based on real image sequences show the very large stability of the solutions
yielded by the 1D self-calibration method and the accurate 3D metric reconstruction that can be
obtained from the internal parameters of the 2D camera estimated by the 1D self-calibration method.
The camera motions that may defeat the self-calibration method developed in Section 4 are described
in [26].



--R



Invariancy Methods for Points
Affine calibration of mobile vehicles.
The twisted cubic and camera calibration.
Camera Calibration for 3D Computer Vision
Stratification of three-dimensional vision: Projective
Motion from point matches: Multiplicity of solutions.
About the correspondences of points between n images.

In defence of the 8-point algorithm
Euclidean reconstruction from uncalibrated views.
A linear method for reconstruction from lines and points.
Geometry and Algebra of Multiple Projective Transformations.
Algebraic Properties of Multilinear Constraints.
A Common Framework for Multiple-View Tensors

A theory of self calibration of a moving camera.

Affine structure from line correspondences with uncalibrated affine cameras.
Algebraic Projective Geometry.
Algebraic functions for recognition.
A unified theory of structure from motion.
Critical Motion Sequences for Monocular Self-Calibration and Uncalibrated Euclidean Recon- struction
Vision 3D non calibr-ee : contributions - a la reconstruction projective et - etude des mouvements critiques pour l'auto-calibrage
Performance characterization of fundamental matrix estimation under image degradation.
Matching constraints and the joint image.
Autocalibration and the Absolute Quadric.
Camera self-calibration from video sequences: the Kruppa equations revisited
A robust technique for matching two uncalibrated images through the recovery of the unknown epipolar geometry.
--TR

--CTR
Mandun Zhang , Jian Yao , Bin Ding , Yangsheng Wang, Fast individual face modeling and animation, Proceedings of the second Australasian conference on Interactive entertainment, p.235-239, November 23-25, 2005, Sydney, Australia
A. C. Murillo , C. Sags , J. J. Guerrero , T. Goedem , T. Tuytelaars , L. Van Gool, From omnidirectional images to hierarchical localization, Robotics and Autonomous Systems, v.55 n.5, p.372-382, May, 2007
