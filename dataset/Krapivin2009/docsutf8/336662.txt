--T
Content-based book recommending using learning for text categorization.
--A
Recommender systems improve access to relevant products and information by making personalized suggestions based on previous examples of a user's likes and dislikes.  Most existing recommender systems use collaborative filtering methods that base recommendations on other users' preferences. By contrast,content-based methods use information about an item itself to make suggestions.This approach has the advantage of being able to recommend previously unrated items to users with unique interests and to provide explanations for its recommendations. We describe a content-based book recommending system that utilizes information extraction and a machine-learning algorithm for text categorization.  Initial experimental results demonstrate that this approach can produce accurate recommendations.
--B
INTRODUCTION
There is a growing interest in recommender systems that suggest
music, films, books, and other products and services to
users based on examples of their likes and dislikes [19, 26,
11]. A number of successful startup companies like Fire-
fly, Net Perceptions, and LikeMinds have formed to provide
recommending technology. On-line book stores like Amazon
and BarnesAndNoble have popular recommendation ser-
vices, and many libraries have a long history of providing
reader's advisory services [2, 21]. Such services are important
since readers' preferences are often complex and not
readily reduced to keywords or standard subject categories,
but rather best illustrated by example. Digital libraries should
be able to build on this tradition of assisting readers by providing
cost-effective, informed, and personalized automated
recommendations for their patrons.
Existing recommender systems almost exclusively utilize a
form of computerized matchmaking called collaborative or
social filtering. The system maintains a database of the preferences
of individual users, finds other users whose known
preferences correlate significantly with a given patron, and
recommends to a person other items enjoyed by their matched
patrons. This approach assumes that a given user's tastes are
generally the same as another user of the system and that a
sufficient number of user ratings are available. Items that
have not been rated by a sufficient number of users cannot
be effectively recommended. Unfortunately, statistics on library
use indicate that most books are utilized by very few
patrons [12]. Therefore, collaborative approaches naturally
tend to recommend popular titles, perpetuating homogeneity
in reading choices. Also, since significant information
about other users is required to make recommendations, this
approach raises concerns about privacy and access to proprietary
customer data.
Learning individualized profiles from descriptions of examples
(content-based recommending [3]), on the other hand,
allows a system to uniquely characterize each patron without
having to match their interests to someone else's. Items
are recommended based on information about the item itself
rather than on the preferences of other users. This also allows
for the possibility of providing explanations that list content
features that caused an item to be recommended; potentially
giving readers confidence in the system's recommendations
and insight into their own preferences. Finally, a content-based
approach can allow users to provide initial subject information
to aid the system.
Machine learning for text-categorization has been applied to
content-based recommending of web pages [25] and newsgroup
messages [15]; however, to our knowledge has not
previously been applied to book recommending. We have
been exploring content-based book recommending by applying
automated text-categorization methods to semi-structured
text extracted from the web. Our current prototype system,
LIBRA (Learning Intelligent Book Recommending Agent),
uses a database of book information extracted from web pages
at Amazon.com. Users provide 1-10 ratings for a selected set
of training books; the system then learns a profile of the user
using a Bayesian learning algorithm and produces a ranked
list of the most recommended additional titles from the sys-
tem's catalog.
As evidence for the promise of this approach, we present initial
experimental results on several data sets of books randomly
selected from particular genres such as mystery, sci-
ence, literary fiction, and science fiction and rated by different
users. We use standard experimental methodology from
machine learning and present results for several evaluation
metrics on independent test data including rank correlation
coefficient and average rating of top-ranked books.
The remainder of the paper is organized as follows. Section
2 provides an overview of the system including the algorithm
used to learn user profiles. Section 3 presents results of our
initial experimental evaluation of the system. Section 4 discusses
topics for further research, and section 5 presents our
conclusions on the advantages and promise of content-based
book recommending.
SYSTEM DESCRIPTION
Extracting Information and Building a Database
First, an Amazon subject search is performed to obtain a
list of book-description URL's of broadly relevant titles. LIBRA
then downloads each of these pages and uses a simple
pattern-based information-extraction system to extract data
about each title. Information extraction (IE) is the task of locating
specific pieces of information from a document, thereby
obtaining useful structured data from unstructured text [16,
9]. Specifically, it involves finding a set of substrings from
the document, called fillers, for each of a set of specified
slots. When applied to web pages instead of natural language
text, such an extractor is sometimes called a wrapper [14].
The current slots utilized by the recommender are: title, au-
thors, synopses, published reviews, customer comments, related
authors, related titles, and subject terms. Amazon produces
the information about related authors and titles using
collaborative methods; however, LIBRA simply treats them
as additional content about the book. Only books that have at
least one synopsis, review or customer comment are retained
as having adequate content information. A number of other
slots are also extracted (e.g. publisher, date, ISBN, price,
etc.) but are currently not used by the recommender. We
have initially assembled databases for literary fiction (3,061
titles), science fiction (3,813 titles), mystery (7,285 titles),
and science (6,177 titles).
Since the layout of Amazon's automatically generated pages
is quite regular, a fairly simple extraction system is suffi-
cient. LIBRA's extractor employs a simple pattern matcher
that uses pre-filler, filler, and post-filler patterns for each slot,
as described by [6]. In other applications, more sophisticated
information extraction methods and inductive learning of extraction
rules might be useful [7].
The text in each slot is then processed into an unordered bag
of words (tokens) and the examples represented as a vector
of bags of words (one bag for each slot). A book's title and
authors are also added to its own related-title and related-
author slots, since a book is obviously "related" to itself, and
this allows overlap in these slots with books listed as related
to it. Some minor additions include the removal of a small list
of stop-words, the preprocessing of author names into unique
tokens of the form first-initial last-name and the grouping of
the words associated with synopses, published reviews, and
customer comments all into one bag (called "words").
Learning a Profile
Next, the user selects and rates a set of training books. By
searching for particular authors or titles, the user can avoid
scanning the entire database or picking selections at random.
The user is asked to provide a discrete 1-10 rating for each
selected title.
The inductive learner currently employed by LIBRA is a bag-
of-words naive Bayesian text classifier [22] extended to handle
a vector of bags rather than a single bag. Recent experimental
results [10, 20] indicate that this relatively simple approach
to text categorization performs as well or better than
many competing methods. LIBRA does not attempt to predict
the exact numerical rating of a title, but rather just a total
ordering (ranking) of titles in order of preference. This task
is then recast as a probabilistic binary categorization problem
of predicting the probability that a book would be rated
as positive rather than negative, where a user rating of 1-5
is interpreted as negative and 6-10 as positive. As described
below, the exact numerical ratings of the training examples
are used to weight the training examples when estimating the
parameters of the model.
Specifically, we employ a multinomial text model [20], in
which a document is modeled as an ordered sequence of
word events drawn from the same vocabulary, V . The "naive
Bayes" assumption states that the probability of each word
event is dependent on the document class but independent of
the word's context and position. For each class, c j , and word
or token, w
must be estimated from the training data. Then the posterior
probability of each class given a document, D, is computed
using Bayes rule:
Y
where a i is the ith word in the document, and jDj is the
length of the document in words. Since for any given docu-
ment, the prior P (D) is a constant, this factor can be ignored
if all that is desired is a ranking rather than a probability es-
timate. A ranking is produced by sorting documents by their
odds ratio, P represents the positive
class and c 0 represents the negative class. An example
is classified as positive if the odds are greater than 1, and
negative otherwise.
In our case, since books are represented as a vector of "doc-
uments," dm , one for each slot (where s m denotes the mth
slot), the probability of each word given the category and the
slot, must be estimated and the posterior category
probabilities for a book, B, computed using:
Y
Y
where S is the number of slots and ami is the ith word in the
mth slot.
Parameters are estimated from the training examples as fol-
lows. Each of the N training books, B e (1 - e - N ) is given
two real weights, 0 - ff ej - 1, based on scaling it's user rat-
positive weight, ff
a negative weight ff . If a word appears n times
in an example B e , it is counted as occurring ff e1 n times in a
positive example and ff e0 n times in a negative example. The
model parameters are therefore estimated as follows:
ff ej =N (3)
where n kem is the count of the number of times word w k
appears in example B e in slot s m , and
denotes the total weighted length of the documents in category
c j and slot s m .
These parameters are "smoothed" using Laplace estimates to
avoid zero probability estimates for words that do not appear
in the limited training sample by redistributing some of
the probability mass to these items using the method recommended
in [13]. Finally, calculation with logarithms of probabilities
is used to avoid underflow.
The computational complexity of the resulting training (test-
ing) algorithm is linear in the size of the training (testing)
data. Empirically, the system is quite efficient. In the experiments
on the LIT1 data described below, the current Lisp
implementation running on a Sun Ultra 1 trained on 20 examples
in an average of 0.4 seconds and on 840 examples in
Slot Word Strength
WORDS ZUBRIN 9.85
WORDS SMOLIN 9.39
WORDS TREFIL 8.77
WORDS DOT 8.67
WORDS ALH 7.97
WORDS MANNED 7.97
RELATED-TITLES SETTLE 7.91
RELATED-TITLES CASE 7.91
RELATED-AUTHORS A RADFORD 7.63
WORDS LEE 7.57
WORDS MORAVEC 7.57
WORDS WAGNER 7.57
RELATED-TITLES CONNECTIONIST 7.51
RELATED-TITLES BELOW 7.51

Table

1: Sample Positive Profile Features
an average of 11.5 seconds, and probabilistically categorized
new test examples at an average rate of about 200 books per
second. An optimized implementation could no doubt significantly
improve performance even further.
A profile can be partially illustrated by listing the features
most indicative of a positive or negative rating. Table 1 presents
the top 20 features for a sample profile learned for recommending
science books. Strength measures how much more
likely a word in a slot is to appear in a positively rated book
than a negatively rated one, computed as:
Producing, Explaining, and Revising Recommendations
Once a profile is learned, it is used to predict the preferred
ranking of the remaining books based on posterior probability
of a positive categorization, and the top-scoring recommendations
are presented to the user.
The system also has a limited ability to "explain" its recommendations
by listing the features that most contributed
to its high rank. For example, given the profile illustrated
above, LIBRA presented the explanation shown in Table 2.
The strength of a cue in this case is multiplied by the number
of times it appears in the description in order to fully
indicate its influence on the ranking. The positiveness of a
feature can in turn be explained by listing the user's training
examples that most influenced its strength, as illustrated in

Table

3 where "Count" gives the number of times the feature
appeared in the description of the rated book.
After reviewing the recommendations (and perhaps disrec-
ommendations), the user may assign their own rating to examples
they believe to be incorrectly ranked and retrain the
The Fabric of Reality:
The Science of Parallel Universes- And Its Implications
by David Deutsch recommended because:
Slot Word Strength
WORDS MULTIVERSE 75.12
WORDS UNIVERSES 25.08
WORDS REALITY 22.96
WORDS UNIVERSE 15.55
WORDS QUANTUM 14.54
WORDS INTELLECT 13.86
WORDS OKAY 13.75
WORDS RESERVATIONS 11.56
WORDS DENIES 11.56
WORDS EVOLUTION 11.02
WORDS WORLDS 10.10
WORDS SMOLIN 9.39
WORDS ONE 8.50
WORDS IDEAS 8.35
WORDS THEORY 8.28
WORDS IDEA 6.96
WORDS IMPLY 6.47
WORDS GENIUSES 6.47

Table

2: Sample Recommendation Explanation
The word UNIVERSES is positive due to your ratings:
Title Rating Count
The Life of the
Before the Beginning : Our Universe and Others 8 7
Unveiling the Edge of Time
Black Holes : A Traveler's Guide 9 3
The Inflationary Universe 9 2

Table

3: Sample Feature Explanation
system to produce improved recommendations. As with relevance
feedback in information retrieval [27], this cycle can
be repeated several times in order to produce the best results.
Also, as new examples are provided, the system can track any
change in a user's preferences and alter its recommendations
based on the additional information.
Methodology
Data Collection Several data sets were assembled to evaluate
LIBRA. The first two were based on the first 3,061
adequate-information titles (books with at least one abstract,
review, or customer comment) returned for the subject search
"literature fiction." Two separate sets were randomly selected
from this dataset, one with 936 books and one with 935, and
rated by two different users. These sets will be called
and LIT2, respectively. The remaining sets were based on
all of the adequate-information Amazon titles for "mystery"
(7,285 titles), "science" (6,177 titles), and "science fiction"
(3,813 titles). From each of these sets, 500 titles were chosen
at random and rated by a user (the same user rated both the
science and science fiction books). These sets will be called
Data Number Exs Avg. Rating % Positive (r ? 5)
MYST 500 7.00 74.4
SCI 500 4.15 31.2

Table

4: Data Information
Rating

Table

5: Data Rating Distributions
MYST, SCI, and SF, respectively.
In order to present a quantitative picture of performance on
a realistic sample; books to be rated where selected at ran-
dom. However, this means that many books may not have
been familiar to the user, in which case, the user was asked
to supply a rating based on reviewing the Amazon page describing
the book. Table 4 presents some statistics about the
data and Table 5 presents the number of books in each rating
category. Note that overall the data sets have quite different
ratings distributions.
Performance Evaluation To test the system, we performed
10-fold cross-validation, in which each data set is randomly
segments and results are averaged
trials, each time leaving a separate segment out for
independent testing, and training the system on the remaining
data [22]. In order to observe performance given varying
amounts of training data, learning curves were generated by
testing the system after training on increasing subsets of the
overall training data. A number of metrics were used to measure
performance on the novel test data, including:
ffl Classification accuracy (Acc): The percentage of examples
correctly classified as positive or negative.
ffl Recall (Rec): The percentage of positive examples classified
as positive.
ffl Precision (Pr): The percentage of examples classified as
positive which are positive.
ffl Precision at Top 3 (Pr3): The percentage of the 3 top ranked
examples which are positive.
ffl Precision at Top 10 (Pr10): The percentage of the 10 top
ranked examples which are positive.
ffl F-Measure weighted average of precision and recall
frequently used in information retrieval:
Data N Acc Rec Pr Pr3 Pr10 F Rt3 Rt10 r s
MYST 100 86.6 95.2 87.2 93.3 94.0 90.9 8.70 8.69 0.55
MYST 450 85.8 93.2 88.1 96.7 98.0 90.5 8.90 8.97 0.61
SCI 100 81.8 74.4 72.2 93.3 83.0 72.3 8.50 7.29 0.65
SCI 450 85.2 79.1 76.8 93.3 89.0 77.2 8.57 7.71 0.71
SF 100 76.4 65.7 46.2 80.0 56.0 52.4 7.00 5.75 0.40

Table

Summary of Results
ffl Rating of Top 3 (Rt3): The average user rating assigned to
the 3 top ranked examples.
ffl Rating of Top 10 (Rt10): The average user rating assigned
to the 10 top ranked examples.
ffl Rank Correlation (r s correlation coefficient
between the system's ranking and that imposed by the
users ratings ties are handled using the
method recommended by [1].
The top 3 and top 10 metrics are given since many users will
be primarily interested in getting a few top-ranked recom-
mendations. Rank correlation gives a good overall picture of
how the system's continuous ranking of books agrees with
the user's, without requiring that the system actually predict
the numerical rating score assigned by the user. A correlation
coefficient of 0.3 to 0.6 is generally considered "moderate"
and above 0.6 is considered "strong."
Basic Results
The results are summarized in Table 6, where N represents
the number of training examples utilized and results are shown
for a number of representative points along the learning curve.
Overall, the results are quite encouraging even when the system
is given relatively small training sets. The SF data set is
clearly the most difficult since there are very few highly-rated
books.
The "top n" metrics are perhaps the most relevant to many
users. Consider precision at top 3, which is fairly consistently
in the 90% range after only 20 training examples (the
exceptions are LIT1 until 70 examples 1 and SF until 450
examples). Therefore, LIBRA's top recommendations are
highly likely to be viewed positively by the user. Note that
the "% Positive" column in Table 4 gives the probability that
a randomly chosen example from a given data set will be
positively rated. Therefore, for every data set, the top 3 and
recommendations are always substantially more likely
than random to be rated positively, even after only 5 training
examples.
1 References to performance at 70 and 300 examples are based on learning
curve data not included in the summary in Table 6.
Correlation
Coefficient
Training Examples
LIBRA
LIBRA-NR

Figure

1:
Considering the average rating of the top 3 recommenda-
tions, it is fairly consistently above an 8 after only 20 training
examples (the exceptions again are LIT1 until 100 examples
and SF). For every data set, the top 3 and top 10 recommendations
are always rated substantially higher than a randomly
selected example (cf. the average rating from Table 4).
Looking at the rank correlation, except for SF, there is at
least a moderate correlation (r s - 0:3) after only 10 exam-
ples, and SF exhibits a moderate correlation after 40 exam-
ples. This becomes a strong correlation (r s - 0:6) for LIT1
after only 20 examples, for LIT2 after 40 examples, for SCI
after 70 examples, for MYST after 300 examples, and for SF
after 450 examples.
Results on the Role of Collaborative Content
Since collaborative and content-based approaches to recommending
have somewhat complementary strengths and weak-
nesses, an interesting question that has already attracted some
initial attention [3, 4] is whether they can be combined to
produce even better results. Since LIBRA exploits content
about related authors and titles that Amazon produces using
collaborative methods, an interesting question is whether this
collaborative content actually helps its performance. To examine
this issue, we conducted an "ablation" study in which
the slots for related authors and related titles were removed
from LIBRA's representation of book content. The resulting
system, called LIBRA-NR, was compared to the original one
using the same 10-fold training and test sets. The statistical
significance of any differences in performance between
the two systems was evaluated using a 1-tailed paired t-test
requiring a significance level of p ! 0:05.
Overall, the results indicate that the use of collaborative content
has a significant positive effect. Figures 1, 2, and
3, show sample learning curves for different important metrics
for a few data sets. For the LIT1 rank-correlation results
shown in Figure 1, there is a consistent, statistically-
significant difference in performance from 20 examples on-20406080100
Precision
TopTraining Examples
LIBRA
LIBRA-NR

Figure

2: MYST Precision at Top 1013570 50 100 150 200 250 300 350 400 450
Rating
TopTraining Examples
LIBRA
LIBRA-NR

Figure

3: SF Average Rating of Top 3
ward. For the MYST results on precision at top 10 shown in

Figure

2, there is a consistent, statistically-significant difference
in performance from 40 examples onward. For the SF
results on average rating of the top 3, there is a statistically-
significant difference at 10, 100, 150, 200, and 450 examples.
The results shown are some of the most consistent differences
for each of these metrics; however, all of the datasets
demonstrate some significant advantage of using collaborative
content according to one or more metrics. Therefore, information
obtained from collaborative methods can be used
to improve content-based recommending, even when the actual
user data underlying the collaborative method is unavailable
due to privacy or proprietary concerns.
We are currently developing a web-based interface so that
LIBRA can be experimentally evaluated in practical use with
a larger body of users. We plan to conduct a study in which
each user selects their own training examples, obtains recom-
mendations, and provides final informed ratings after reading
one or more selected books.
Another planned experiment is comparing LIBRA's content-based
approach to a standard collaborative method. Given
the constrained interfaces provided by existing on-line rec-
ommenders, and the inaccessibility of the underlying proprietary
user data, conducting a controlled experiment using the
exact same training examples and book databases is difficult.
However, users could be allowed to use both systems and
evaluate and compare their final recommendations. 2
Since many users are reluctant to rate large number of training
examples, various machine-learning techniques for maximizing
the utility of small training sets should be utilized.
One approach is to use unsupervised learning over unrated
book descriptions to improve supervised learning from a smaller
number of rated examples. A successful method for doing
this in text categorization is presented in [23]. Another approach
is active learning, in which examples are acquired
incrementally and the system attempts to use what it has already
learned to limit training by selecting only the most
informative new examples for the user to rate [8]. Specific
techniques for applying this to text categorization have been
developed and shown to significantly reduce the quantity of
labeled examples required [17, 18].
A slightly different approach is to advise users on easy and
productive strategies for selecting good training examples
themselves. We have found that one effective approach is to
first provide a small number of highly rated examples (which
are presumably easy for users to generate), running the system
to generate initial recommendations, reviewing the top
recommendations for obviously bad items, providing low ratings
for these examples, and retraining the system to obtain
new recommendations. We intend to conduct experiments on
the existing data sets evaluating such strategies for selecting
training examples.
Studying additional ways of combining content-based and
collaborative recommending is particularly important. The
use of collaborative content in LIBRA was found to be use-
ful, and if significant data bases of both user ratings and item
content are available, both of these sources of information
could contribute to better recommendations [3, 4]. One additional
approach is to automatically add the related books
of each rated book as additional training examples with the
same (or similar) rating, thereby using collaborative information
to expand the training examples available for content-based
recommending.
A list of additional topics for investigation include the following

ffl Allowing a user to initially provide keywords that are of
known interest (or disinterest), and incorporating this information
into learned profiles by biasing the parameter esti-
2 Amazon has already made significantly more income from the first author
based on recommendations provided by LIBRA than those provided by
its own recommender system; however, this is hardly a rigorous, unbiased
comparison.
mates for these words [24].
ffl Comparing different text-categorization algorithms: In addition
to more sophisticated Bayesian methods, neural-network
and case-based methods could be explored.
Combining content extracted from multiple sources: For
example, combining information about a title from Amazon,
BarnesAndNoble, on-line library catalogs, etc.
ffl Using full-text as content: A digital library should be able
to efficiently utilize the complete on-line text, as well as abstracted
summaries and reviews, to recommend items.
CONCLUSIONS
The ability to recommend books and other information sources
to users based on their general interests rather than specific
enquiries will be an important service of digital libraries.
Unlike collaborative filtering, content-based recommending
holds the promise of being able to effectively recommend unrated
items and to provide quality recommendations to users
with unique, individual tastes. LIBRA is an initial content-based
book recommender which uses a simple Bayesian learning
algorithm and information about books extracted from
the web to recommend titles based on training examples supplied
by an individual user. Initial experiments indicate that
this approach can efficiently provide accurate recommendations
in the absence of any information about other users.
In many ways, collaborative and content-based approaches
provide complementary capabilities. Collaborative methods
are best at recommending reasonably well-known items to
users in a communities of similar tastes when sufficient user
data is available but effective content information is not. Content-based
methods are best at recommending unpopular items to
users with unique tastes when sufficient other user data is
unavailable but effective content information is easy to ob-
tain. Consequently, as discussed above, methods for integrating
these approaches will perhaps provide the best of both
worlds.
Finally, we believe that methods and ideas developed in machine
learning research [22] are particularly useful for content-based
recommending, filtering, and categorization, as well as
for integrating with collaborative approaches [5, 4]. Given
the future potential importance of such services to digital li-
braries, we look forward to an increasing application of machine
learning techniques to these challenging problems.

ACKNOWLEDGEMENTS

Thanks to Paul Bennett for contributing ideas, software, and
data, and to Tina Bennett for contributing data. This research
was partially supported by the National Science Foundation
through grant IRI-9704943.



--R

The New Statistical Analysis of Data.
Laying a firm foundation: Administrative support for readers' advisory services.

Recommendation as classification: Using social and content-based information in recommendation
Learning collaborative information filters.
Relational learning of pattern-match rules for information extraction
Empirical methods in information extrac- tion
Improving generalization with active learning.

A probabilistic analysis of the Rocchio algorithm with TFIDF for text categorization.
Papers from the AAAI

Improving simple Bayes.
Wrapper induction for information extraction.
Learning to filter netnews.
A performance evaluation of text-analysis technologies
Heterogeneous uncertainty sampling for supervised learning.
Active learning with committees for text categorization.
Agents that reduce work and information overload.
A comparison of event models for naive Bayes text classification.
Developing Readers' Advisory Services: Concepts and Committ- ments
Machine Learning.
Learning to classify text from labeled and unlabeled documents.
The identification of interesting web sites.


Improving retrieval performance by relevance feedback.
--TR
User models: theory, method, and practice
Learning internal representations by error propagation
An evaluation of text analysis technologies
Using collaborative filtering to weave an information tapestry
The Utility of Knowledge in Inductive Learning
C4.5: programs for machine learning
Agents that reduce work and information overload
Theory refinement combining analytical and empirical methods
Improving Generalization with Active Learning
GroupLens
Knowledge-based artificial neural networks
Recommender systems
Fab
Feature selection, perception learning, and a usability case study for text categorization
Learning and Revising User Profiles
Comparing feature-based and clique-based user models for movie selection
Recommendation as classification
Learning to classify text from labeled and unlabeled documents
A re-examination of text categorization methods
Relational learning of pattern-match rules for information extraction
Combining collaborative filtering with personal agents for better recommendations
An Evaluation of Statistical Approaches to Text Categorization
Machine Learning
A Comparative Study on Feature Selection in Text Categorization
A Probabilistic Analysis of the Rocchio Algorithm with TFIDF for Text Categorization
Learning Collaborative Information Filters

--CTR
Gary Geisler , David McArthur , Sarah Giersch, Developing recommendation services for a digital library with uncertain and changing data, Proceedings of the 1st ACM/IEEE-CS joint conference on Digital libraries, p.199-200, January 2001, Roanoke, Virginia, United States
Kai Yu , Anton Schwaighofer , Volker Tresp , Xiaowei Xu , Hans-Peter Kriegel, Probabilistic Memory-Based Collaborative Filtering, IEEE Transactions on Knowledge and Data Engineering, v.16 n.1, p.56-69, January 2004
Fiona Y. Chan , William K. Cheung, Customizing digital storefronts using the knowledge-based approach, Information management: support systems & multimedia technology, Idea Group Publishing, Hershey, PA,
Tomoharu Iwata , Kazumi Saito , Takeshi Yamada, Recommendation method for extending subscription periods, Proceedings of the 12th ACM SIGKDD international conference on Knowledge discovery and data mining, August 20-23, 2006, Philadelphia, PA, USA
Junichi Iijima , Sho Ho, Common structure and properties of filtering systems, Electronic Commerce Research and Applications, v.6 n.2, p.139-145, Summer 2007
Ana Gil , Francisco Garca, E-commerce recommenders: powerful tools for E-business, Crossroads, v.10 n.2, p.6-6, 31 August 2004
Kai Yu , Volker Tresp , Shipeng Yu, A nonparametric hierarchical bayesian framework for information filtering, Proceedings of the 27th annual international ACM SIGIR conference on Research and development in information retrieval, July 25-29, 2004, Sheffield, United Kingdom
Hahn-Ming Lee , Chi-Chun Huang , Tzu-Ting Kao, Personalized Course Navigation Based on Grey Relational Analysis, Applied Intelligence, v.22 n.2, p.83-92, March     2005
Wei-Po Lee , Cheng-Che Lu, Customising WAP-based information services on mobile networks, Personal and Ubiquitous Computing, v.7 n.6, p.321-330, December
Justin Basilico , Thomas Hofmann, Unifying collaborative and content-based filtering, Proceedings of the twenty-first international conference on Machine learning, p.9, July 04-08, 2004, Banff, Alberta, Canada
Prem Melville , Raymod J. Mooney , Ramadass Nagarajan, Content-boosted collaborative filtering for improved recommendations, Eighteenth national conference on Artificial intelligence, p.187-192, July 28-August 01, 2002, Edmonton, Alberta, Canada
Hung-Chen Chen , Arbee L. P. Chen, A music recommendation system based on music and user grouping, Journal of Intelligent Information Systems, v.24 n.2, p.113-132, May 2005
Andrew I. Schein , Alexandrin Popescul , Lyle H. Ungar , David M. Pennock, Methods and metrics for cold-start recommendations, Proceedings of the 25th annual international ACM SIGIR conference on Research and development in information retrieval, August 11-15, 2002, Tampere, Finland
Zan Huang , Wingyan Chung , Thian-Huat Ong , Hsinchun Chen, A graph-based recommender system for digital library, Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries, July 14-18, 2002, Portland, Oregon, USA
Daniel M. Fleder , Kartik Hosanagar, Recommender systems and their impact on sales diversity, Proceedings of the 8th ACM conference on Electronic commerce, June 11-15, 2007, San Diego, California, USA
Andrew I. Schein , Alexandrin Popescul , Lyle H. Ungar , David M. Pennock, CROC: A New Evaluation Criterion for Recommender Systems, Electronic Commerce Research, v.5 n.1, p.51-74, January 2005
Yiyang Zhang , Jianxin (Roger) Jiao, An associative classification-based recommendation system for personalization in B2C e-commerce applications, Expert Systems with Applications: An International Journal, v.33 n.2, p.357-367, August, 2007
Zan Huang , Wingyan Chung , Hsinchun Chen, A graph model for E-commerce recommender systems, Journal of the American Society for Information Science and Technology, v.55 n.3, p.259-274, February 2004
George Lekakos , George M. Giaglis, Improving the prediction accuracy of recommendation algorithms: Approaches anchored on human factors, Interacting with Computers, v.18 n.3, p.410-431, May, 2006
Marco Degemmis , Pasquale Lops , Giovanni Semeraro, A content-collaborative recommender that exploits WordNet-based user profiles for neighborhood formation, User Modeling and User-Adapted Interaction, v.17 n.3, p.217-255, July      2007
Przemysaw Kazienko , Micha Adamski, AdROSA-Adaptive personalization of web advertising, Information Sciences: an International Journal, v.177 n.11, p.2269-2295, June, 2007
Raymond J. Mooney , Razvan Bunescu, Mining knowledge from text using information extraction, ACM SIGKDD Explorations Newsletter, v.7 n.1, p.3-10, June 2005
Michael Bieber , Douglas Engelbart , Richard Furuta , Starr Roxanne Hiltz , John Noll , Jennifer Preece , Edward A. Stohr , Murray Turoff , Bartel Van De Walle, Toward Virtual Community Knowledge Evolution, Journal of Management Information Systems, v.18 n.4, p.11-35, Number 4/Spring 2002
Saverio Perugini , Marcos Andr Gonalves , Edward A. Fox, Recommender Systems Research: A Connection-Centric Survey, Journal of Intelligent Information Systems, v.23 n.2, p.107-143, September 2004
Loren Terveen , David W. McDonald, Social matching: A framework and research agenda, ACM Transactions on Computer-Human Interaction (TOCHI), v.12 n.3, p.401-434, September 2005
Uri Hanani , Bracha Shapira , Peretz Shoval, Information Filtering: Overview of Issues, Research and Systems, User Modeling and User-Adapted Interaction, v.11 n.3, p.203-259, August 2001
Lee Keener, Bibliography, Technology supporting business solutions: Advances in computation: Theory and practice, Nova Science Publishers, Inc., Commack, NY,
