--T
Stabilized Sequential Quadratic Programming.
--A
Recently, Wright proposed a stabilized sequential
quadratic programming algorithm for inequality constrained optimization.
Assuming the Mangasarian-Fromovitz constraint qualification and
the existence of a strictly positive multiplier
(but possibly dependent constraint gradients), he proved a local
quadratic convergence result. In this paper, we establish quadratic
convergence in cases where
both strict complementarity and the
Mangasarian-Fromovitz constraint qualification do not hold.
The constraints on the stabilization parameter are relaxed, and linear
convergence is demonstrated when the parameter is kept fixed.
We show that the analysis of this method can be carried out using
recent results for the stability of variational problems.
--B
Introduction
.
Let us consider the following inequality constrained optimization problem:
minimize f(z) subject to c(z) - 0; z 2 R n ; (1)
where f is real-valued and c : R n the Lagrangian L is defined
by
denote the current approximation to a local minimizer z   and an associated
multiplier -   for (1). In the sequential quadratic programming (SQP) algorithm,
the new approximation z k+1 to z   is given by z \Deltaz where \Deltaz is a local
minimizer of the following quadratic problem:
z
subject to c(z k
There are various ways to specify the new multiplier. Often - k+1 is a multiplier
associated with the constraint in the quadratic problem (2).
The typical convergence theorem for (2) (for example, see Robinson's paper [16])
states that for (z a neighborhood of a solution/multiplier pair (z   ; -   ) associated
with (1), the iteration is quadratically convergent when the following conditions
hold:
(R1) The gradients of the active constraints are linearly independent.
(R2) The multipliers associated with the active constraints are strictly positive.
There exists a scalar ff ? 0 such that
z L(z   ; -   )w - ffkwk 2 (3)
for each w satisfying rc i (z
When the constraint gradients are linearly dependent, quadratic convergence in
the SQP algorithm is lost in even the simplest cases. For example, consider the
problem
subject to z 2 - 0: (4)
The unique solution is z  can be any nonnegative number. If the
multiplier approximation - k is held fixed at - 0 and if z 0 ? 0, then for 0 - 1,
the iteration reduces to z iteration reduces to
z In either case, the convergence is linear.
Wright's stabilized sequential quadratic programming algorithm [19] is obtained
by applying Rockafellar's augmented Lagrangian [18] to the quadratic program (2).
is the penalty parameter at iteration k, then (z local minimax
for the problem
min z
z
Wright shows that this method is locally quadratically convergent if the following
conditions hold:
(W1) The Mangasarian-Fromovitz [15] constraint qualification (MFCQ) holds. In the
context of the inequality constrained problem (1), this means that there exists
y such that c(z   ) +rc(z   )y ! 0.
There exists a multiplier vector whose components associated with the active
constraints are strictly positive.
For some fixed ff ? 0, the coercivity condition (3) holds for all choices of -
satisfying the the following first-order conditions:
r z L(z   ; -
(W4) The parameter ae k tends to zero proportional to the error in (z
Notice that (W1) is weaker than (R1) since there may exist y such that
c(z
even when the constraint gradients are linearly dependent. On the other hand, the
MFCQ does not hold for the example (4), or in cases where an equality constraint is
written as a pair of inequalities.
Let us consider the stabilized iteration (5) for the example (4) with z 0 near z
There are two cases to consider, depending on the choice of - sufficiently
large (for example, - z 2
)), then at the solution of (5), the maximizing -
is positive and the successive iterates are given by
z
Hence, if ae (the error at step k), then we have
z
which implies local quadratic convergence to the solution z
The second case corresponds to the situation where the maximizing - in (5) van-
ishes. For this to happen, we must have - z 2
k ), and the new iterate is
z
Again, if ae
and we have
z
In each of these cases, the convergence to the solution z  locally quadratic.
Also notice in this example that choosing ae k much smaller than the error at step
can slow the convergence. In particular, if ae and the max is changed to sup
in (5), then the scheme (5) reduces to the usual SQP iteration (2) for which the
convergence in the example (4) is linear. On the other hand, we still obtain fast
convergence even when ae k is much larger than the error at step k. For example, if
convergence. Likewise, the initial inequality in
(8) implies that - z 2
which combines with (9) to give z k+1 - z 3
In either
case, when ae k ? 0 is fixed, we obtain cubic convergence near the solution z
Hence, from an implementational viewpoint, a large ae k is safer than a small one.
In this example, quadratic convergence is preserved with the stabilized SQP
scheme even though strict complementarity and the MFCQ are violated. In fact,
strict complementarity is violated, we have convergence in one step.
In this paper, we show in general that Wright's stabilized scheme is locally, quadratically
convergent even though both the MFCQ and strict complementarity are violated.
In contrast to Wright's assumption (W3) that the second-order condition holds for
all multipliers, we give in this paper a local analysis where a second-order condition is
required to hold only at a given solution/multiplier pair (z   ; -   ). When strict complementarity
is violated, our second-order condition is slightly stronger than the usual
second-order condition in that we assume
z L(z
for all w satisfying rc i (z
strengthened form of the second-order sufficient condition first appears in Robinson's
study [17] of Lipschitz stability of optimization problems. Dontchev and Rockafellar
show that this condition along with linear independence of the active constraint
gradients are necessary and sufficient for Lipschitz stability of the solution and multipliers
under canonical perturbations of the problem constraints and cost function.
The strong second-order sufficient condition is stable in the sense that it holds
when r 2
z L(z   ; -   ) and rc i (z   ) are replaced by nearby matrices, while the usual
second-order condition is unstable under problem perturbations. The usual second-order
sufficient condition imposes on w in (10) the additional constraint rc i (z   )w - 0
for every i such that c i (z   must hold for all w in the set
where
If the usual second-order condition holds for some pair (z   ; -   ), then we can perturb
the constraint c(z) - 0 to c(z)
otherwise. For this perturbed problem, (z   ; -   ) again satisfies the first order
conditions, however, the active constraints for the perturbed problem are precisely
the constraints in the unperturbed problem with positive multipliers. Therefore, even
though the usual second-order sufficient condition holds at (z   ; -   ), small perturbations
in the constraints can yield a problem whose stationary point does not satisfy
this condition.
Our analysis of (5) is based on the application of tools from stability analysis.
That is, we introduce parameters in the iteration map and we study how the map
depends on the parameters using a stability result established in [4, Lemma 2.1]. Once
we understand how the iteration map depends on the parameters, we can write down
a convergence theorem. Other applications of stability theory to the convergence of
algorithms and to the analysis of discretizations appear in [3], [4], [5], [6], and [11].
Our analysis of (5) also leads to a new expression for the error in each iterate. In
particular, we show that linear convergence is achieved when ae k is fixed, but small.
This paper is a revised version of the report [12].
Another approach for dealing with degeneracy in nonlinear programming is developed
by Fischer in [9]. In his approach, the original quadratic program (2) is retained,
however, the multiplier estimate is gotten by solving a separate quadratic program.
Fischer obtains quadratic convergence assuming the MFCQ, the second-order sufficient
optimality condition, a constant rank condition for the active constraint gradients
in a neighborhood of z   , and a condition concerning the representation of the cost
function gradient in terms of the constraint gradients. Although these assumptions
seem more stringent than those used in our analysis of Wright's method, there are no
parameters like ae k in Fischer's method that must be specified in each iteration.
2. Convergence theory.
Let z   denote a local minimizer for (1) and let -   be an associated multiplier
satisfying the first-order conditions (6). To state our assumptions, we partition c and
- into (g; h) and (-) where the components of h correspond to components of c
associated with strictly positive components -   of -   , while the components of g are
the remaining components of c for which the associated components -   of -   could
be zero. Let M denote the set of all multipliers associated with a local minimizer z
for (1):
Letting B ffi (z) denote the ball with center z and radius ffi, our main result is the
Theorem 1. Suppose that f and c are twice Lipschitz continuously differentiable
in a neighborhood of a local minimizer z   of (1), that -  is an associated
multiplier in M with -   ? 0, and that
z L(z
for each w such that rh(z   choice of the constant oe 0 sufficiently
large, there exist constants oe 1 , ffi, and -
fi with the property that oe 0 ffi - oe 1 and for each
starting guess (z
where each z k+1 is a strict local minimizer in the stabilized problem (5), - k+1 is the
unique maximizer in (5) associated with z = z k+1 , and ae k is any scalar that satisfies
the condition
Moreover, the following estimate holds:
- k+1 are the closest elements of M to - k and - k+1 respectively.
By Theorem 1, letting ae k go to zero proportional to the total error
leads to local quadratic convergence. Techniques for estimating the error in the
current iterate can be found in [13] and [19]. Since Theorem 1 is a local convergence
result, we assume (without loss of generality), that c(z
constraint is inactive at z   , we simply discard this constraint and apply Theorem 1
to the reduced problem, obtaining a neighborhood where the iterations converge and
holds. When this constraint is included in c, it can be shown that for (z
near (z   ; -   ), the associated component of the maximizing multiplier in (5) vanishes.
Hence, the iterates obtained either with or without this inactive constraint included
in c are identical.
Although an equality constraint does not appear explicitly in (1), we can include
the equality constraint writing it as a pair of inequalities: e(z) - 0 and
One of these constraint functions should be included in g and the other in
h. There are an infinite number of multipliers associated with this pair of constraint
functions with linearly dependent gradients, and it can always be arranged so that
the associated component in -   is strictly positive.
Throughout this paper, k \Delta k denotes the Euclidean norm and fi denotes a generic
positive constant that has different values in different equations, and which can be
bounded in terms of the derivatives through second order of f and c in a neighborhood
of (z   ; -   ) and in terms of fixed constants like ff in (11). In order to prove Theorem 1,
we recast (5) in the form of a perturbed variational inequality. Let T be the function
defined by
r z L(z; -) +r 2
z
where ae and are regarded parameters. Since we later impose a constraint
on ae in terms of p, as in (12), we do not make ae an explicit argument of T .
We study properties of solutions to the following inclusion relative to the parameters:
Find (z; -) such that
where N is the usual normal cone: If - 0, then y 2 N(-) if and only if y - 0
and y T analyzing how the solutions to (15) depend on p, we will establish
Theorem 1.
If (z local solution to (5), then for
a solution to (15), and in this case, (15) represents the first-order
optimality conditions associated with (5). More explicitly, (15) implies that
z
Conditions (17) and (18) are equivalent to saying that - k+1 achieves the maximum
in (5) corresponding to z = z k+1 . By the standard rules for differentiating under a
maximization (see [2]), the derivative of the extremand in (5) with respect to z is
obtained by computing the partial derivative with respect to z and evaluating the
resulting expression at that - where the extremand is maximized. Hence, (16) is
equivalent to saying the derivative of the extremand with respect to z vanishes at
Observe that when is an arbitrary element of M, then
-) is a solution to (15). In this section, we apply the following stability
result, describing how the solution to (15) changes as p changes, to obtain Theorem
1. The proof of this stability result is given in the next section.
Lemma 1. Under the hypotheses of Theorem 1, for any choice of the constant oe 0
sufficiently large and for any oe 1 ? 0, there exist constants fi and ffi such that oe 0
and for each for each ae satisfying
(15) has a unique solution (z;
Moreover, for every p 1 and satisfying (19) for
are the associated solutions to (15), then we have
There are three parts to the proof of Theorem 1. Initially, we show that the
estimate (13) holds for each (z a solution to
associated with Next, we show that for (z sufficiently
close to (z   ; -   ), we can construct a sequence (z contained in a
fixed ball centered at (z   is the unique solution in N (ae k ) to
Finally, we show that for this unique solution (z
(15), z k+1 is a local minimizer of (5).
Part 1. Error estimate. Let oe 1 ? 0 be any fixed scalar (independent of
and let oe 0 and ffi be chosen in accordance with Lemma 1. By Lemma 1, there exists
a with the property that for each
unique solution (z; ae is any scalar that satisfies the
condition
We apply Lemma 1 taking
If - k is near -   , then -
- k is near -   since k-
Suppose that close enough to p   that
and (ae). Note that (19) holds for
Assuming that ae is chosen so that (19) holds for it follows from
(20) that
where
z
Expanding E k in a Taylor series around z   gives
where fi is a generic positive constant. The second inequality (25) is obtained using
the relation ab - (a Combining (22), (23), and (25) establishes the estimate
for z k+1 in Theorem 1.
Dividing (24) by ae gives
Utilizing the lower bound ae - oe 0 kz k \Gamma z   k, it follows that
Hence, dividing (22) by ae and referring to (26), we deduce that
By the triangle inequality, we have
and combining this with (27) gives
This shows that - k+1 is near -   when (z
We now show that
In order to establish this, we exploit the Lipschitz continuity of r z L, the bound (22),
and our observation that - k+1 is near -   to obtain
Expanding r z L(z in a Taylor series around z k and substituting from (16)
gives
kr z L(z
z
z
z
By the triangle inequality, we have
Squaring this gives
If it can be shown that
then by squaring, we have
Combining (31) with (32) and (34) gives
kr z L(z
and combining this with (30) yields
which completes the proof of (29).
To prove (33), we focus on the individual components of -
the relation
for each i. There are three cases to consider:
. For these components, (35) is a triviality.
complementary slackness (18), we have
Expanding c(z k ) in a Taylor expansion around z k+1 , utilizing (32), and taking
absolute values yields
Dividing (36) by ae and utilizing (37) and (26) gives (35).
we have
Dividing this by ae and again utilizing (37) and (26) gives (35).
This completes the proof of both (33) and (29).
Consider the following system of linear equations and inequalities in -:
r z L(z   ;
This system is feasible since any - 2 M is a solution. By (29) and a result of Hoffman
[14], the closest solution -
- k+1 of (38) to - k+1 satisfies
That is, Hoffman's result states that if a linear system of inequalities is feasible, then
the distance from any given point to the set of feasible points is bounded by a constant
times the norm of the constraint violation at the given point. By (29), the norm of the
constraint violation is at most fi -
E k at - k+1 , from which it follows that the distance
from - k+1 to the closest solution of (38) is bounded by a constant times -
c(z   solution of (38) is contained in M and it is the closest element of M
to - k+1 . Relations (25) and (39) combine to complete the proof of (13).
Part 2. Containment. Collecting results, we have shown that if
is sufficiently close to p  has a unique solution (z
N (ae) where ae is any scalar satisfying (21), where z k+1 and - k+1 satisfy (13), and
where - k+1 also satisfies (28). As oe 1 or ffi in Lemma 1 decreases, the constant fi in
(20) can be kept fixed since the set of ae and p that satisfies the constraints of the
lemma becomes smaller. That is, if (20) holds for one set of ae and p values, then it
holds for all subsets. Let -
fi be the constant appearing in (13) that we estimated in
Part 1 using Lemma 1. Given any positive ffl ! 1, let us choose oe 1 and ffi of Lemma 1
small enough that
for all From the analysis of Part 1, both (13) and (28),
there exists, for all
a unique solution (z to (15), and we have
and
where fi 0 denotes the specific constant fi appearing in (28).
We now show in an inductive fashion that for (z sufficiently close to (z   ; -   ),
there exists a sequence (z is the unique
solution to (15) in N (ae k ) corresponding to and to ae k satisfying (40).
In particular, let r 0 be chosen small enough that
Starting from any (z 0
(z   ; -   ), we proceed by induction and suppose that
are all contained in B r 1
exists a unique solution (z
follows that for
By (42) and (43), we have
Combining (43) and (44) yields
Hence, (z
(z   ; -   ) and the induction is complete.
Part 3. Local minimizer. Finally, we show that z k+1 is a local minimizer for
(5). Since -  that by taking r 0 sufficiently small,
k. By complementary slackness (18), we have
As noted after (18), if (z; solution of (15), then -
achieves the maximum in (5) for z = z k+1 . Since the maximizing -
in (5) is a continuous function of z (see [3, Lemma 4]), we conclude that for z near
z k+1 , the maximizing -) has - ? 0; hence, by complementary slackness and
for z near z k+1 , the maximizing - is given by
After making this substitution in (5), the cost function of the minimax problem can
be decomposed into the sum of a convex function of z:
and a strongly convex part
z
The first part is convex since the extremand is a linear function of z and the max of a
sum is less than or equal to the sum of the maxs. The second part is strongly convex
since the Hessian matrix
z
ae k
is positive definite for ae k and r 0 sufficiently small by Lemma 3 in the Appendix.
Hence, the cost function of (5) is a strongly convex function of z in a neighborhood
of z k+1 , and since the derivative vanishes at z k+1 by (16), z k+1 is a local minimum.
This completes the proof of Theorem 1.
3. Stability for the linearized system.
The proof of Lemma 1 is based on the following result, which is a variation of
Lemma 2.1 in [4].
Lemma 2. Let X be a subset of R n and let k \Delta k ae denote the norm on X. Given
In other words, W is the intersection of the closure of X and the ball with center w
and radius - . Suppose that F maps W to the subsets of R m , and T
where P is a set. Let p  be an m \Theta n matrix, and
let - , j, ffl, and fl denote any positive numbers for which fflfl ! 1, -
and the following properties hold:
For some set N oe fT (w; the following problem has
a unique solution for each /
Find x 2 X such that Lx
And if x(/) denotes the solution corresponding to /, we have
for each
Then for each p 2 P , there exists a unique w 2 W such that T (w; p) 2 F (w).
Moreover, for every denotes the w associated with p i , then we
have
Proof. Fix denote the solution to (45) corresponding
to
. Observe that
for each w a contraction on W with contraction constant
ffl. From the assumption T (w
Given w 2 W , we have
since
itself. By the Banach contraction mapping principle, there exists a unique w 2 W
such that is equivalent to T (w; p) 2 F (w) for w 2 W
and we conclude that for each there is a unique w 2 W such that
(w). For denote the associated solutions
to T (w; p) 2 F (w). We have
Rearranging this inequality, the proof is complete.
Proof of Lemma 1. In order to apply Lemma 2 to T defined in (14), we identify
w or x with the pair (z; -), we identify p with the triple (z; choose
and
The set P , chosen later, is a neighborhood of (z   In presenting the linearization
L of Lemma 2, we partition both the constraint function c and the multiplier -
into their components (g; h) and (-) respectively. The linearization L of T (\Delta; p   )
around w   is given by
z
-C A =B @
where
z L(z
In order to apply Lemma 2 to the function T in (14), we need to establish the
Lipschitz property (46). This leads us to consider the problem: Find x 2 X such
that L(x) +/ 2 F (x). Since L has three components, we partition
the linearized problem takes the form: Find (z; -) 2 X such that
where in the last equation (50), we exploit the fact that - ? 0 for all (z; -) 2 X.
In order to analyze the linearization (48)-(50), we introduce the following auxiliary
problem:
min z
z
By (11) and Lemma 3 in the Appendix, the matrix positive definite
with smallest eigenvalue at least ff=2 for ae sufficiently small, where ff appears in (11).
Hence, the extremand in (51) is strongly convex in z and strongly concave in -. By
[8, Prop. 2.2, p. 173], the max and the min can be interchanged. For fixed -, the
min in (51) is attained by the solution z of the following linear equation:
Q+ae
After substituting this z in (51), we obtain an equivalent strongly concave maximization
problem in the variable - and the parameters ', r, and s appear linearly in the
cost function. Since strongly concave maximization problems are Lipschitz continuous
functions of linear parameters in the cost (for example, see [3, Lemma 4]), the
maximizing - is a Lipschitz continuous function of the parameter /, and by (52), the
minimizing z is also a Lipschitz continuous function of /.
Since (48)-(50) are the first-order conditions for a solution of (51), and since the
first-order conditions are necessary and sufficient for optimality in this convex/concave
setting, we conclude that (48)-(50) have a unique solution (z(/); -(/)) depending
Lipschitz continuously on the parameters ', r, and s. We now apply [10, Theorem
2.1] in order to determine more precisely how the Lipschitz constant of (z(/); -(/))
depends on ae. Defining the set
where -(/)), it follows from [10, Theorem 2.1], that if fl 1 and fl 2 satisfy
constants work for all / 1 and / 2 .
After substituting for - in (52), using the relation (Az(/)
we see that z = z(/) satisfies
Q+ae
where C and t are gotten by augmenting B and s by the rows of A and the components
of r associated with i 2 c(/). Let UR denote an orthogonal decomposition of C where
R is right triangular 1 with linearly independent rows and U has orthonormal columns.
After substituting in (54), we obtain the equivalent system
R \GammaaeI
!/
z
\Gamma'
The second equation t)=ae in this system is the definition of - and the
first equation in this system is (54). Since the coefficient matrix is nonsingular for ae
sufficiently small (see [1, Lemma 1.27]), both z(/) and -(/) are Lipschitz continuous
functions of /, where the Lipschitz constant is independent of ae for ae sufficiently
small:
Let V have orthonormal columns chosen so that the matrix ( U
The vector -(/) satisfies (50) and the components - 0 (/) of -(/) associated with
satisfy an analogous relation in (53). Hence, we have
Multiplying by ( U
ae
U
Multiplying again by ( U
Since -(/) is a Lipschitz continuous function of /, it follows from (56) that - 0 (/)
and -(/)) are Lipschitz continuous functions of /, while the remaining components
That is, in the rectangular matrix
of -(/) vanish. Therefore, when c(/ give us the estimates
and fi is independent of ae for ae sufficiently small. By [10, Theorem
2.1], this estimate is valid for arbitrary choices of the parameters.
Given a fixed positive scalar oe 1 , we assume that ae is always - oe 1 . Hence, after
multiplying (58) by ae and adding to (57), we conclude that
for some constant fl independent of ae, where
For the choice
(48)-(50) have the solution z Defining the parameter
it follows from (59) that for all
for all Combining this with (59), we conclude that (45) has a unique
solution and (46) holds for all
Given an arbitrary scalar oe 1 ? 0, and positive scalars oe 0 and ffi, chosen shortly, we
define
choosing oe 0 sufficiently large and ffi sufficiently small, we
will satisfy the condition fflfl ! 1 of Lemma 2, and by choosing ffi smaller if necessary,
the remaining conditions of Lemma 2 will be satisfied.
have
r z L(z; -
z
Expanding in a Taylor series around p   gives
for all p 2 P . Since the right side of (62) is bounded by fi ffi, the constant j in (P1)
can be made arbitrarily small by taking ffi small.
ffl be any positive number small enough that fflfl ! 1 where fl appears in
(59). Observe that
z
z L(z
By the assumed Lipschitz continuity of the
derivatives, and by (61), we have, for all p 2 P and for any choice of w 1 and w 2 ,
Choose oe 0 large enough and ffi small enough that the factor multiplying kw
in (63) is - ffl. This establishes (P2) and fflfl ! 1.
Choosing ae, the set W of Lemma 2 is
By (62) and (63), we have for all w 2 W and
since
smaller if necessary so that
where \Delta is defined in (60). Hence, by (64), we have
for all w 2 W and
for all w 2 W and This completes the proof of (P3) since we already showed
that (45) has a unique solution satisfying (46) for all
Finally, let us consider the condition
of Lemma 2, where Pg. Recalling that
and utilizing (62), we see that (65) is satisfied if
for each (z; - here the factor fl=(1 \Gamma fflfl ) of (65) is absorbed into fi. Assuming
is small enough that fik- rearrange (66) to obtain the equivalent
relation
By the definition of P , ae - oe
(67) will be satisfied. Choosing ffi small enough that (68) is satisfied, it follows that (67)
holds, which implies in turn (65). Since all the assumptions of Lemma 2 are satisfied,
almost directly. The neighborhood N (ae) of Lemma 1 coincides with
W of Lemma 2, while the ball B ffi of Lemma 1 is the same ball appearing in the
definition of P in (61). The constant fi of Lemma 1 is the expression fl=(1 \Gamma fl ffl) of
(47).


Appendix

. A matrix bound.
Lemma 3. Given matrices Q   and B   where Q   is symmetric, suppose that
Then given any ffi ? 0, there exists oe ? 0 and neighborhoods B of B   and Q of Q
such that
ae
for all v 2 R n ,
Proof. If w lies in the null space of B   , then
by (69). There exists a scalar - ? 0 such that kB   uk -kuk for all u in the row
space of B   . Hence, for u in the row space of B   , we have
An arbitrary vector in v 2 R n has the orthogonal decomposition
is in the row space of B   and w is in the null space of B   . Since B
that
ae
Utilizing the inequality
ab - ffla
with
Inserting this in (70), we have
ae
Let us choose oe small enough that
oe
for all v and for all oe. Since the expression B=oe is a continuous
function of B and Q, there exists neighborhoods Q of Q   and B of B   such that
for all v and for all oe, we have
Taking 2ffl, the proof is complete.



--R

New York
Generalized gradients and applications
Lipschitzian stability in nonlinear control and optimization
Lipschitzian stability for state constrained nonlinear optimal control
The Euler Approximation in State Constrained Optimal Control

Characterizations of strong regularity for variational inequalities over polyhedral convex sets
Convex Analysis and Variational Problems
Modified Wilson method for nonlinear programs with nonunique multipliers
Lipschitz continuity for constrained processes
Approximations to the multiplier method
Convergence of Wright's Stabilized SQP Algorithm
Stability in the presence of degeneracy and error estimation
On approximate solutions of systems of linear inequalities
The Fritz-John necessary optimality conditions in the presence of equality and inequality constraints
Perturbed Kuhn-Tucker points and rates of convergence for a class of nonlinear-programming algorithms
Strongly regular generalized equations
The multiplier method of Hestenes and Powell applied to convex programming
Superlinear convergence of a stabilized SQP method to a degenerate solution
--TR
Lipschitzian stability in nonlinear control and optimization
Lipschitzian Stability for State Constrained Nonlinear Optimal Control
Superlinear Convergence of a Stabilized SQP Method to a Degenerate Solution
Convex analysis and variational problems
Characterizations of Strong Regularity for Variational Inequalities over Polyhedral Convex Sets

--CTR
Hiroshi Yamashita , Hiroshi Yabe, Quadratic Convergence of a Primal-Dual Interior Point Method for Degenerate Nonlinear Optimization Problems, Computational Optimization and Applications, v.31 n.2, p.123-143, June      2005
D. Goldfarb , R. Polyak , K. Scheinberg , I. Yuzefovich, A Modified Barrier-Augmented Lagrangian Method for Constrained Minimization, Computational Optimization and Applications, v.14 n.1, p.55-74, July 1999
Jin-Bao Jian, A Superlinearly Convergent Implicit Smooth SQP Algorithm for Mathematical Programs with Nonlinear Complementarity Constraints, Computational Optimization and Applications, v.31 n.3, p.335-361, July      2005
Andreas Fischer , Houyuan Jiang, Merit Functions for Complementarity and Related Problems: A Survey, Computational Optimization and Applications, v.17 n.2-3, p.159-182, December 2000
