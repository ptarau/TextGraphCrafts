--T
Evaluation of Binarization Methods for Document Images.
--A
AbstractThis paper presents an evaluation of eleven locally adaptive binarization methods for gray scale images with low contrast, variable background intensity and noise. Niblacks method with the addition of the postprocessing step of Yanowitz and Brucksteins method added performed the best and was also one of the fastest binarization methods.
--B
Introduction
T HERE is currently a large and growing interest in the
field of document image analysis. Numerous research
groups are trying to design systems for extracting relevant
information from such diverse documents as engineering
drawings, maps, magazines and newspapers, forms, and
mail envelopes [1] [2] [3]. In most of these systems, binarization
of the scanned gray level image (labeling each
pixel print or background) is done prior to further process-
ing. This paper will focus on binarization of map images.
Maps are central to a large number of public and private
agencies. An example is utility maps, which are film copies
of ordinary maps with specialized information added in the
form of text, symbols, lines, and networks. The availability
of high quality digital geographical information systems has
created the possibility for a more flexible use and maintenance
of the utility maps. However, these systems demand
that the maps are available in digital format, preferably in
a vector representation, with the associated text information
in symbolic form.
To digitize a map, it is first scanned at a high resolution,
giving a gray scale image which is then binarized. Sub-
sequently, symbol recognition and line vectorization programs
are used to label symbols, text, lines and solid areas
in the binary raster image. However, these algorithms
require a high quality binary raster image to be able to
correctly identify all the information present. Professionally
drawn map originals for use in offset printing usually
fulfill this requirement when global binarization methods
are applied. In contrast, the print quality of utility maps is
poor due to extensive manual handling and updating over
time, giving areas with variable background intensity, low
contrast and stochastic noise. Therefore, it is essential to
-. D. Trier and T. Taxt are with the University of Oslo, Department
of Informatics, P.O. Box 1080 Blindern, N-0316 Oslo, Norway. Email:
trier@ifi.uio.no, torfinn@tor.pki.uib.no .
find binarization methods which will correctly label all the
information present, even in low contrast areas of the map,
while being insensitive to variable background intensity and
stochastic noise. These requirements exclude global binarization
methods, and lead us to evaluate the best adaptive
binarization methods available in the literature.
In our evaluation, binarization methods requiring manual
inspection of subimages and fine-tuning of parameters
were excluded. This is because the binarization algorithms
are intended to be part of an automatic digitizing system
for maps, being able to process a large number of maps per
time unit. Each gray scale map image may consist of a billion
or more pixels due to its large size and small symbols.
A typical utility map is 1m 2 large and is scanned at 50-m
pixel size (- 500 dpi) with 256 gray levels.
A total of seven subimages taken from five gray scale map
images were used in this evaluation study. Each subimage
was carefully selected to contain particularly challenging
areas with low contrast, variable background intensity or
stochastic noise. The eleven most promising locally adaptive
binarization methods we could identify in the literature
were applied to the test images. The resulting binary
images were compared and evaluated using five visual cri-
teria. We found that the postprocessing step (PS) of the
Yanowitz and Bruckstein method [4] improved all the other
binarization methods as well. Niblack's method [5] with PS
performed the best. Preliminary results of this work have
been presented earlier [6].
II. Binarization Methods
Global binarization methods calculate a single threshold
value for the entire image. Pixels having a gray level darker
than the threshold value are labeled print (black), otherwise
background (white).
Locally adaptive binarization methods (local methods)
compute a threshold for each pixel in the image on the
basis of information contained in a neighborhood of the
pixel. Some methods calculate a threshold surface. If a
pixel (x; y) in the input image has a higher gray level than
the threshold surface evaluated at (x; y), then the pixel
(x; y) in the output image is labeled background, otherwise
it is labeled as print. Other methods do not use explicit
thresholds, but search for print pixels in a transformed image

We tested the following eleven locally adaptive binarization
methods:
1. Bernsen's method (BM) [7],
2. Chow and Kaneko's method (CKM) [8][9],
3. Eikvil et al.'s method (ETMM) [10],
4. Mardia and Hainsworth's method (MHM) [11],
5. Niblack's method (NM) [5],
6. Taxt et al.'s method (TFJM) [12],
7. Yanowitz and Bruckstein's method (YBM) [4],
8. White and Rohrer's Dynamic Threshold Algorithm
9. Parker's method (PM) [14],
10. White and Rohrer's Integrated Function Algorithm
(WRM2) [13], and
11. Trier and Taxt's method (TTM) [15].
These methods were selected because they have been frequently
referred to in the literature, or appeared to be
promising. The first eight methods use explicit thresholds
or threshold surfaces, while the last three methods search
for print pixels after having located the edges. All these
methods are briefly described below.
To demonstrate that global binarization was inadequate
to threshold the map images properly, we also tested four
global binarization methods:
1. Abutaleb's method (AM) [16],
2. Kapur et al.'s method (KSWM) [17],
3. Kittler and Illingworth's method (KIM) [18], and
4. Otsu's method (OM) [19].
The latter two are known to be optimal under two different
normality assumptions [20]. Cho et al.'s improvement
of KIM [21] was not used. Abutaleb's method (AM)
seemed to be the most promising global method, since it
uses some local information to generate an extra feature
for each pixel, followed by a two-dimensional thresholding
to classify the pixels.
A. Bernsen's method
For each pixel (x; y), the threshold T (x;
Z high )=2 is used, where Z low and Z high are the lowest and
highest gray level pixel values in a square r \Theta r neighborhood
centered at (x; y). However, if the contrast measure
then the neighborhood consists
only of one class, print or background. In our images,
wide print areas rarely occur, so the pixel is labeled background
in such cases. We found to be
good choices.
B. Chow and Kaneko's method
We used Nakagawa and Rosenfeld's implementation [9]
of this algorithm. This method calculates a thresholding
surface. The image is divided into non-overlapping win-
dows, and the histograms for each window are tested for
bimodality. As part of the bimodality test, each histogram
is approximated by a mixture of two Gaussian distribu-
tions. For each bimodal window, a threshold is calculated
based on the estimated parameters (- 1 ,
of the bimodal mixture distribution. These thresholds are
interpolated to give thresholds for the unimodal windows.
The window thresholds are smoothed, and the thresholds
for the individual pixels are determined by a bilinear interpolation
of the window thresholds.
Fig. 1. Principle of Eikvil et al.'s method. The large window L with
the small window S in the center is moved across the image in a
zig-zag fashion, in steps equal to the size of S. Each pixel inside
S is labeled print or background on the basis of the clustering of
the pixels inside L.
C. Eikvil et al.'s method
The pixels inside a small window S are thresholded on
the basis of clustering of the pixels inside a larger concentric
window L (Fig. 1). We let S and L slide across the image
in steps equal to the side of S. We used windows of sizes
3 \Theta 3 and 15 \Theta 15 pixels. For all the pixels inside L, Otsu's
threshold T [19] is calculated to divide the pixels into two
clusters. If the two estimated cluster means -
are
further apart than a user-specified limit,
then the pixels inside S are binarized using the threshold
value T . When
all the pixels inside S are
assigned to the class with the closest updated mean value.
We used
D. Mardia and Hainsworth's method
This method first makes an initial binarization, using,
for example, Otsu's method [19]. Then several steps are
iterated until convergence is reached. First, the estimated
mean -
and the number of pixels n i in both print and background
of the current binary image are calculated. Then,
a threshold t   based on these values is calculated, and, for
each pixel, a weighted mean, G, of the pixel and its eight
neighbors is computed. If G - t   , then the pixel is classified
as "print", otherwise "background". The weights may
be equal, or they may be calculated in a more sophisticated
(and time-consuming) manner [11]. We found that
the two approaches gave equivalent binarization results for
our images.
The last step of the iteration is to smooth the binary
image using a 3 \Theta 3 median filter.
E. Niblack's method
The idea of this method is to vary the threshold over
the image, based on the local mean and local standard
deviation. The threshold at pixel (x; y) is calculated as
where m(x; y) and s(x; y) are the sample mean and standard
deviation values, respectively, in a local neighborhood
of (x; y). The size of the neighborhood should be small
enough to preserve local details, but at the same time large
enough to suppress noise. We found a 15 \Theta 15 neighborhood
to be a good choice. The value of k is used to adjust
how much of the total print object boundary is taken as
a part of the given object; well-separated
print objects.
F. Taxt et al.'s method
The image is divided into non-overlapping windows of
size 32 \Theta 32 pixels. The histogram in each window is approximated
by a mixture of two Gaussian distributions.
The parameters of the mixture are estimated using an
expectation-maximization (EM) algorithm [22]. In each
window, the pixels are classified using the quadratic Bayes'
classifier. The EM algorithm requires initial values for the
estimated class means -
1 and
2 , and mixing weight -. We used -
0:5, and the other
four parameter estimates were obtained by clustering [23]
100 randomly selected pixels.
This method gives abrupt changes at the window bor-
ders. ETMM [10] is in fact an improvement of this method.
In ETMM the use of two windows avoids the abrupt
changes, and Otsu's method is used as a fast approximation
of the EM algorithm.
G. Yanowitz and Bruckstein's method
A potential surface passing through the image at local
maxima of the gradient is used as a thresholding surface.
The gradient magnitude image is computed, using, for ex-
ample, Sobel's edge operator [24]. (Jain and Dubuisson
[25] have used the Canny edge detector.) This image is
thresholded and thinned to one-pixel wide lines to identify
edge pixels. The input image is smoothed by a 3 \Theta 3 mean
filter. A potential surface is constructed by an iterative
interpolation scheme [4]. The interpolated surface is made
to pass through the edge pixels. An interpolation residual
R(x;y) is calculated for each non-edge pixel, and the new
pixel value P (x; y) at iteration (n + 1) is calculated as
Rn
Yanowitz and Bruckstein recommend that the value of fi
lie in the interval 1:0 ! fi ! 2:0 for fast convergence (over-
relaxation). However, we found that fi ? 1:0 resulted in
divergence, so chosen.
The image is binarized using the threshold surface. Fi-
nally, false print objects are removed by the postprocessing
step described below.
H. White and Rohrer's Dynamic Thresholding Algorithm
In this method, a biased running average is used as a
threshold at each pixel location. The horizontal running
average H at pixel (x; y) is
where Z is the input image, and the weighing factor f is a
non-linear function of [Z(x; y)\GammaH(x\Gamma1; y)], with
for all arguments. The vertical running average V at pixel
(x; y) is
where the weighing factor g is a non-linear function of
guments. In order to use information on both sides of the
pixel to be thresholded, a number of look ahead pixels are
used. The threshold T at (x; y) is
where b is a bias function. We used '
as the number of look ahead pixels in x- and y-direction,
respectively. The nonlinear functions f and g were kindly
provided as lookup-tables by White [26]. The bias function
b has to be individually tuned for each class of images.
I. Parker's method
This method first detects edges, and then the interior of
objects between edges is filled. First, for each pixel (x; y)
in the input image Z, calculate
where are the 8 eight-connected neighbors
of (x; y). Thus, the negative of the gradient in the
direction of the brightest neighbor is found. Then D is
broken up in regions of size r \Theta r, and for each region, the
sample mean and standard deviations are calculated. Both
values are smoothed, and then bilinearly interpolated to
give two new images M and S, originating from the mean
values and standard deviations, respectively. Then for all
pixels then the pixel
is regarded as part of a flat region, and remains unlabeled
else, if D(x; labeled
print; else, (x; y) remains unlabeled. The resulting binary
image highlights the edges. This is followed by pixel aggregation
and region growing steps to locate the remaining
parts of the print objects. We used the following parame-
ters:
J. White and Rohrer's Integrated Function Algorithm
This method applies a gradient-like operator, called the
activity, on the image. Pixels with activity below a manually
set threshold are labeled '0'. The other pixels
are further tested. If the Laplacian edge operator of the
pixel is positive, then the pixel is labeled '+', otherwise `\Gamma'.
This way, a three-level label-image is constructed, with legal
pixel values '+', `0' and '\Gamma'. The idea is that in a sequence
of labels, edges are identified as '\Gamma+' transitions or `+\Gamma'
transitions. Object pixels are assumed to be '+' and `0' labeled
pixels between a '\Gamma+' and `+\Gamma' pair. A 2 \Theta 2 region
is classified at a time, requiring that all the four pixels are
inside either horizontal or vertical object pixel sequences.
In Parker's paper [14], the test is M(x;y) - \Gamma1 or S(x; y) - \Gamma1,
which is fulfilled by all pixels, since S(x; y) is never negative.
GRAY LEVEL
"GHOST" OBJECT
THRESHOLD SURFACE
Classified as object
Fig. 2. Cross section through an image, illustrating how "ghost" objects
might occur. Based on a figure by Yanowitz and Bruckstein
[4].
K. Trier and Taxt's method
Trier and Taxt [15] have suggested the following 3 improvements
to WRM2: (1) The input image is smoothed
by a 5 \Theta 5 mean filter. (2) The use of search vectors is
abandoned. Instead, all '+'-marked regions are labeled
print, and '\Gamma'-marked regions are labeled background. `0'-
marked regons are labeled print if a majority of the pixels
that are 4-connected to it are '+'-marked, otherwise
it is labeled background. (3) The postprocessing step of
Yanowitz and Bruckstein's method is used to remove false
print objects.
L. Postprocessing step
A postprocessing step is used in YBM and TTM to remove
"ghost" objects (Fig. 2), and can be incorporated
into other methods as well. The average gradient value at
the edge of each printed object is calculated. Objects having
an average gradient below a threshold TP are labeled
as misclassified, and are removed. The main steps of the
algorithm are given below:
1. Smooth the original image by a (3 \Theta mean filter to
remove noise.
2. Calculate the gradient magnitude image G of the
smoothed image, using, e.g., Sobel's edge operator
[24].
3. Select a value for TP . There is no automatic method
to specify TP for a given image, so it is specified by
trial and error (see below).
4. For all 4-connected print components, calculate the
average gradient of the edge pixels. Edge pixels are
print pixels that are 4-connected to the background.
Remove print components having an average edge gradient
below the threshold TP .
III. Experimental Methodology and Results
We present the evaluation results of the published methods
first. Then, we add the postprocessing step and describe
the improved results. Four of the test images are
shown along with some of the 19 binarization results for
each image, to give the reader a fair impression of the methods

A. Test Images
The maps were scanned with a SysScan Flatbed scan-
ner, with pixel size 50-m and 256 gray levels. At this reso-
lution, even the finest details in the maps are preserved in
the gray-scale image. Of the four images shown, NEGATIVE
IMAGE (256 \Theta 256 pixels, Fig. 3a) is from a scanned
negative of a utility map. It shows two bright numerals and
some weak lines. The CABLE IMAGE (512 \Theta 512 pixels,
Fig. 4a) is from a scanned paper copy of a map showing a
dense network of electricity cables, courtesy of Oslo Energi
AS, Norway. The HYDRO IMAGE (256 \Theta 256 pixels,
Fig. 5d) is from a scanned hydrographic original, courtesy
of the Norwegian Mapping Authorities. It shows depth
numbers and contours from a part of the Norwegian coast.
BASE MAP IMAGE (Fig. 5d) shows a seriously degraded
part of a base map, courtesy of Asker Skiklubb,
Norway. The drawing film was too small, and additional
pieces of film have been attached, using transparent tape
at the joints. The portion shown in the image contains four
film pieces and two tape pieces.
B. Evaluation Procedure
As no ground truth was available, which is usually the
case with real data, we had to use visual criteria when
evaluating the binarization results. The result of applying
a binarization method to a test image was evaluated using
the following five criteria:
1. Broken line structures. Gaps of all sizes in lines
were roughly counted. Large gaps were considered
worse than small.
2. Broken symbols, text, etc. Symbols and text
characters with gaps were roughly counted, and the
degree of fragmentation was also assessed.
3. Blurring of lines, symbols and text. Both the
number of blurred print objects, and the degree of
blurring were assessed.
4. Loss of complete objects. The number of print objects
which were completely lost was roughly counted.
5. Noise in homogeneous areas. Both the number
and the size of noisy spots and false objects in both
background and print were estimated.
A scale of 1-5 was used for each criterion. Thus, for each
method, the maximum score on a single test image was
25, and the maximum total score for one method on 7 test
images was 175.
The performance evaluation was a blind test. For each
test image, the order of the 19 binarized images was selected
at random, and independently for the seven different
test images. An expert in the field of image binarization
evaluated the binary images. The 1-5 scale was used as a
relative scale to discriminate as much as possible on each
criterion. Then, the scores on each criterion were adjusted
and weighted against each other by comparing each binary
image with the other binary images which originated from
the same test image and scored about equally well.
BASE MAP IMAGE is exceptional, since it is not a representative
of map images to be digitized. Therefore, the
ranking of the methods is based on the total score for test

I
Scores for the binarization methods. The best scores are shown in boldface. Execution times are given for CABLE IMAGE
(512 \Theta 512 pixels) on a UNIX workstation (Silicon Graphics INDY).
image number a sum sum execution
method
locally adaptive methods
Yanowitz/Bruckstein YBM 17 24 21 21
Trier/Taxt TTM 21 20 21
Niblack NM 20 19 19
Eikvil/Taxt/Moen ETMM
Bernsen BM
White/Rohrer
Parker PM 17 15 14 14 19
Mardia/Hainsworth MHM 9
Taxt/Flynn/Jain TFJM 9
White/Rohrer IFA WRM2
global methods
Kapur/Sahoo/Wong KSWM 12 14
Abutaleb AM 8
Kittler/Illingworth KIM 9 9
modified locally adaptive methods
Niblack NM/PS 23 21 24 22 23 25 22 160 138 1 3 s
Eikvil/Taxt/Moen ETMM/PS 21 23 22 23 21 23 14 147 133 2
Bernsen BM/PS 19 20 23 24 22 24 24 156 132 3 3 s
Parker PM/PS 17
a 3=CABLE IMAGE, 5=HYDRO IMAGE
b Nakagawa and Rosenfeld's implementation
images 1-6. Still, the BASE MAP IMAGE gives us useful
insight into how the different methods behave in a very
difficult situation.
An alternative evaluation method would be to apply an
automatic symbol recognition and line vectorization algorithms
to the binarized images to quantify the binarization
quality. An early attempt is reported in [27].
C. Results
C.1 Published methods
Of the existing methods, YBM was the best overall, and
was somewhat better than TTM (Table I). All the local
methods gave rather poor binarization results on at least
one test image in addition to BASE MAP IMAGE. None
of the global methods gave reasonable binarization results
on any of the test images. When applied to NEGATIVE
IMAGE, IWRM was the best, with only a few small gaps
in the middle vertical line and a few small noisy spots in
the background (Fig. 3c). The tiny line near the top right
corner was lost due to the use of median filtering of the
three level label image (see above). The improvement over
WRM2 is remarkable (Fig. 3d). NM captured all lines and
numerals well, but the background was very noisy (Fig. 3e).
ETMM gave three large gaps in the lines, the part of the
line passing through the "0" was almost lost, and a few
tiny noisy spots in the background were present (Fig. 3h).
Like NM, both PM and BM gave a lot of background noise
(Fig. 3k,m). Both gave gaps in the lines, and PM made the
lines blurred as well. YBM mislabeled the top half of the
middle vertical line as background (Fig. 3g). WRM1 gave
many gaps in some of the lines, and completely lost the
part of the line passing through the '0' (Fig. 3j). CKM,
MHM and TFJM all lost important details in the image
(Fig. 3o-q), as did the global methods (Fig. 3r-u). Of the
global methods, KSWM performed the best (Fig. 3r), and
scored better than the two local methods having the lowest
total score (Table I).
Applied to CABLE IMAGE, YBM resulted in a minor
gap in some numerals, and a few gaps in the lines (Fig. 4c).
IWRM gave a few noisy spots, some of which were attached
to lines and numerals (Fig. 4d). ETMM captured the numerals
well, gave a few gaps in the lines, but produced several
small and medium-sized noisy spots in the background
(Fig. 4e). BM kept too much of the background noise
(Fig. 4f). OM was the best of the global methods, even
better than five of the local methods. However, the lines
were smeared out, especially at junctions, weak lines almost
vanished, and the numerals were fragmented (Fig. 4g).
Applied to HYDRO IMAGE, YBM performed the best
overall. Still, many numbers were attached to lines or other
numerals (Fig. 5e). Of the global methods, OM performed
the best, but most of the numerals were smeared out, and
many more numerals were attached to other print objects
than by YBM (Fig. 5f).
On BASE MAP IMAGE, the most difficult image, all
published methods had difficulties in capturing the essential
details. BM performed the best, but retained too much
background noise (Fig. 5h), as it did when applied to the
other test images.
C.2 Performance with postprocessing step
The postprocessing step used in YBM and IWRM was
added to ETMM, NM, BM and PM. For all the four meth-
ods, most mislabeled objects were removed by the postprocessing
step. Except for PM, the modified methods scored
higher than their original counterparts on all seven test im-
ages. The postprocessing step was not added to the other
methods, as they have serious defects that the postprocessing
step could not repair.
NM/PS was the best method overall, slightly better than
ETMM/PS and BM/PS (Table I).
Applied to NEGATIVE IMAGE, NM/PS was the best,
with only one gap in the lines, and just a few noisy spots
(Fig. 3i). ETMM/PS, BM/PS and PM/PS all gave more
gaps in the lines (Fig. 3j-l). On CABLE IMAGE, BM/PS
was the best (Fig. 4i), and ETMM/PS performed almost as
well (Fig. 4h). For both methods, the postprocessing step
removed almost all the noisy spots. On HYDRO IMAGE,
NM/PS performed almost as well as YBM (Fig. 5g), but
had the same shortcomings.
BM/PS was best on BASE MAP IMAGE, and performed
remarkably well on this very difficult image (Fig. 5i). All
lines and symbols were captured, although some noisy
spots were attached. The tape strips left just a few marks.
ETMM/PS had some special problems, resulting in horizontal
stripes in some regions. This is due to the use of
updated class mean values for regions with only one class
present. In some of these regions, the current values of the
updated class mean for both print and background tend
to have one range of values in odd-numbered rows (when
sliding from left to right, Fig. 1) contrasted with another
range of values in even-numbered rows (when sliding the
opposite way).
We selected the parameter T P manually by trial and er-
ror. The best identified value for TP was close to the mean
value -
-G of the gradient image G for six of the seven test
images. Thus, can not always be used.
NM and BM were the fastest of the local methods according
to execution times for CABLE IMAGE (Table I).
Even with the addition of the postprocessing step, these
two methods were faster than the other local methods.
IV. Discussion
We have evaluated the most promising published locally
adaptive binarization methods. Our experimental results
on a range of gray scale images of maps with low print
quality indicates that no single method performs better
than the other methods on all the images. In our evaluation
scheme, Yanowitz and Bruckstein's method [4] had the
best overall performance of the published methods, slightly
better than our improved White and Rohrer's method [15].
Furthermore, inclusion of the postprocessing step used in
the Yanowitz and Bruckstein method lead to improvement
of the other binarization methods. By adding the post-processing
step, Niblack's method [5] was the best over-
all, followed by Eikvil et al.'s method [10] and Bernsen's
method [7]. These five methods were significantly better
than the other methods.
We believe that for images like HYDRO IMAGE, better
binarization results will be achieved if the original map is
scanned at even a higher resolution, for example at 25-m
pixel size instead of 50-m. Then, we will expect fewer
numerals to be attached to other print objects after bi-
narization. However, some numerals will still suffer from
this problem, as they actually are (unintentionally) written
attached to other objects in the original map.
More generally, even the best local method does not necessarily
give sufficiently high quality binarization results
which could be used for further automatic processing. The
map may simply be of such a low printing quality that no
available automatic binarization method will work properly
on it. In such cases, direct recognition of the objects
in the gray scale image may be a better alternative.
This evaluation study includes only seven test images.
Still, we believe these images contain numerous challenges
to establish a tentative ranking of the locally adaptive binarization
methods that is applicable to a wide range of
images. For document image applications, it is probably
sufficient to implement a few of the best methods. If the
postprocessing step is to be included, a method to automatically
determine the threshold TP must be found.

Acknowledgments

We thank Anil K. Jain, Fritz Albregtsen, Knut Liest-l
and Xinli Wang for useful discussions. This work was supported
by The Research Council of Norway.



--R



"Special issue on postal processing and character recognition"
"A new method for image segmentation"
An Introduction to
"Evaluation of binarization methods for utility map images"
"Dynamic thresholding of grey-level images"
shown in black.
"Automatic detection of the left ventricle from cineangiograms"
"Some experiments on variable thresholding"
"A fast adaptive method for binarization of document images"
"A spatial thresholding method for image segmentation"
"Segmenta- tion of document images"
"Image thresholding for optical character recognition and other applications requiring character image extraction"
"Gray level thresholding in badly illuminated im- ages"

"Automatic thresholding of gray-level pictures using two-dimensional entropy"
"A new method for gray-level picture thresholding using the entropy of the his- togram"
"Minimum error threshold- ing"
"A threshold selection method from gray-level histograms"
"Maximum likelihood thresholding based on population mixture models"

Statistical Analysis of Finite Mixture Distributions
Algorithms for Clustering Data

"Segmentation of X-ray and C-scan images of fiber reinforced composite materials"

"Goal-directed evaluation of binarization methods"
--TR

--CTR
Ilya Blayvas , Alfred Bruckstein , Ron Kimmel, Efficient computation of adaptive threshold surfaces for image binarization, Pattern Recognition, v.39 n.1, p.89-101, January, 2006
ivind Due Trier , Anil K. Jain, Goal-Directed Evaluation of Binarization Methods, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.17 n.12, p.1191-1201, December 1995
I. Abraham , R. Abraham , A. Desolneux , S. Li-Thiao-Te, Significant edges in the case of non-stationary Gaussian noise, Pattern Recognition, v.40 n.11, p.3277-3291, November, 2007
Victor Wu , R. Manmatha , Edward M. Riseman, Finding text in images, Proceedings of the second ACM international conference on Digital libraries, p.3-12, July 23-26, 1997, Philadelphia, Pennsylvania, United States
Ankush Mittal , Ankur Jain , Ganesh K. Agarwal, Audio---Video based People Counting and Security Framework for Traffic Crossings, Journal of VLSI Signal Processing Systems, v.49 n.3, p.377-391, December  2007
Cline Thillou , Silvio Ferreira , Bernard Gosselin, An embedded application for degraded text recognition, EURASIP Journal on Applied Signal Processing, v.2005 n.1, p.2127-2135, 1 January 2005
Michael Droettboom , Ichiro Fujinaga , Karl MacMillan , G. Sayeed Chouhury , Tim DiLauro , Mark Patton , Teal Anderson, Using the Gamera framework for the recognition of cultural heritage materials, Proceedings of the 2nd ACM/IEEE-CS joint conference on Digital libraries, July 14-18, 2002, Portland, Oregon, USA
S. Hemachander , A. Verma , S. Arora , Prasanta K. Panigrahi, Locally adaptive block thresholding method with continuity constraint, Pattern Recognition Letters, v.28 n.1, p.119-124, January, 2007
Salvatore Tabbone , Laurent Wendling, Multi-scale binarization of images, Pattern Recognition Letters, v.24 n.1-3, p.403-411, January
Badekas , N. Papamarkos, Optimal combination of document binarization techniques using a self-organizing map neural network, Engineering Applications of Artificial Intelligence, v.20 n.1, p.11-24, February, 2007
Ardhendu Behera , Denis Lalanne , Rolf Ingold, DocMIR: An automatic document-based indexing system for meeting retrieval, Multimedia Tools and Applications, v.37 n.2, p.135-167, April     2008
George Nagy, Twenty Years of Document Image Analysis in PAMI, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.22 n.1, p.38-62, January 2000
A. K. Jain , M. N. Murty , P. J. Flynn, Data clustering: a review, ACM Computing Surveys (CSUR), v.31 n.3, p.264-323, Sept. 1999
