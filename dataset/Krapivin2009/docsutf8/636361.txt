--T
Approximate Nonlinear Filtering for a Two-Dimensional Diffusion with One-Dimensional Observations in a Low Noise Channel.
--A
The asymptotic behavior of a nonlinear continuous time filtering problem is studied when the variance of the observation noise tends to 0. We suppose that the signal is a two-dimensional process from which only one of the components is noisy and that a one-dimensional function of this signal, depending only on the unnoisy component, is observed in a low noise channel. An approximate filter is considered in order to solve this problem. Under some detectability assumptions, we prove that the filtering error converges to 0, and an upper bound for the convergence rate is given. The efficiency of the approximate filter is compared with the efficiency of the optimal filter, and the order of magnitude of the error between the two filters, as the observation noise vanishes, is obtained.
--B
Introduction
Due to its vaste application in engineering, the problem of filtering a random signal X t
from
noisy observations of a function h(X t ) of this signal has been considered by several authors.
In particular, the case of small observation noise has been widely studied, and several articles
are devoted to the research of approximate filters which are asymptotically efficient when the
observation noise vanishes. Among them, one notices a first group in which a one-dimensional
system is observed through an injective observation function h (see [4, 5, 1]); in this case, the
filtering error is small when the observation noise is small, and one can find efficient suboptimal
finite-dimensional filters. The multidimensional case appears later on with [6, 7], but an assumption
of injectivity of h is again required; in particular, the extended Kalman filter is studied in
When h is not injective, the process fX t g cannot always be restored from the observation of
so the filtering error is not always small; such a case is studied in [3]. However, there
are some classes of problems in which fX t
g can be restored from fh(X t
)g; in these cases, the
filtering error is small and one again looks for efficient suboptimal filters. For instance, fX t g
is sometimes obtained from fh(X t
)g and its quadratic variation, see [2, 8, 9, 11]. Here, we are
interested in another case in which h(X t ) is differentiable with respect to the time t, and fX t g
is obtained from fh(X t
)g and its derivative. More precisely, we consider the framework of [10]
which we now describe.
We consider the two-dimensional process X
given by the It"o equation
dx (1)
dx (2)
(1)
with initial condition X 0
and we are concerned by the problem of estimating the
signal
when the observation process is modelled by the equation
dy t
and f
are standard independent real-valued Wiener processes, and " is a small
nonnegative parameter. In particular, if f 1
, then x (1)
is the position of some moving
body on R, x (2)
is its speed, the body is submitted to a dynamical force described by f 2
and to
a random force described by oe, and one has a noisy observation of the position.
if the functions h and x 2
are injective, then the signal X t
can
(at least theoretically) be exactly restored from the observation; we are here interested by the
asymptotic case " ! 0, and we look for a good approximation of the optimal filter
This approximation should be finite dimensional (solution of a finite dimensional equation driven
by y t ).
The same problem has been dealt with in [10] (with oe constant), by means of a formal
asymptotic expansion of the optimal filter in a stationary situation. Our aim is to work out a
rigorous mathematical study of the filter proposed by [10], namely the solution M
of
with F 12
and with initial condition M 0
This filter does in fact correspond to
the extended Kalman filter with stationary gain if one neglects the contribution of the derivatives
of f other than @f 1
. The stability of this filter is not evident and requires some assumptions.
When it is stable, we prove in this work that
x (1)
and
x (2)
O(
We also verify that (6) can be improved when oe is constant, h is linear, and f 2
is linear with
respect to x 1
(this case will be referred to as the almost linear case). The proofs follow the
method of [7].
The contents are organized as follows. In Section 2, we introduce the assumptions which will
be needed in the sequel, and we study the filtering error as " converges to zero; more precisely,
we obtain the rate (5). In Section 3 the error between the approximate filter and the optimal
filter is studied, and we prove (6). Section 4 is devoted to the almost linear case.
Notations
The following notations are used:
"oe
F 22
and
are the Jacobian matrices of f and \Sigma;
is either a 2 \Theta 2 matrix (if \Phi is R 2 -valued) or a line-vector (if \Phi is
real-valued), see Section 3;
The symbol   is used for the transposition of matrices.
When describing the behaviour of approximate filters, we will write asymptotic expressions
with the meaning given by the following definition.
Definition 1.1 Consider a real or vector-valued stochastic process f t
g. If fi is real and p  1,
we will write that
when for some q  0, ff ? 0 and some positive constants C 1 , C 2 , c 3 ,
e \Gammac 3 t=" ff
for t  0 and " small. In this situation the process f t g is usually said to converge to zero with
rate of order " fi , in a time scale of order " ff .
2 Estimation of X
The following assumptions will be used throughout this article. The last one depends on a
parameter ffi  1.
is a random variable, the moments of which are finite;
are standard independent Wiener processes independent from X 0
(H3) the function h is C 3 with bounded derivatives, and h 0 is positive;
(H4) the function f is C 3 with bounded partial derivatives and F 12
is positive;
(H5) the function oe is C 2 with bounded partial derivatives;
(H6.ffi) one hasffi
oe
F
for any x =
), and for some positive
oe,
H and
F .
We consider the system (1)-(2) and the filter (3). We let F t
be the filtration generated by
t be the filtration generated by (y t ). Assumption (H6.ffi) says that the system
does not contain too much nonlinearity; when it is not satisfied, there may be a small positive
probability for the filter to loose the signal (see [8] for a similar problem). This is a rather
restrictive condition, so we discuss at the end of the section the general case where it does not
hold.
Theorem 2.1 Assume (H1) to (H5). There exists some universal ffi ? 1 such that if (H6.ffi)
holds, then one has
x (1)
in L p for any p  1.
Before entering the proof of this result, we re-scale the system in order to reduce the notation
in (H6.ffi). If we replace the processes x (1)
t and y t by x (1)
t =oe and y t =(oe
F
the functions f 1
, oe and h are respectively replaced by
(oe
(oe
oe(oe
F
and " is replaced by " / (oe
F
H). We can apply the filter (3) to this new system, and we obtain
F
t =oe. This shows that the problem can be reduced to the case
Now consider a change of basis defined by a matrix T and its inverse T \Gamma1 , where
"=2
"=2
Then consider the process
We are going to check that Z t is the solution of a linear stochastic differential equation; the
study of the exponential stability of this equation will enable the estimation of both components
of Z t
, and the theorem will immediately follow.
An equation for Z t
From equations (1) to (3), we have
))dt
d
In this equation, we introduce the Taylor expansions for the functions f and h
and
h(x (1)
are R 2 and R valued processes depending on fX t g and fM t g, and
We obtain a linear equation for X t
. By applying the transformation (7), we deduce for Z t
an equation of the type
d
The precise computation shows that
A t
with
A (11)
A (12)
A (11)
A (21)
A (22)
and where e
A t is a 2 \Theta 2 matrix valued process which is uniformly bounded as " converges to 0;
similarly, the matrix-valued process U t is also uniformly bounded.
Stability of A t
A t
is the constant matrix
and
A t
A
In the general case ffi ? 1, the coefficients of
A t
A
can be controlled so that this matrix is
uniformly close to \Gamma2I if ffi is close to 1; in particular, if
2, one can choose ffi so
that
A
I
and therefore
I
if " is small.
End of the proof of Theorem 2.1
Our goal is now to deduce an estimate of Z t in L 2p for p integer. From It"o's Formula and (8),
the process kZ t
is solution of
d
We deduce that the moment of order p of kZ t k 2 is finite, and that
d
dt
From (9), one has
Z
As a consequence of the Cauchy-Schwarz inequality, one has
kU
Thus we obtain the inequality
d
dt
\Gammap
ff
Moreover, there exists C 0
such that
so
d
dt
By solving this differential inequality, one obtains that for some C 00
Thus Z t is O(" 1=4 ), and the order of magnitude of the components of X follows from (7)
and the form of T \Gamma1 . \Pi
We remark in (10) that the time scale of the estimation is of order
"; one can compare it
with the time scale " obtained when the observation function is injective (see for instance [5]).
This means that it takes here more time to estimate the signal, and this is not surprising since
the second component of the signal is not well observed. There are also other systems where the
time scale is not the same for the different components of the signal (see [8]).
In Theorem 2.1, we need the assumption (H6.ffi) which is a restriction to the non linearity of
the system; otherwise it is difficult to ensure that the filter does not loose the signal (this problem
also occurs in [8]). Actually, we have chosen the filter (3) because it gives a good approximation
of "
next section), but it is not the most stable one. If in (4) we replace the processes
by constant numbers  oe,
F and
H, we obtain a filter with constant
gain; we can again work out the previous estimations and prove that the result of Theorem 2.1
holds for this filter without (H6.ffi), as soon as
F
Thus we have two filters: a filter which is stable and tracks the signal under rather weak assump-
tions, and the filter (3) which seems more fragile but gives (under good stability assumptions) a
better approximation of the optimal filter.
3 Estimation of "
The main result contained in this section is Theorem 3.1, which states the rate of convergence
of the approximate filter considered in this paper towards the optimal filter. In order to give
a proof of this theorem a sequence of steps are needed: a change of probability measure, the
differentiation with respect to the initial condition and an integration by parts formula. A similar
method of proof is adopted in [7]. As in Theorem 2.1, we may have a problem of stability if the
general non linear case.
Theorem 3.1 Assume (H1) to (H6.ffi) and
(H7) The law of X 0
has a C 1 positive density p 0
with respect to the Lebesgue measure
and rp 0
) is in L 2 .
If ffi in (H6.ffi) is close enough to 1, then the filter M t given by equation (3) satisfies
x (1)
O(
in L 2 .
The rest of this section is devoted to the proof of this theorem. We suppose as in Theorem
2.1 that
1. Consider the matrix
which depends only on M t
. Notice that P t
is the solution of the stationary Riccati equation
e
F   (M t
)\Sigma   (M t
with
e
and that the process R t of (4) is
We will also need the inverse of P t
namely
Change of probability measure
Our random variables can be viewed as functions of the initial condition X 0
and of the Wiener
processes w and
w. We are going to make a change of variables; in view of the Girsanov theorem,
this can be viewed as a change of probability measure; however, all the estimations will be made
under the original probability P . Thus consider the new probability measure which is given on
F t by
d
dP
where
ae
\Gamma"
Z th(x (1)
s
Z th 2 (x (1)
s
ds
oe
The probability
P is the so-called reference probability, and one checks easily from the Girsanov
Theorem that y t =" and w t are standard independent Wiener processes under
. Let us define
now the probability measure e
P on F t
by
d
where
s
s
ds
oe
Then, the processes
e
Z t\Sigma   (M s
s
ds
and y t =" are standard independent Wiener processes under e
P . On the other hand, one has
and
log(L t
Z th(x (1)
s
Z th 2 (x (1)
s
ds \Gamma
Z t\Sigma   (M s
s
s
Differentiation with respect to the initial condition and an estimation
The random variables involved in our computation can now be viewed as functions of
and fy t g; let us denote by r 0
the differentiation with respect to the initial condition X 0
puted in L p ). In particular, we can see on (13) and (14) that the processes X t
and log(L t
are differentiable, and we obtain respectively matrix and vector-valued processes. Our aim is to
estimate the process
log(L t
U
with
Then an integration by parts will enable to conclude.
By applying the operator r 0
to (14), one gets
log(L t
Z th 0 (x (1)
s
x (1)
s
(dy s
s
s
s
="
Z th 0 (x (1)
s
x (1)
s
d
s
We can also differentiate (13), and if \Sigma 0 is the Jacobian matrix of \Sigma, we obtain
The matrix r 0
X t is invertible and It"o's calculus shows that
From this equation and (16) one can write that:
d
log(L t   t )(r 0
="
log(L t   t )(r 0
log(L t   t )(r 0
since one has h 0 (x (1)
x (1)
On the other hand, from the equations of X t and M t (equations (1) and (3), respectively),
one has
By writing the differential of P
in the form
we obtain
d
dt
d
d
J (2)
One can write the Taylor expansions for f and h
h(x (1)
with
By using these expansions together with the consequence of (12)
in equation (19), we obtain
d
dt
d
d
J (2)
dt
By adding this equation to (18), we obtain that the process V t
of (15) satisfies
)d
d
d
J (2)
dt (20)
is the matrix given by
Consider also the matrix-valued process
A t
)\Sigma   (M t
Then the equation (20) can be written in the form
where
U
\Gamma"R
J (2)
="
U
U
(apply (12) for the last line). We deduce that E[kV 0
is of order " \Gamma3 , and that
d
dt
We have to estimate the terms of the right-hand side.
By computing the matrix A t , we obtain that
A t
A t
with
A (11)
A (21)
A (12)
A (22)
and e
A t is uniformly bounded. As in the proof of Theorem 2.1, we see that if then the
matrix
A t is simply
A t
which satisfies
A
Thus, for
2, when ffi is close enough to 1, and when " is small enough, we have
ff
We also notice that
ffp
and that
U
)U is bounded. Thus (24) implies that for " small,
d
dt
Let us first estimate J (3)
. We deduce from the Riccati equation (11) satisfied by P t that the
process S t
defined in (21) satisfies
By computing this matrix and applying Theorem 2.1, we check that
in the spaces L p . Thus
The term \Sigma   (M t
)U is easily shown to have the same order of magnitude. On the
other hand, by looking at the equation of M t and by applying It"o's formula, we can prove that
for any C 2 function ae with bounded derivatives, one has
By applying this result to the functions involved in P \Gamma1
, it appears that
J (1)
We deduce that the terms of J (3)
involving J (1)
and J (2)
are also of order " \Gamma1 . Finally, OE t
and
are respectively of order " 1=2 and " 3=2 , so the last term is of order " \Gamma1 , and we deduce that
We can also estimate J (4)
and J (5)
and check that they are of order " \Gamma3=4 . Thus (26) enables
to conclude that
in L 2 . We can take the conditional expectation with respect to Y t in this estimation, because
the conditional expectation is a contraction in L 2 ; thus E[V t
is O(1=
") in L 2 , and therefore,
we obtain from the definition (15) that
log(L t   t )(r 0
Application of an integration by parts formula
The estimation of the right-hand side of (27) can be completed by means of an integration by
parts formula. It is proved in Lemma 3.4.2 of [7] that if
w; y) is a functional defined
on the probability space which is differentiable with respect to the initial condition (in the spaces
iis the differentiation with respect to the ith component of X 0
, then
0: (28)
We can write equation (17) in the form
)\Sigma   (M t
dt
)d e
with (r 0
This equation can be differentiated with respect to X 0
, so we can apply
the integration by parts formula (28) to the coefficients of the matrix (r 0
its ith line. Then
(p \Gamma1@p 0
0:
By summing on i and multiplying by U , we have
log(L t
r i(r 0
U
0: (30)
The first term of (30) is exactly the term that we want to estimate in (27).
For the second term of (30), if
we have from (17) and (22) that
We proceed as in the study of (23). The stability of the matrix A t
which has been obtained in
(25) and the boundedness of U implies that (r 0
exponentially small in L 2 , so
the second term is negligible.
Let us study the third term of (30). If
then by differentiating (29) and transforming back e
w into w, we get
U
\Gamma\Sigma   (M t )P \Gamma1
with
By summing on i and using
@ae
is the jth line of ae, we obtain that \Phi t
is solution of
\Gamma\Sigma   (M t
where oe 0 is the Jacobian of oe. A computation shows that
The multiplication on the right by U yields a process of
is also O(" \Gamma1 ), and the term involving the second derivative of oe is O(" \Gamma1=2 ). By proceeding
again as in the study of (23), we deduce that \Phi t is of order " \Gamma1=2 .
Thus (27), (30) and the estimation of \Psi t
and \Phi t
yield
We multiply on the right by the matrix U
, the coefficients of which are of order " 3=2 for
the first column and " for the second column, and we deduce the order of "
which was
claimed in the Theorem. \Pi
4 An almost linear case
It is interesting to consider a particular case in which oe, h 0 and F 12
are constant, so that the
system (1)-(2) is 8
dx (1)
x (2)
dx (2)
dy
In particular, (H6.ffi) holds with it is possible to improve the upper bounds given
in Theorem 3.1. The result is stated in the following proposition.
Proposition 4.1 Assuming that (H1) to (H7) hold for (32), the filter M t given by equation
verifies
x (1)
in L 2 .
Proof The proof follows closely the sequence of steps adopted in Theorem 3.1. The matrices
R are now constant; the processes J (1)
, J (2)
and J (5)
are zero. The order
of S t is improved into
and
so that
is of order " \Gamma3=4 . Thus V t is O(" \Gamma1=4 ) and we obtain O(" \Gamma1=4 ) in (27).
For the end of the proof, we see that
so
is bounded. Multiplication by U yields a process of order " \Gamma1=2 , so the process \Phi t
of (31) is
bounded for " small. We can conclude that
and deduce the proposition. \Pi



--R

On some approximation techniques in non linear filtering theory
Piecewise monotone filtering with small observation noise
Approximate filter for the conditional law of a partially observed process in nonlinear filtering
Asymptotic analysis of the optimal filtering problem for one-dimensional diffusions measured in a low noise channel
Nonlinear filtering of one-dimensional diffusions in the case of a high signal-to- noise ratio
Nonlinear filtering and smoothing with high signal-to-noise ratio
Efficiency of the extended Kalman filter for non linear systems with small noise
Estimation of the quadratic variation of nearly observed semimartingales with application to filtering
Filtrage lin'eaire par morceaux avec petit bruit d'observation
Asymptotic analysis of the optimal filtering problem for two dimensional diffusions measured in a low noise channel
Nonlinear filtering and control of a switching diffusion with small observation noise
--TR
