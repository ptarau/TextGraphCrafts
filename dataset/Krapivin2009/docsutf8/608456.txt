--T
A statistical approach to case based reasoning, with application to breast cancer data.
--A
Given a large set of problems and their individual solutions case based reasoning seeks to solve a new problem by referring to the solution of that problem which is "most similar" to the new problem. Crucial in case based reasoning is the decision which problem "most closely" matches a given new problem. A new method is proposed for deciding this question. The basic idea is to define a family of distance functions and to use these distance functions as parameters of local averaging regression estimates of the final result. Then that distance function is chosen for which the resulting estimate is optimal with respect to a certain error measure used in regression estimation. The method is illustrated by simulations and applied to breast cancer data.
--B
Introduction
Assume that one is interested in the solution of a problem where the latter is described by a
number of observable variables. It is not necessary that these variables determine the solution
completely, but we assume that there is a correlation between the observable variables and the
solution. Furthermore, we assume that a list of problems of the same type is available for which
the values of the observable variables and the solutions are known. Instead of the categories
problem/solution one may also think of premise/conclusion, cause/effect, current state/final result,
or, more concretely, health condition/observed survival time.
Case based reasoning seeks to solve a new problem by refering to the solution of that problem
which is "most similar" to the new problem. The basic assumption is that the solution of the
problem in the data base which is "most similar" to the new problem is close to the unknown
solution of the new problem. This is different to rule based reasoning where the knowledge is
represented in the rules rather than in a data base built up by previous experience. The phrases
case based reasoning and rule based reasoning were coined in the field of artificial intelligence (see,
e.g., Menachem and Kolodner (1992)).
Crucial in case based reasoning is the decision, which cases "most closely" match a given new
case. In this article we propose a new method for deciding this question. The basic idea is to define
a family of distance functions, to use these distance functions as parameters of local averaging
regression estimates of the final outcome, and to choose that distance function for which the
resulting estimate is optimal with respect to a certain error measure used in regression estimation.
In this article we apply case based reasoning in the context of prediction of survival times of
breast cancer patients treated with different therapies. For each therapy a list of cases is given
which includes observed survival time and variables describing the case. Examples for the latter are
size of primary tumor, number of affected lymph nodes and menopausal status. These categorial
variables are part of the tumor classification system TNM. From each of these lists one selects those
cases which are "most similar" to a given new case. Then a physician tries to gain information
about an appropriate therapy for the new patient by considering the therapy and the final result
A statistical approach to case based reasoning 3
of these cases.
A naive approach to decide which cases "most closely" match a new case is to use only those
cases which have the same values in all variables as the new case. Unfortunately, if the number of
observable variables is large, then there will be usually none or only very few such cases. Therefore
this naive approach is in general not useful.
Another approach was used in the context of predicting survival times of breast cancer patients
by Mariuzzi et al. (1997). There the range of each observable variable is divided into (four)
quartiles. The values of the variables are coded by 1, 2, 3 or 4 according to the quartile to which
they belong to. Then the distance of observable variables (x (1)
which are coded by (i l ), is defined by
d
l
and those cases are chosen which are (with respect to this distance function) most closely to the
new case.
The main drawback of this distance is that it does not reflect a possibly different influence
of each individual variable on the final result (for instance, the values of x (1) might influence the
survival time much more than the value of x (2) ).
In this article we propose a new method to determine that cases of a data base which "most
closely" match the new case. In Section 2 we give a short introduction to nonparametric regression.
The proposed method is described in detail in Section 3. It is illustrated with some simulated data
in Section 4 and applied to breast cancer data in Section 5.
2. Nonparametric regression
independent identically distributed random variables, where
X is R d -valued, Y is real-valued and EY 2 ! 1. In regression analysis one wants to predict Y
after having observed X, i.e., one wants to find a function m   : R d ! R such that m   (X) is "close
to" Y . If closeness is measured by the mean squared error, one is interested in a function m   which
4 J. Dippon et al.
minimizes the so-called L 2 risk Efjm
Introduce the regression function xg. For an arbitrary (measurable) function
R one has
Z
where - denotes the distribution of X. Therefore m  and the L 2 risk of an arbitrary function
f is close to the optimal value if and only if the (squared) L 2 error
The optimal predictor m depends on the distribution of (X; Y ). In applications this distribution
will be usually unknown, thus m will be unknown, too. But often it is possible to observe independent
copies Based on the data
the nonparametric regression problem asks for an estimate mn of m such
that the L 2 error
R jmn
2.1. Local averaging estimates
The response variables Y i can be rewritten as
0:
Hence one can consider the Y i 's as the sum of the function m evaluated at X i and a random error
with zero mean. This motivates to construct an estimate of m(x) by averaging over those Y i 's
where X i is close to x (hopefully then m(X i ) is close to m(x) and the average of the ffl i is close to
zero). Such an local averaging estimate can be represented as
are nonnegative weights which sum up to
one.
A statistical approach to case based reasoning 5
The most popular local averaging estimate is the Nadaraya-Watson kernel estimate (Nadaraya
(1964) and Watson (1964)) with weights
hn
hn
R is the so-called kernel function,
is the so-called
bandwidth, and for
hn
Often one uses spherical symmetric kernels K which satisfy for a univariate kernel function R! R,
which we denote again by K,
where jxj is the euclidean norm of x. For such kernel functions one gets
hn
hn
Usually kernel functions are chosen such that K(0) ? 0,
If the so-called naive kernel
are used, then m(x) is estimated by the average of those Y i where X i is in a ball with radius h (1)
and center x. For more general K : R! R+ , e.g. the Gaussian kernel
one uses a weighted average of the Y i 's as an estimate of m(x) where more weight is given to those
are close to x.
2.2. Choice of bandwidth
The choice of the bandwidth hn is crucial for the kernel estimate. If the components of hn are too
small, then only very few of the Y i 's have a weight not close to zero and the estimate is determined
by only these very few Y i 's which induces a high variance in the estimate. On the other hand, if
the components of hn are too big, then there will be Y i 's which are far away from x and which
have a big weight. But if X i is far away from x then even if m is smooth m(X i ) might be far away
from m(x) which induces again a large error into the estimate.
6 J. Dippon et al.
For the quality of the estimate it is important to apply a method which chooses hn close to
the optimal value (which depends on the usually unknown distribution of (X; Y using only the
data Dn . A well known method which tries to do this is cross-validation, which will be described
in the sequel. For further information see H-ardle (1990), e.g.
Recall that the aim in nonparametric regression is to construct mn such that the L 2 risk
is small. So the bandwidth of a kernel estimate should be chosen such that (2) is minimal compared
to other choices of the bandwidth. In an application this is not possible because (2) depends on
the unknown distribution of (X; Y ). What we will propose is to estimate the L 2 risk (2) and to
choose the bandwidth by minimizing the estimated L 2 risk.
For a fixed function f : R d ! R the L 2 risk can be estimated by the so-called
empirical
If we use (3) with (which depends on Dn ) to estimate (2), then this so-called resubstitution
estimate is a too optimistic estimate of the L 2 risk and minimization of it leads to estimates which
are well adapted to Dn but which are not suitable to predict new data independent from Dn . This
can be avoided by splitting the data Dn into two parts, learning and testing data, by computing
the kernel estimate with the learning data and by choosing the bandwidth such that the empirical
risk on the testing data is minimal.
The resulting bandwidth depends on the way the data is splitted. This is a drawback if one
wants to use this bandwidth for a kernel estimate which uses the whole data Dn (and thus has
nothing to do with the way the data is splitted). It can be avoided by repeating this procedure for
several (e.g. randomly chosen) splits of the sample and by choosing the bandwidth such that the
average empirical L 2 risk on the testing data is minimal.
For k-fold cross-validation the splits are chosen in a special deterministic way. Let 1 - k - n.
For notational simplicity we assume that n
k is an integer. Divide the data into k groups of equal
A statistical approach to case based reasoning 7
size n
k and denote the set consisting of all groups except the lth one by D n;l :
For each data set D n;l and bandwidth h 2 R d
construct a kernel estimate mn\Gamma n
;h (x; D n;l
k l+1;:::;ng K
k l+1;:::;ng K
Choose the bandwidth such thatk
l=1n
l
is minimal and use this bandwidth h   as bandwidth hn for the kernel estimate (1).
n-fold cross-validation is denoted as cross-validation. In this case D n;l is the whole sample
is minimized with respect to h.
2.3. Curse of dimensionality
If d is large, then estimating a regression function is especially difficult. The reason for this is
that in this case it is in general not possible to densely pack the space of X with finitely many
sample points, even if the sample size n is very large. This fact is often referred to as curse of
dimensionality, a phrase which is due to Bellman (1961). It will be illustrated by an example;
further examples can be found in Friedman (1994).
independent identically distributed R d -valued random variables with X
uniformly distributed in the hypercube [0; 1] d . Consider the expected supremum norm distance of
X to its closest neighbor in X 1
ae
min
oe
(where k(x
For
ae
min
oe
8 J. Dippon et al.
thus
Z 1P
ae
min
oe
For instance, d1 (10; 1000) - 0:22 and d1 (20; 10000) - 0:28. Thus, for d large, even for a large
sample size n, the supremum norm distance of X to its closest neighbor in the sample is not close
to zero (observe that the supremum norm distance between any two points in the above sample is
always less than or equal to one).
3. Basic idea
In this section we will apply the concepts of nonparametric regression introduced in the previous
section to case based reasoning. To do this, we will consider the prediction of the final result in
case based reasoning as a regression estimation problem. Hence we assume that the final result
is a real valued random variable and that the observable variables are components of a R d -valued
random variable In applications often some of the observable variables are
categorical random variables. How to handle such variables will be described in Section 5 below.
Recall that our aim in case based reasoning is to determine those cases of a given list which
"most closely" match a new case. This can be done by defining a distance function
which determines a distance d(x; x i ) between two cases with observable variables x and x i . Given
such a distance function one can define the k "most similar" cases to a new case as those k cases
among whose distances to the new case x belongs to the k smallest occuring distances.
The basic idea to determine such a distance function is to define a regression estimate depending
on this distance function and then to choose that distance function which minimizes (an estimate)
of the L 2 risk of the resulting regression estimate.
As regression estimates we use kernel estimates introduced in the previous section. For
define a distance function dh : R d \Theta R d ! R+ by
dh
=@ d
A
A statistical approach to case based reasoning 9
Then the kernel estimate can be written as
so the family of distance functions
\Psi corresponds to a family of regresion estimates
is the kernel function. In our simulations in Section 4 and
in our application in Section 5 we use the Gaussian kernel
Our aim is to choose the distance function such that the corresponding regression estimate has
minimal L 2 risk. As already explained in the previous section, this is not possible because the L 2
risk depends on the unknown distribution of (X; Y ). What we do instead is to estimate the L 2 risk
by cross-validation and to choose the distance function dh   (h   2 R d
which the estimated L 2
risk of m n;h   is minimal.
Our method can be summarized as follows: Estimate the L 2 risk of a kernel estimate m n;h by
cross-validation, determine h   2 R d
such that this estimated L 2 risk is minimal (compared to all
other choices h 2 R d
use the corresponding distance function dh   .
3.1. Subset selection
We have seen in the previous section that estimating the regression function is very difficult if the
dimension d of X is large. This problem occurs for every estimate, hence for the kernel estimate
which we use in this paper as well. The only way to handle this problem is to make assumptions on
the underlying distibution (e.g. to assume that the regression function is additive, cf. Stone (1985),
Stone (1994). In this paper we assume that, even if d is large (say 20), the regression
function mainly depends on a few (say 3 to 6) of the components of X. If this assumption holds,
then a distance function can be determined by applying our method to this small subset of the
observable variables (which leads to a regression estimation problem with small dimension of X).
Of course, in applications we will not know on which of the observable variables the regression
function mainly depends. In order to check this we consider all (or if d is too big all small) subsets
of the observable variables, apply our method to each of these subsets and choose that subset for
which the via cross-validation estimated L 2 risk of the "optimal" estimate is minimal.
J. Dippon et al.
3.2. Simplification of computation
For each subset of the observable variables and for each variable of this subset the method described
above requires computation of a vector of scaling factors which leads to a multivariate minimization
problem. In order to avoid solving many such minimization problems, we use the following
simplification: In a first step determine for each of the d observable variables a univariate scaling
n by computing (as described above) the "optimal" bandwidth of a univariate kernel estimate
fitting the data (X (i)
This yields scaling factors h (1)
In a second step we choose for each subset fi
d g of dg the distance function
h@
d
on R d , such that the (via cross-validation) estimated L 2 risk of
is minimal with respect to h 2 R+ . Finally we choose that subset fi
d g of
which the (via cross-validation) estimated L 2 risk of the corresponding optimal kernel estimate is
minimal.
Observe that minimization of the estimated L 2 risk of m n;h (h 2 R+ ) is only a univariate
minimization problem.
3.3. Robustification of the bandwidth estimation
It is well known that bandwidth selection by cross-validation is often highly variable (see Simonoff
(1996)). Some authors suggested alternatives such as plug-in methods and claimed that these are
superior to ordinary cross-validation. But Loader argues that no theoretical results support these
claims and that the plug-in methods itself depends sensitively on a pilot estimate of the bandwidth
(see Ch. 10 in Loader (1999)).
Let m n;h be a kernel estimate with bandwith h. In order to robustify the cross-validation
bandwidth selection given by
A statistical approach to case based reasoning 11
with CV function cv(h) as defined in (4), we suggest the following heuristic rule:
where
and
is specified below.
The idea behind the rule is the following. Often the graph of the CV function resembles a
valley with possibly several local minima at the bottom or with a flat bottom where the position of
the global minimum seems to be accidental. Then an estimate of the middle of the valley appears
to be a less variable measure.
Choose two parameters We define a level r p;q in dependence of
For the regression estimate equals the mean of the responses fY
coincides with the empirical variance 1
Yng. In
our simulations we first used
but for this choice it might happen that r p;q ? sup h?h0 cv(h). In order to avoid this we set
r p;q := cv(h
For values of p and q we used
in this paper. Our robustification implies that we possibly dispense with explaining a fraction
of the empirical variance of fY g. This doesn't seem to be too harmful,
since we are more interested in finding similarity neighborhoods than in prediction.
J. Dippon et al.
4. Application to Simulated Data
4.1. Simulated Model
Let us consider the defined by
where a 2 (0; 1] is a fixed constant, and the model
with independent random variables X - U ([0; 1). Apparently, the third component
X (3) has no influence on Y . If a is small, the first component X (1) of the random vector X
has a larger influence on the response Y than the second component X (2) . In other words, for a
small constant a the regression function m is almost constant with respect to X (2) ; in any case m
is constant with respect to X (3) (Figure 1).
We simulate realizations of (X independent copies
of (X; Y ). Then, as described in Subsections 3.2 and 3.3, for each i 2 f1; 2; 3g a robustified
estimate e h (i)
0 of the "optimal" bandwidth h (i)
0 of the univariate kernel estimator based on the sample
computed by cross-validation. If a is significantly less than 1, then we
expect the relation
1. Furthermore, the corresponding estimated L 2
risks should be in ascending order, too (Figure 2). For
Our model selection procedure defines different distance functions on [0; 1] 3 as given
by (5). For each distance function we compute the minimal estimated L 2 risk of the multivariate
kernel estimator (6) with respect to h ? 0 and consider that distance function as best for which
the estimated L 2 risk is minimal. If inclusion of a further component reduces the estimated L 2
risk only "slightly", the distance function with the smaller set of components but slightly larger
corresponding estimated L 2 risk will be preferred.
We use the resulting distance functions to define neighborhoods
A statistical approach to case based reasoning 130.20.61
Component 1 of X0.20.61
Component 2 of X
Fig. 1. Graph of the regression function m for (considered as function of the first two components of
the predictor variable X) and scatter plot of simulated realizations of (X
For each ffi ? 0 we consider every point in N (x; ffi ) to be more similar to x than every point outside
of N (x; ffi ). In this sense those realizations of X are the k most similar ones to x whose
distances belongs to the k smallest among
4.2. Results of the Simulations
For each pair (a; n) of parameters a 2 f0:3; 0:5; 1g and n 2 f100; 200g the simulation is repeated
20 times but with different seeds of the generator producing the (pseudo) random numbers. Below
we discuss the results of the simulations performed with parameter set (a; n) = (0:5; 100) in some
detail.
As suggested in Section 3 we compute bandwidths e h (1)
0 . In all but one of the
twenty runs the procedure found
0 . In most cases the CV curves and the
univariate regression estimates corresponding to the minimal value of the CV curve look as in
14 J. Dippon et al.

Figure

2. Furthermore, the optimal value h (i)and the robustified value e h (i)differ only slightly.
However, in some cases the situation appears as in Figure 3. There the smooths related to e h (i)seem to be appropriate. Only in one of the twenty runs the method fails to detect that the
first component has a stronger influence on the function values of m than the second component,
see

Figure

4.
To compare multivariate kernel estimates (6) with distance functions selecting different subsets
of prediction variables we compute the minimum cv(h 0 of the estimate cv(h)
(given by (4)) of the L 2 risk of (6) and compare the ratios cv(h 0 )=cv(1). Table 1 shows that
for each run the subset selection procedure favors fX (1) ; X (2) g or fX (1) g. But in cases
is preferred the gain is too small to be significant. Hence in all 20 runs
we choose subset fX (1) ; X (2) g.
The relation of e h (1)
determines the geometry of the neighborhoods which will be used
to characterize "similar cases". For the parameter pair (a; n) = (0:5; 100) the ratios of the the
computed bandwidths e h (1)
turned out to be
2, 0.16, 0.54, 0.45, 0.60, 0.50, 0.23, 0.57, 0.34, 0.76
From the second row of Table 2 one can extract minimum, maximum, median and interquartile
range of this ratios.
To visualize the variability of the resulting distance functions neighborhoods
projected on the first two components are plotted around center 0:5). The parameter
chosen in such a way that the area of these sets equals 1=10, see Figure 5.
Comparing the results for sample sizes indicates that larger sample sizes
lead to less variable neighborhoods.
A statistical approach to case based reasoning 15
y
Component 2
y
y
Fig. 2. Determination of univariate bandwidths e h (1); e h (2); e h (3)for the 6th simulation run. Left column: Cross-validated
L2 risk of the regression estimate of E(Y jX (i) ) depending on the bandwidth h (i) . The optimal
bandwidth h (j)and the robustified bandwidth e h (j)are indicated by tick marks (if within the range of the x-
axis). Right column: Univariate kernel estimate of E(Y jX (i) ) using the optimal bandwidth h (i)(dotted) and
the robustified bandwidth e h (i)(solid), i 2 f1; 2; 3g. The finding that the curves related to h (i)and e h (i)can be
hardly distinguished is true for most of the simulated samples.
J. Dippon et al.
Component 1
y
Component 2
y
Component 3
y
Fig. 3. Determination of univariate bandwidths e h (1); e h (2); e h (3)for the 3th simulation run. Left column: Cross-validated
L2 risk of the regression estimate of E(Y jX (i) ) depending on the bandwidth h (i) . The optimal
bandwidth h (j)and the robustified bandwidth e h (j)are indicated by tick marks (if within the range of the x-
axis). Right column: Univariate kernel estimate of E(Y jX (i) ) using the optimal bandwidth h (i)(dotted) and
the robustified bandwidth e h (i)(solid), i 2 f1; 2; 3g. Despite the fact that the bandwidths h (i)minimizes the
CV error criterion, these bandwidths are not useful to compare the dependency of the random variable Y on
. For this sample the robustified bandwidth e h (i)seems to be more appropriate.
A statistical approach to case based reasoning 17
Component 1
y
Component 2
y
y
Fig. 4. Determination of univariate bandwidths e h (1); e h (2); e h (3)for the 11th simulation run. Left column:
Cross-validated L2 risk of the regression estimate of E(Y jX (i) ) depending on the bandwidth h (i) . The
optimal bandwidth h (j)and the robustified bandwidth e h (j)are indicated by tick marks (if within the range of
the x-axis). Right column: Univariate kernel estimate of E(Y jX (i) ) using the optimal bandwidth h (i)(dotted)
and the robustified bandwidth e h (i)(solid), i 2 f1; 2; 3g. This is the only of all 20 samples for which the
suggested method fails to detect that the first component has a stronger influence on the function values of
m than the second component.
J. Dippon et al.

Table

1. Ratio cv(h0)=cv(1) of estimated L2 risk of the multivariate
kernel estimate with the specified set of components. Parameters of
the simulated model were a
Run Selected Subset
9 0.40 0.97 1.00 0.19 0.43 0.98 0.23
A statistical approach to case based reasoning 19

Table

2. Statistics of the ratios e h (1)= e h (2)each computed from 20 simulation runs.
Parameters Statistics of e h (1)= e h (2)a n Min. 1st Quart. Median Mean 3rd Quart. Max.
1.0 200 0.14 0.93 1.10 1.02 1.16 1.43
Component 1 of X
Componentof
Component 1 of X
Componentof
Component 1 of X
Componentof
Component 1 of X
Componentof
Component 1 of X
Componentof
Component 1 of X
Componentof
Fig. 5. These plot show level lines of the regression function m for parameter a 2 f0:3; 0:5; 1g and (cutted)
ellipsoidal neighborhoods around the point (0:5; 0:5). Each neighborhood set is computed from one of the
simulated samples of size contains 1/10th of the unit square.
J. Dippon et al.
5. Application to Breast Cancer Data
We compute a distance function on the space of covariates as suggested in Section 3 to determine
for a given new breast cancer patient (with unknown survival time) "similar" cases among patients
in a database with known (censored) survival time. The data were made available by the Robert-
Bosch-Krankenhaus in Stuttgart, Germany. They are collected between the years 1987 and 1991
with a follow-up of 80 months. Each of the cases is described by a 10-dimensional
parameter vector. We consider the nine first parameters as predictor variables. It includes age
at diagnosis AGE (in years), menopause status MS (which equals 1 and 2 for pre and post
menopause, respectively), histological type of the breast cancer HT (with values in
number of affected lymph nodes PN (grouped into four classes with values in
size PT (grouped into four classes occurence of metastases PM (with values 0 and 1
for no and yes, respectively), grading of tumor GR (with values in f1; 2; 3g), estrogene status ES
(with values 1 and 2 for positive and negative, resp.) and progesterone status PS (with values 1
and 2 for positive and negative, resp.
The last component of the parameter vector describes the observed (censored) survival time
OST (in years) and is considered as reponse variable. In many cases the actual survival time
can't be observed, since the patient is still alive after the end of the study or because of the
patient's withdrawal from the study. Hence the observed survival time can be understood as the
minimum of the actual survival time T (time elapsed from date of diagnosis to date of death) and
a censoring time C (time between date of diagnosis and a date at which the patient is known to be
alive). In our approach we estimated the censored survival time instead of the more complicated
and often unknown uncensored survival time in order to simplify the problem. We hope that
this simplification doesn't affect our result too much, because we used the estimate to construct
similarity neighborhoods rather than to estimate the survival time as a function of a covariate
vector (as to the latter compare Carbonez et al. (1995)).
AGE and OST are continuously distributed r.v.'s, PT , PN , GR are ordered categorical r.v.'s,
and MS, HT , PM , ES and PS are nominal r.v.'s. Values in the predictor variables are allowed
A statistical approach to case based reasoning 21

Table

3. Robustified bandwidths e h (j)for the the univariate regression problems.
The products 1= e h (j)times the spread of component j allow to compare the maximal
influence of component j on the distance function.
component AGE MS HT PT PN PM GR ES PS
to be missing.
Assume that is the covariate vector of a new patient and z = (z
is the covariate of a patient in the database fz related
to a continuous or ordered categorical r.v., we define the distance function d (j)
h by
d (j)
z (j) are not missing
k and z (j)
l are not missingg otherwise
is related to a nominal r.v., we define the distance function d (j) by
d (j)
z (j) and both x (j) and z (j) are not missing
z (j) or x (j) or z (j) is missing
As described in Section 3, for each j 2 we compute by cross-validation "optimal"
0 for the univariate regression estimates
n;h
where y i is a realization of the response variable Y i . Figures 6 and 7 show the CV function cv and
the regression estimates related to the robustified estimate e h (j)
0 as displayed in Table 3.
Now, on the range X of the covariate variable X we define for each l 2 for each
22 J. Dippon et al.
50 100 150 20011.82AGE
Observed
Survival
Time
Observed
Survival
Time
1.0 1.2 1.4 1.6 1.8 2.02610
12.6 .
Observed
Survival
Time
Observed
Survival
Time
PN
Observed
Survival
Time
A statistical approach to case based reasoning 23
Observed
Survival
Time
Observed
Survival
Time
Observed
Survival
Time
1.0 1.2 1.4 1.6 1.8 2.02610h
PS
Observed
Survival
Time
J. Dippon et al.

Table

4. For each number l 2 of covariates that subset
is displayed which possesses the smallest estimated L2 risk.
l Best subset cv(h l
6 AGE, PT, PN, PM, ES, PS 0.7894
7 AGE, HT, PT, PN, PM, ES, PS 0.7894
8 AGE, MS, HT, PT, PN, PM, ES, PS 0.7932
9 AGE, MS, HT, PT, PN, PM, GR, ES, PS 0.8281
subset dg the distance function d
where h is a fixed positive number, and we obtain the multivariate regression estimate
compute its CV function cv as a function of h ? 0, and determine that subset J l :=
with smallest global minimum cv(h l ) of the related CV function. Table 4 shows the results of this
model selection step.
Together with Table 4 the scree plot in Figure 8 suggests to choose that distance function which
includes the variables PN , PM , ES. By adding a fourth component the relative improvement of
the ratio cv(h 4 )=cv(1) compared to cv(h 3 )=cv(1) is less than 0:05. Hence, we propose to use the
distance d, defined by
d (PN)
0:71
d (PM)
1:42
d (ES)
0:79
Now, for a given new case with covariate x compute r i := d(x; z i
the z 0
according to increasing values of the r 0
s: fz g. Then, for fixed k 2
A statistical approach to case based reasoning 25
Number of Components
Fig. 8. Scree plot of (l; cv(h l
9g.
the subset fz of the database consists of that k cases which are "most similar" to the
new case with respect to the distance d. Their history logs may help the physician in choosing an
appropriate therapy for the new patient.
6. Discussion
As it is often the case with nonparametric estimators in a multivariate setting, the suggested
method has its deficiencies and limitations, too. For instance, assume that the random vector
26 J. Dippon et al.
takes on the values (0; 0), (1; 0), (0; 1) and (1; 1) each with probability 1=4, and let
Y be defined by 0, if x 2 f(0; 0); (1; 1)g, and by 1 otherwise. Then E(Y jX (1)
but m is far from being a constant. Hence, in this case, the proposed method will fail.
Adjusting the so-called resubstitution estimate of the L 2 risk by penalizing functions leads
to various other bandwidth selection procedures such as generalized cross-validation, Shibata's
model selector, Akaike's information criterion, Akaike's finite prediction error and Rice's T (see,
e.g., H-ardle (1990), H-ardle (1991). These may be used instead of the least squares cross-validation
criterion.
Further research should improve the regression estimate. For instance, dimension reduction as
handled in projection pursuit regression is adapted to find structures in linear subspaces of the
covariate space. Additionally, the fact of censored observation times should be taken into account.
In our application concerning survival times of breast cancer patients the influence of the chosen
therapy was ignored. This might lead to an underestimation of the influence of known predictor
variables. The reason is that the choice of the therapy usually depends on some of the predictor
variables and that the therapy has an influence on the survival time. There isn't a problem, if one
has data for which the choice of the therapy is independent of the predictor variables. Such data
does not exist for the predictors we used. But this requirement is fulfilled for any new predictor
which was unknown in the past or was considered to be unimportant and therefore was not used
for choosing the therapy. A distance function which is additionally based on such a new predictor
allows to judge the importance of the new predictor.
7.

Acknowledgements

This research was partly supported by the Robert Bosch Foundation, Stuttgart. We are grateful
to Prof. H. Walk, Stuttgart, for stimulating discussions.
Readers wishing to obtain the breast cancer data or the S-Plus code of the algorithms should
contact the authors.
A statistical approach to case based reasoning 27



--R

Adaptive Control Processes.

An overview of predictive learning and function approximation.
Theory and Pattern Recognition Applications.
Applied Nonparametric Regression.
Smoothing Techniques with Implementations in S.
Local Regression and Likelihood.



On estimating regression.
Smoothing Methods in Statistics.
Additive regression and other nonparametric models.
The use of polynomial splines and their tensor products in multivariate function estimation.
Smooth regression analysis.
--TR
Random approximations to some measures of accuracy in nonparametric curve estimation
