--T
Algorithms for Model-Based Gaussian Hierarchical Clustering.
--A
Agglomerative hierarchical clustering methods based on Gaussian probability models have recently shown promise in a variety of applications. In this approach, a maximum-likelihood pair of clusters is chosen for merging at each stage. Unlike classical methods, model-based methods reduce to a recurrence relation only in the simplest case, which corresponds to the classical sum of squares method. We show how the structure of the Gaussian model can be exploited to yield efficient algorithms for agglomerative hierarchical clustering.
--B
Introduction
Multivariate Gaussian models have been proposed for quite some time as a basis for clustering
algorithms. Recently, methods of this type have shown promise in a number of practical
applications [9]. Examples in the geophysical sciences include seismic data processing, in the
biological sciences classification of cell types based on chemical responses, and in the social
sciences classification based on attachment theory in psychology. They have also been used
for clustering various types of industrial and financial data. Image-processing applications
include unsupervised texture image segmentation, tissue classification in biomedical images,
identification of objects in astronomy, analysis of images from molecular spectroscopy, and
recognition and classification of surface defects in manufactured products.
Agglomerative hierarchical clustering (Murtagh and Raftery [8], Banfield and Raftery
[1]), the EM algorithm and related iterative techniques (Celeux and Govaert [3]) or some
combination of these (Dasgupta and Raftery [4]) are effective computational techniques for
obtaining partitions from these models. The subject of efficient computation in this context
has however received little attention. We aim to fill this gap in the case of agglomerative
hierarchical clustering. Although no iterative computation is involved, the issue of efficiency
is nevertheless important since the practical value of these methods is limited by a growth
in time complexity which is at least quadratic in the number of observations. This paper is
organized as follows: The remainder of this section gives the necessary background in model-based
clustering and hierarchical agglomeration. In Section 2, we propose computational
techniques for each of the four simplest and most common Gaussian models, and compare
the performance of each method to an appropriate benchmark. Finally, extension to more
complex Gaussian models is discussed in Section 3.
1.1 Model-Based Cluster Analysis
The relevant probability model is as follows: the population of interest consists of G different
subpopulations; the density of a p-dimensional observation x from the kth subpopulation
is f k (x; ') for some unknown vector of parameters '. Given observations
denote the identifying labels for the classification, where
comes from the kth subpopulation. In the classification likelihood approach to clustering,
parameters ' and labels fl are chosen so as to maximize the likelihood
G
Y
Our focus is on the case where f k (x; ') is multivariate normal (Gaussian) with mean vector
- k and variance matrix \Sigma k . The overall approach is much more general and is not restricted
to multivariate normal distributions [1]. However, experience to date suggests that clustering
based on the multivariate normal distribution is useful in a great many situations of interest
([8], [1], [9], [3],
When f k (x; ') is multivariate normal, the likelihood (1) has the form
G
Y
Y
(2)
where I is the set of indices corresponding to observations belonging
to the k-th group. Replacing - k in (2) with its maximum likelihood estimator -
is the number of elements in I k , yields the concentrated
log-likelihood
in which W
is the sample cross-product matrix for the kth
group.
If then the log-likelihood (3) is maximized by classifications fl that minimize
tr
. This is the well-known sum of squares criterion which, for example, was
suggested by Ward [11] as a possible metric when he proposed the agglomerative hierarchical
method for clustering. An alternative that allows a different variance for each group is
k I; fl is chosen so as to minimize
[1]. If \Sigma k is the same for all
groups but otherwise has no structural constraints, then values of fl that minimize
maximize the log-likelihood [5]. When \Sigma k is allowed to vary completely between groups, the
log-likelihood is maximized whenever fl minimizes
the equivalent criteria to be minimized corresponding to these four parameterizations of \Sigma k .
criterion
I
G
tr
'-
criterion
G
G

Table

1: Four parameterizations of the covariance matrix \Sigma k in the Gaussian model with the
corresponding criteria to be minimized.
1.2 Hierarchical Agglomeration
Agglomerative hierarchical clustering (Ward [11]) is a stagewise procedure in which 'opti-
mal' pairs of clusters are successively merged. Each stage of merging corresponds to a unique
number of clusters, and a unique partition of the data. Classifications differ according to
the criterion for optimality, and the strategy for choosing a single pair when more than one
is optimal. In model-based hierarchical clustering, a maximum-likelihood pair is merged
at each stage. Although the resulting partitions are suboptimal, agglomerative hierarchical
clustering methods are in common use because they often yield reasonable results and are
relatively easy to compute. For model-based clustering, another advantage of hierarchical
agglomeration is that there is an associated Bayesian criterion for choosing the best partition
(hence the optimal number of clusters) from among those defined by the hierarchy
[1]. Hierarchical clustering can be accomplished by splitting rather than agglomeration, but
the complexity of such algorithms is combinatorial unless severe restrictions on the allowed
subdivisions are applicable.
The process of hierarchical agglomeration is usually assumed to start with each observation
in a cluster by itself, and proceed until all observations are in a single cluster. However
it could just as well be started from a given partition and proceed from there to form larger
clusters. The value returned consists of a 'classification tree' (a list of pairs of clusters
merged), and possibly the optimal value of the change in criterion at each stage.
In classical agglomerative methods (e. g. sum of squares, nearest and farthest neighbor
(single and complete link) [7]), there is a metric or 'cost' based on geometric considerations
associated with merging a pair of clusters. For a particular pair this cost remains fixed as
long as neither of the clusters in that pair is involved in a merge, so that the time complexity
of hierarchical agglomeration can be significantly reduced if the cost of merging pairs is
retained and updated during the course of the algorithm. The overall memory usage is then
proportional to the square of the initial number of clusters (usually just the initial number of
observations), which could be a severe limitation. For large data sets, one possible strategy
is to apply hierarchical agglomeration to a subset of the data and partition the remaining
observations via supervised classification or discriminant analysis. Banfield and Raftery [1]
used only 522 out of 26,000 pixels in an initial hierarchical phase to successfully classify
tissue from an MRI brain-scan image via Gaussian model-based techniques.
For each classical method, there is a simple recurrence relation for updating the cost of
merging pairs. In the sum-of-squares method, this recurrence is
where \Delta(i; j) is the cost of merging groups i and j, and hi; ji represents the group formed by
merging groups i and j. Once the initial cost of merging each pair is obtained, computation
can proceed without further reference to the data; the size of each group must be retained
and updated.
The amount of space needed for \Delta(i; decreases as the number of groups increases.
A memory-efficient scheme for maintaining \Delta(i; j) is as follows. Assume (without loss of
generality) that observation number k is the observation of smallest index in group k in the
initial classification. If for each group j ? 1 values of \Delta(j; i) are stored for all i ! j, then it is
easy to recover space during the course of the computation. Assuming that j is the highest
index in a particular merge, and l is the largest current index, the space associated with
group j can be used for group l, thereby freeing the (larger) space associated with group
l. The original indexes for the groups can easily be recovered at the end. In programming
languages such as Fortran 77 in which memory allocation is static, values of \Delta(j; i) can be
stored sequentially in the order \Delta(2; 1); \Delta(3; 1); \Delta(3; 2); \Delta(4; 1); \Delta(4; 2); \Delta(4; so that
the scheme described above leaves contiguous free space that can be used for the classification
tree and other return values. In languages such as C that allow dynamic memory allocation,
a separate list of values \Delta(j; i) for all can be maintained for each j; the space associated
with the list for the largest value of j can be freed at each stage under this scheme.
Model-based methods generally require more computational resources than classical meth-
ods. In some there is no advantage in storing the cost of merging pairs, and some require
relatively expensive computations such as determinants of the cross-product matrices. The
object of this paper is to show that there are relatively efficient methods for agglomerative
hierachical clustering based on Gaussian models.
Efficient Algorithms for the Four Basic Models
There is clearly structure to be exploited in the various criteria, in which W k is a symmetric,
positive semidefinite matrix (see Table 1 in Section 1.1). Moreover, since only two groups
are merged at each stage of hierarchical agglomeration, there should be a close relationship
between criteria at successive stages. In fact, the sample cross-product matrix for the
merged group can be obtained from the sum of the sample cross-product matrices of its two
component groups by means of a symmetric rank-1 update :
where
where s k denotes the sum of the observations for group k. A derivation is given in the


Appendix

. In the remainder of this section we show that this relation leads to efficient
algorithms for all of the methods of Table 1.
We assume that the input consists of an n \Theta p matrix whose rows correspond to individual
observations and a vector of length n indicating the initial classification of each observation.
2.1 \Sigma k
When the covariance matrix is constrained to be diagonal and uniform across all groups, the
criterion to be minimized at each stage is
tr
G
tr
This is the sum-of-squares criterion, long known as a heuristic before any relationship to the
Gaussian model was recognized: tr (W k ) is the sum of squares of observations in group k with
the group mean subtracted out. Of the classical methods, it is the only one known to have an
underlying statistical model. In view of the recurrence relation (4), all that is required to start
the hierarchical clustering procedure is a set of values \Delta(i; j) and the number of observations
in each group. First, the value of \Delta(i; for each pair of observations can be computed; in
the absence of other information, the individual observations usually constitute the initial
partition of the data, and nothing further need be done. For coarser initial partitions,
the recurrence relation could be used to obtain the initial values for hierarchical clustering
given \Delta(i; for each pair of observations. Merges for initialization are determined by the
given partition rather than by the minimum value of \Delta(i; j) at any stage. The process just
described, however, requires storage proportional to the square of the number observations
n, which is undesirable if there are m ! n groups to begin with. The update formula (5),
leads to a better initialization procedure for \Delta(i; j), since
tr
Because we are assuming that k is smallest index associated with observations in group k,
we can overwrite the kth observation by the sum s k of observations in that group and the
kth element of the classification vector with n k . This can be accomplished in O(np) time,
and requires no additional storage since the input is overwritten. Then (8) and (6) can be
used to initialize \Delta(i; j). The total storage required would then be O(np +m
the input, and O(m 2 ) for \Delta(i; j).
2.2 \Sigma k
I
When the convariance of each group is constrained to be diagonal, but otherwise allowed to
vary between groups, the criterion to be minimized at each stage is
G
G
tr
As for the sum-of-squares and the other classical methods, \Delta(i; remains unchanged from
stage to stage unless either group i or group j is involved in a merge, so that storing \Delta(i;
results in a gain in time efficiency for hierarchical clustering. From the update formula (5),
we have
tr
tr
#)
Unlike the classical methods, there is no simple recurrence relation for \Delta(hi; ji;
\Delta(i; j), \Delta(i; k), and \Delta(j; k). However, there is a reasonably efficient update; all that is
required is to maintain values of n k , tr in addition to s k . The
vector s k can overwrite the kth observation, as was done for the sum of squares. But this
time for each group k that has more than one element, the index k 0 of the next observation in
that group is stored in the kth element of the classification vector. The number of elements
in the group is stored in the k 0 th entry of the classication vector, while the trace of the
sample cross product matrix and the corresponding term of the criterion overwrite the first
two elements of the k 0 th observation. With this scheme no additional storage is necessary
(p - 2) beyond that required for \Delta(i; j) and the input.
The issue of terms in which tr (W k remains to be resolved; this includes those terms
corresponding to groups consisting of a single observation, as well as those groups in which
all observations coincide. Hence the first stages of hierachical clustering will be arbitrary
without some sort of initialization procedure. We replace (9) with a modified criterion in
order to handle these cases transparently :
G
tr
tr (W)
where W is the sample cross product matrix for the group consisting of all observations, and
- has the default value of 1. The factor tr (W) =np is an attempt to take into account scaling
in the data, since the resulting criterion is scale dependent.
2.3 Constant \Sigma k
When the covariance matrix is uniform across all groups but otherwise has no structural
constraints, the criterion to be minimized at each stage is
G
In contrast to the methods discussed up to this point, the change in criterion caused by
merging two groups is affected by merges among other groups in the current classification.
Hence it is of no advantage here to store \Delta(i; j) for the duration of the computation. Nev-
ertheless, (5) leads to an efficient update, because the change in
are merged can be represented as
Instead of W itself, we maintain a lower triangular Cholesky factor L for W (see e. g. [6]),
since then the determinant can be easily computed as the square of the product of the
diagonals of L :
diag(L)
Noting that since when each observation is in a group by itself, we
can then either build the Cholesky factor for a coarser initial partition, or else merge optimal
groups in hierarchical agglomeration as follows:
\Theta \Theta \Theta \Theta
\Theta \Theta \Theta
\Theta \Theta \Theta \ThetaC C C C C A
Givens
~
\Theta ~
\Theta ~
\Theta ~
\Theta
\Theta \Theta \Theta
\Theta ~
\Theta ~
Givens
\Theta \Theta \Theta \Theta
\Theta ~
\Theta ~
\Theta
\Theta ~
Givens
\Theta \Theta \Theta \Theta
\Theta \Theta \Theta
\Theta ~
\Theta
Givens
\Theta \Theta \Theta \Theta
\Theta \Theta \Theta
\Theta
The symbol Givens \Gamma! stands for application of a Givens rotation, an elementary orthogonal
transformation that allows selective and numerically stable introduction of zero elements
in a matrix. The marked entries are values changed in the last transformation. The time
efficiency for the Cholesky update is O(p 2 ), in contrast to O(p 3 ) for forming a new Cholesky
factor from the updated p \Theta p matrix W . For details of the Cholesky update via Givens
rotations, see e. g. [6].
Although the criterion is defined for all possible partitions, there remains a problem with
initialization: has rank less than p. In particular, the first stages of
hierachical clustering using this criterion will be arbitrary if initially each observation is in
a cluster by itself, since merging any pair of observations i and j will result in
and
are singletons (p - 2). To circumvent this, we use the
sum-of-squares criterion tr (W ) to determine merges until the value of jW j is positive, while
maintaining the Cholesky factor of W . As the computation proceeds, s k overwrites the
data and n k overwrites the classification vector; most of the information needed to recover
the classification tree and optimal values of the criterion can be stored in the portions of
these structures that are no longer needed in the algorithm. Other than the necessary O(p 2 )
storage for maintaining L T , additional storage of size O(M ), where M is the number of
stages, is needed when p ! 4 to store the merge indexes in order to completely reconstruct
the classification tree.
2.4 Unconstrained \Sigma k
When the covariance matrix is allowed to vary completely between groups, the criterion to
be minimized at each stage is
G
Like the criteria discussed in sections 2.1 and 2.2, each group contributes a separate additive
term to (13), so that it is time efficient to save values of \Delta(i; j). If L k denotes the Cholesky
factor of W k , then, in view of (5),
L hi;ji can be computed efficiently from L i , L j and w ij by applying Givens rotations (see
Section 2.3) to the composite matrix
The composite matrix is never explicitly formed; instead, w ij and each row of L T
are treated
as separate updates to L T
. Although the time efficiency to form the updated Cholesky factor
is of the same order of magnitude as that for forming a new Cholesky factor from the updated
W hi;ji , the use of (14) has an advantage in storage efficiency. Maintaining the upper triangle
of each W k and updating directly via (5) would require more storage since W k has p rows
regardless of n k , whereas L k has at most min(n
can overwrite the data and the entries corresponding to the
lower triangle of L T
k can be used for the necessary pointers and values to be updated (the
kth term in (13)). Besides what is required for the data and for \Delta(i; j), additional O(p 2 )
storage is needed for the Cholesky factors when updating \Delta(i; j).
Finally, because jW k there is even greater ambiguity with criterion
than with either (11) or (9). For this reason, we use
G
tr (W)
in place of (13), which gives a hybrid between the modified criterion for \Sigma
I (10) and
(13). The default value for both - 1 and - 2 is 1.
2.5 Benchmark Comparisons
In this section we compare the algorithms developed in Sections 2.1-2.4 with approaches that
do not use the update formula (5) but are otherwise efficient. The 'benchmark' algorithms
leave the data intact, and keep track of the composition of all groups in the classification
vector used to transmit the initial partition. For \Sigma I and \Sigma
updated
after a merge by first forming the column means for the combined group, then forming the
sum of squares for all of observations in both groups with the column mean subtracted out.
For constant \Sigma k , the upper triangle of
after a merge is updated using the
first part of (12)
. Column sums for W i and W j are computed
from the list of observations in each group, and these are added to form the column sum
for W hi;ji . Then the upper triangle of W
is formed using symmetric
rank one operations. The quantity jW j for the merge is then computed from its Cholesky
decomposition as described in Section (2.3). For unconstrained \Sigma k , the upper triangle of
W hi;ji is formed from scratch after a merge involving i or j, and the Cholesky decomposition
is used to obtain its determinant.
In addition to the O(p 2 ) storage used for the sample cross-product matrices, O(n) storage
is allocated in the benchmarks for the return values, for the number of groups in each cluster,
and, to facilitate updating \Delta(i; j), for the term contributed by each cluster to the criterion.
The methods of Section 3 use considerably less storage: constant variance requires O(n)
additional storage to recover results, and constant and unconstrained variance require O(p 2 )
additional storage for maintaining Cholesky factors.

Figure

shows marked gains in time efficiency for the methods of Section 2 over the
benchmarks. Randomly generated observations of dimension used, with the
default initial partition in which each singleton observation constitutes a cluster. The basic
methods were written in Fortran with an S-Plus interface, and the time shown is the average
over nine different data sets using S-Plus version 3.3 1 on a Silicon Graphics Iris workstation
under the IRIX 5.2 operating system. The solid line represents the performance of algorithms
based on the update formula (5), while the dashed line represents performance of algorithms
in which the necessary quantities are obtained without updating. The effect is most dramatic
in the unconstrained case, where extrapolated results (ignoring effects of increased memory
usage) show an improvement by a factor of around 15 for as compared to a factor
of around 4 for 500. The results for \Sigma are also a point of comparison, since in
that case the solid line represents that classical sum of squares approach via the well-known
recurrence relation (4). Note that the time scale for the constant-variance method differs
from that of the other methods, which use more memory in exchange for improved time
efficiency.
3 Extension to more Complex Gaussian Models
Banfield and Raftery [1] developed a model-based framework that subsumes all of the parameterizations
in Table 1. The resulting clustering methods include some criteria that are
more general than \Sigma I or constant \Sigma k , while still constraining the structure of \Sigma k .
This is accomplished by means of a reparameterization of the covariance matrix \Sigma in terms
of its eigenvalue decomposition
where D k is the orthogonal matrix of eigenvectors and A k is a diagonal matrix whose elements
are proportional to eigenvalues of \Sigma k , and - k is a scalar proportional to the volume of the
ellipsoid. The orientation of the principal components of \Sigma k is determined by D k , while A k
determines the shape of the density contours. This paradigm is particularly useful for two
and three-dimensional data, where geometric features can often be identified visually. It
may also be applicable for higher-dimensional data when multivariate visualization analysis
some structure. For example, Banfield and Raftery [1] were able to closely match
the clinical classification of a biomedical data set using Gaussian hierarchical clustering after
analyzing its geometric features. The parameterization can be selected so as to allow some
but not all of the characteristics (orientation, volume and shape) of distributions to vary
between groups, while constraining others to be the same.
Analysis of the model that leads to the sum of squares criterion (\Sigma
of (16) suggests that it is likely to be most appropriate when groups are spherical and of
approximately the same size. The constant-variance assumption in which D k , - k and A k are
the same for all groups but otherwise unconstrained favors clusters that are ellipsoidal with
1 S-Plus 3.3 Version 3.3 for Unix, MathSoft, Inc., Seattle, WA (1995).
number of observations
time
100 200 300 400 500103050Diagonal (Uniform)
number of observations
time
100 200 300 400 500103050Diagonal (Varying)
number of observations
time
100 200 300 400 500103050Unconstrained
number of observations
time
100 200 300 400 50050015002500Constant Variance

Figure

1: CPU time vs number of observations for the four basic models. The solid line represents
the methods proposed in this paper.
the same orientation, shape, and volume. If all elements of \Sigma k are allowed to vary between
groups, the resulting classification is likely to contain elliptical groups with differing geometric
features. Metrics appropriate for various intermediate situations can also be formulated. For
example, assuming that \Sigma I or oe 2
I implies that the underlying densities are spherical,
while variation in - k between groups allows their volumes to differ. Celeux and Govaert [2]
analyzed this criterion and showed that it can give classification performance that is much
better than traditional methods. In one example, they successfully apply the method to an
astronomical image in which one tightly clustered galaxy is contained within another more
dispersed one.

Table

shows relationships between orientation, volume and shape discussed in [1]. Criteria
based on other combinations of these factors are also possible [3]. Software for hierarchical
clustering based on these models is available in the public domain (see [1]); it has been
used in a variety of applications with some success [9]. A revision based on the techniques
described in this paper is currently in progress.
Efficient computational methods for the first four models in Table 2 were given in Section
Distribution Volume Shape Orientation Reference
-I Spherical fixed fixed NA [11],[5], [10], [8], [1], [3]
I Spherical variable fixed NA [1], [3]
-DAD Elliptical fixed fixed fixed [5], [10], [1], [3]
Elliptical variable variable variable [10], [1], [3]
fixed fixed variable [8], [1], [3]
Elliptical variable fixed variable [1], [3]

Table

2: Parameterizations of the covariance matrix \Sigma k in the Gaussian model and their geometric
interpretation. The models shown here are those discussed in Banfield and Raftery [1].
2. We conclude this section by showing how those techniques can be applied to the remaining
. The relevant criteria are
\Gamma1\Omega
respectively,
k is the diagonal matrix of eigenvalues
of W k . In both cases, efficient algorithms are possible if s k and L T
are maintained and
updated in the storage provided for the original data, as described in Section 2.4. Instead
of information pertaining to the terms of (15), the kth term of the sum in the appropriate
criterion is stored in the space corresponding to the lower triangle of L T
k . As in all of
the methods in Section 3, the matrix W hi;ji is never explicitly formed. Instead, its nonzero
eigenvalues are obtained as the squares of the singular values of L T
, which has min(n
rows and p columns. For \Sigma we include the additive term inside the logarithm
that appears in (10) for \Sigma I or oe 2
k I in order to accommodate those cases in which W k
(and
vanishes.
Concluding Remarks
This paper has made several contributions toward computational efficiency in agglomerative
hierarchical clustering. First, we gave a memory-efficient scheme suitable for any method
that stores the change in criterion for each merged pair. Second, we showed that the sample-
cross product matrix for the union of two Gaussian clusters can be formed by a rank-one
update of the sum of sample-cross matrices of its constituent clusters, and described how
this can be used to obtain efficient algorithms for model-based clustering. This included a
memory-efficient initialization strategy for the sum of squares method, which corresponds
to the simplest Gaussian model, as well as time and memory efficient algorithms for three
other Gaussian models that have no counterpart in classical hierarchical agglomeration. At
the same time, we gave strategies to resolve the inherent ambiguities in some of the models.
Finally, we showed how these techniques can be easily extended to two additional Gaussian
models based on a more sophisticated parameterization of the covariance matrix that has
recently shown promise in practical applications.
A Derivation of the Update Formula
The following holds for the group hi; ji formed by merging groups i and
denotes the sample cross-product matrix for group k, and
where s k is the sum of the observations and n k the cardinality of group k.
Proof. Let X k be the matrix of observations corresponding to group k. If
denotes the vector of length n k in which every element is equal to 1 ( g X k is X k
with the mean subtracted out of each column), then W
Hence
since s
. It follows that
f
s hi;ji
Since X hi;ji is the matrix consisting of observations in groups i and j,
Hence
\Gamman
\Gamman
If
then
so that



--R


Comparison of the mixture and the classification maximum likelihood in cluster analysis.
Gaussian parsimonius clustering models.
Detecting features in spatial point processes with clutter via model-based clustering
On some invariant criteria for grouping data.
Matrix Computations.
Finding Groups in Data.
Fitting straight lines to point patterns.
Transitions from ONR Contract N00014-91-J-1074 'Time Series and Image Analysis'
Clustering methods based on likelihood ratio criteria.
Hierarchical groupings to optimize an objective function.
--TR

--CTR
A. Pigeau , M. Gelgon, Building and tracking hierarchical geographical & temporal partitions for image collection management on mobile devices, Proceedings of the 13th annual ACM international conference on Multimedia, November 06-11, 2005, Hilton, Singapore
Jun Liu , Jim P. Y. Lee , Lingjie Li , Zhi-Quan Luo , K. Max Wong, Online Clustering Algorithms for Radar Emitter Classification, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.27 n.8, p.1185-1196, August 2005
Bin He , Tao Tao , Kevin Chen-Chuan Chang, Organizing structured web sources by query schemas: a clustering approach, Proceedings of the thirteenth ACM international conference on Information and knowledge management, November 08-13, 2004, Washington, D.C., USA
Peter Meinecke , Helge Ritter, Resolution-Based Complexity Control for Gaussian Mixture Models, Neural Computation, v.13 n.2, p.453-475, February 2001
Marina Meil , David Heckerman, An Experimental Comparison of Model-Based Clustering Methods, Machine Learning, v.42 n.1-2, p.9-29, January-February 2001
D. J. Robinson , F. Murtagh , P. A. M. Basheer, Gaussian segmentation of BSE images to assess the porosity of concrete, Proceedings of the sixth conference on Computational structures technology, p.253-254, September 04-06, 2002
Diansheng Guo , Donna J. Peuquet , Mark Gahegan, ICEAGE: Interactive Clustering and Exploration of Large and High-Dimensional Geodata, Geoinformatica, v.7 n.3, p.229-253, September
Nevin L. Zhang, Hierarchical latent class models for cluster analysis, Eighteenth national conference on Artificial intelligence, p.230-237, July 28-August 01, 2002, Edmonton, Alberta, Canada
Nevin L. Zhang, Hierarchical Latent Class Models for Cluster Analysis, The Journal of Machine Learning Research, 5, p.697-723, 12/1/2004
Huidong Jin , Man-Leung Wong , K. -S. Leung, Scalable Model-Based Clustering for Large Databases Based on Data Summarization, IEEE Transactions on Pattern Analysis and Machine Intelligence, v.27 n.11, p.1710-1719, November 2005
Shi Zhong , Joydeep Ghosh, A unified framework for model-based clustering, The Journal of Machine Learning Research, 4, p.1001-1037, 12/1/2003
Fionn Murtagh, Clustering in massive data sets, Handbook of massive data sets, Kluwer Academic Publishers, Norwell, MA, 2002
