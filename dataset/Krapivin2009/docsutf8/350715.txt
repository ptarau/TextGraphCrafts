--T
An Inexact Hybrid Generalized Proximal Point Algorithm and Some New Results on the Theory of Bregman Functions.
--A
We present a new Bregman-function-based algorithm which is a modification of the generalized proximal point method for solving the variational inequality problem with a maximal monotone operator. The principal advantage of the presented algorithm is that it allows a more constructive error tolerance criterion in solving the proximal point subproblems. Furthermore, we eliminate the assumption of pseudomonotonicity which was, until now, standard in proving convergence for paramonotone operators. Thus we obtain a convergence result which is new even for exact generalized proximal point methods. Finally, we present some new results on the theory of Bregman functions. For example, we show that the standard assumption of convergence consistency is a consequence of the other properties of Bregman functions, and is therefore superfluous.
--B
Introduction
In this paper, we are concerned with proximal point algorithms for solving
the variational inequality problem. Specifically, we consider the methods
which are based on Bregman distance regularization. Our objective is two-
fold. First of all, we develop a hybrid algorithm based on inexact solution of
proximal subproblems. The important new feature of the proposed method is
that the error tolerance criterion imposed on inexact subproblem solution is
constructive and easily implementable for a wide range of applications. Sec-
ond, we obtain a number of new results on the theory of Bregman functions
and on the convergence of related proximal point methods. In particular, we
show that one of the standard assumptions on the Bregman function (con-
vergence consistency), as well as one of the standard assumptions on the
operator defining the problem (pseudomonotonicity, in the paramonotone
operator case), are extraneous.
Given an operator T on R n (point-to-set, in general) and a closed convex
subset C of R n , the associated variational inequality problem [12], from now
on VIP(T; C), is to find a pair x   and v   such that
where h\Delta; \Deltai stands for the usual inner product in R n . The operator
stands for the family of subsets of R n , is monotone if
for any x; y 2 R n and any u 2 T (x), v 2 T (y). T is maximal monotone if
it is monotone and its graph G(T
contained in the graph of any other monotone operator. Throughout this
paper we assume that T is maximal monotone.
It is well known that VIP(T; C) is closely related to the problem of finding
a zero of a maximal monotone operator -
Recall that we assume that T is maximal monotone. Therefore, (2) is a
particular case of VIP(T; C) for On the other hand, define NC as
the normal cone operator, that is NC
The operator T +NC is monotone and x   solves VIP(T; C) (with some v   2
only if
Additionally, if the relative interiors of C and of the domain of T intersect,
then T +NC is maximal monotone [31], and the above inclusion is a particular
case of (2), i.e., the problem of finding a zero of a maximal monotone operator.
Hence, in this case, VIP(T; C) can be solved using the classical proximal
point method for finding a zero of the operator -
. The proximal
point method was introduced by Martinet [26] and further developed by
Rockafellar [34]. Some other relevant papers on this method, its applications
and modifications, are [27, 33, 3, 29, 25, 17, 18, 15]; see [24] for a survey.
The classical proximal point algorithm generates a sequence fx k g by solving
a sequence of proximal subproblems. The iterate x k+1 is the solution of
regularization parameter. For the method to be imple-
mentable, it is important to handle approximate solutions of subproblems.
This consideration gives rise to the inexact version of the method [34], which
can be written as
where e k+1 is the associated error term. To guarantee convergence, it is
typically assumed that (see, for example, [34, 8])X
Note that even though the proximal subproblems are better conditioned than
the original problem, structurally they are as difficult to solve. This observation
motivates the development of the "nonlinear" or "generalized" proximal
point method [16, 13, 11, 19, 23, 22, 20, 6].
In the generalized proximal point method, x k+1 is obtained solving the
generalized proximal point subproblem
The function f is the Bregman function [2], namely it is strictly convex, differentiable
in the interior of C and its gradient is divergent on the boundary
of C (f also has to satisfy some additional technical conditions, which we
shall discuss in Section 2). All information about the feasible set C is embedded
in the function f , which is both a regularization and a penalization term.
Properties of f (discussed in Section 2) ensure that solutions of subproblems
belong to the interior of C without any explicit consideration of constraints.
The advantage of the generalized proximal point method is that the subproblems
are essentially unconstrained. For example, if VIP(T; C) is the classical
nonlinear complementarity problem [28], then a reasonable choice of f gives
proximal subproblems which are (unconstrained!) systems of nonlinear equa-
tions. By contrast, subproblems given by the classical proximal algorithm
are themselves nonlinear complementarity problems, which are structurally
considerably more difficult to solve than systems of equations. We refer the
reader to [6] for a detailed example.
As in the case of the classical method, implementable versions of the
generalized proximal point algorithm must take into consideration inexact
solution of subproblems:
In [14], it was established that if
exists and is finite , (3)
then the generated sequence converges to a solution (provided it exists) under
basically the same assumptions that are needed for the convergence of the
exact method. Other inexact generalized proximal algorithms are [7, 23,
41]. However, the approach of [14] is the simplest and the easiest to use in
practical computation (see the discussion in [14]). Still, the error criterion
given by (3) is not totally satisfactory. Obviously, there exist many error
sequences that satisfy the first relation in (3), and it is not very clear
which e k should be considered acceptable for each specific iteration k. In this
sense, criterion (3) is not quite constructive. The second relation in (3) is
even somewhat more problematic.
In this paper, we present a hybrid generalized proximal-based algorithm
which employs a more constructive error criterion than (3). Our method is
completely implementable when the gradient of f is easily invertible, which
is a common case for many important applications. The inexact solution is
used to obtain the new iterate in a way very similar to Bregman generalized
projections. When the error is zero, our algorithm coincides with the generalized
proximal point method. However, for nonzero error, it is different from
the inexact method of [14] described above. Our new method is motivated
by [40], where a constructive error tolerance was introduced for the classical
proximal point method. This approach has already proved to be very useful
in a number of applications [38, 37, 35, 39, 36].
Besides the algorithm, we also present a theoretical result which is new
even for exact methods. In particular, we prove convergence of the method
for paramonotone operators, without the previously used assumption of pseu-
domonotonicity (paramonotone operators were introduced in [4, 5], see also
[9, 21]; we shall state this definition in Section 3, together with the definition
of pseudomonotonicity). It is important to note that the subgradient of a
proper closed convex function is paramonotone, but need not be pseudomono-
tone. Hence, among other things, our result unifies the proof of convergence
for paramonotone operators and for minimization.
We also remove the condition of convergence consistency which has been
used to characterize Bregman functions, proving it to be a consequence of
the other properties.
This work is organized as follows. In Section 2, we discuss Bregman
functions and derive some new results on their properties. In Section 3, the
error tolerance to be used is formally defined, the new algorithm is described
and the convergence result is stated. Section 4 contains convergence analysis.
A few words about our notation are in order. Given a (convex) set A,
ri(A) will denote the relative interior, -
A will denote the closure, int(A) will
denote the interior, and bdry(A) will denote the boundary of A. For an
operator T , Dom(T ) stands for its domain, i.e., all points x 2 R n such that
Bregman Function and Bregman Distance
Given a convex function f on R n , finite at x; y 2 R n and differentiable at y,
the Bregman distance [2] between x and y, determined by f , is
Note that, by the convexity of f , the Bregman distance is always nonnegative.
We mention here the recent article [1] as one good reference on Bregman
functions and their properties.
Definition 2.1 Given S, a convex open subset of R n , we say that
is a Bregman function with zone S if
1. f is strictly convex and continuous in -
2. f is continuously differentiable in S,
3. for any x 2 -
S and ff 2 R, the right partial level set
fy
is bounded,
4. If fy k g is a sequence in S converging to y, then
lim
Some remarks are in order regarding this definition. In addition to the above
four items, there is one more standard requirement for Bregman function,
namely Convergence Consistency :
If fx k g ae -
S is bounded, fy k g ae S converges to y, and
also converges to y.
This requirement has been imposed in all previous studies of Bregman functions
and related algorithms [10, 11, 13, 19, 9, 14, 1, 22, 20, 6]. In what
follows, we shall establish that convergence consistency holds automatically
as a consequence of Definition 2.1 (we shall actually prove a stronger result).
The original definition of a Bregman function also requires the left partial
level sets
to be bounded for any y 2 S. However, it has been already observed that this
condition is not needed to prove convergence of proximal methods (e.g., [14]).
And it is known that this boundedness condition is extraneous regardless,
since it is also a consequence of Definition 2.1 (e.g., see [1]). Indeed, observe
that for any y, the level set L 0 (0; so it is nonempty and bounded.
Also Definition 2.1 implies that D f (\Delta; y) is a proper closed convex function.
Because this function has one level set which in nonempty and bounded, it
follows that all of its level sets are bounded (i.e., L 0 (ff; y) is bounded for every
ff) [32, Corollary 8.7.1].
To prove convergence consistency using the properties given in Definition
2.1, we start with the following results.
Lemma 2.2 (The Restricted Triangular Inequality)
Let f be a convex function satisfying items 1 and 2 of Definition 2.1. If x 2 -
and w is a proper convex combination of x and y, i.e.,
with
Proof. We have that
Since rf is monotone,
Taking into account that w latter relation yields
Therefore
Lemma 2.3 Let f be a convex function satisfying items 1 and 2 of Definition
2.1. If fx k g is a sequence in -
S converging to x, fy k g is a sequence in S
converging to y and y 6= x, then
lim inf
Proof. Define
is a sequence in S converging to z
S. By the
convexity of f , it follows that for all k:
Therefore
Letting k !1 we obtain
Using the strict convexity of f and the hypothesis x 6= y, the desired result
follows.
We are now ready to prove a result which is actually stronger than the
property of convergence consistency discussed above. This result will be
crucial for strengthening convergence properties of proximal point methods,
carried out in this paper.
Theorem 2.4 Let f be a convex function satisfying items 1 and 2 of Definition
2.1. If fx k g is a sequence in -
fy k g is a sequence in S,
lim
and one of the sequences (fx k g or fy k g) converges, then the other also converges
to the same limit.
Proof. Suppose, by contradiction, that one of the sequences converges and
the other does not converge or does not converge to the same limit. Then
there exist some " ? 0 and a subsequence of indices fk j g satisfying
Suppose first that fy k g converges and
lim
i.e., ~ x j is a proper convex combination of x k j and y k j . Using Lemma 2.2 we
conclude that D f (~x which implies that
lim
fy k j g converges, it follows that f~x j g is bounded and
there exists a subsequence f~x j i g converging to some ~
x. Therefore we have
the following set of relations
which is in contradiction with Lemma 2.3.
If we assume that the sequence fx k g converges, then reversing the roles of
and fy k g in the argument above, we reach a contradiction with Lemma
2.3 in exactly the same manner.
It is easy to see that Convergence Consistency is an immediate consequence
of Theorem 2.4.
We next state a well-known result which is widely used in the analysis of
generalized proximal point methods.
Lemma 2.5 (Three-Point Lemma)[11]
Let f be a Bregman function with zone S as in Definition 2.1. For any
holds that
In the sequel, we shall use the following consequence of Lemma 2.5, which can
be obtained by subtracting the three-point inequalities written with
and s; x; z.
Corollary 2.6 (Four-Point Lemma)
Let f be a Bregman function with zone S as in Definition 2.1. For any
holds that
3 The Inexact Generalized Proximal Point
Method
We start with some assumptions which are standard in the study and development
of Bregman-function-based algorithms.
Suppose C, the feasible set of VIP(T; C), has nonempty interior, and we
have chosen f , an associated Bregman function with zone int(C). We also
assume that
so that T +N C is maximal monotone [31]. The solution set of VIP(T; C) is
We assume this set to be nonempty, since this is the more interesting case.
In principle, following standard analysis, results regarding unboundedness of
the iterates can be obtained for the case when no solution exists.
Additionally, we need the assumptions which guarantee that proximal
subproblem solutions exist and belong to the interior of C.
H1 For any x 2 int(C) and c ? 0, the generalized proximal subproblem
has a solution.
H2 For any x 2 int(C), if fy k g is a sequence in int(C) and
lim
then
lim
A simple sufficient condition for H1 is that the image of rf is the whole
space R n (see [6, Proposition 3]). Assumption H2 is called boundary coer-
civeness and it is the key concept in the context of proximal point methods
for constrained problems for the following reason. It is clear from Definition
2.1 that if f is a Bregman function with zone int(C) and P is any open sub-set
of int(C), then f is also a Bregman function with zone P , which means
that one cannot recover C from f . Therefore in order to use the Bregman
distance D f for penalization purposes, f has to possess an additional prop-
erty. In particular, f should contain information about C. This is precisely
the role of H2 because it implies divergence of rf on bdry(C), which makes
C defined by f :
Divergence of rf also implies that the proximal subproblems cannot have
solutions on the boundary of C. We refer the readers to [9, 6] for further
details on boundary coercive Bregman functions. Note also that boundary
coerciveness is equivalent to f being essentially smooth on int(C) [1, Theorem
4.5 (i)].
It is further worth to note that if the domain of rf is the interior of C,
and the image of rf is R n , then H1 and H2 hold automatically (see [6,
Proposition 3] and [9, Proposition 7]).
We are now ready to describe our error tolerance criterion. Take any
consider the proximal subproblem
which is to find a pair (y; v) satisfying the proximal system
The latter is in turn equivalent to
Therefore, an approximate solution of (5) (or (6) or (7)) should satisfy
We next formally define the concept of inexact solutions of (6), taking the
approach of (8).
Definition 3.1 Let x 2 int(C), c ? 0 and oe 2 [0; 1). We say that a pair
(y; v) is an inexact solution with tolerance oe of the proximal subproblem (6)
if
and z, the solution of equation
satisfies
Note that from (4) (which is a consequence of H2), it follows that
Note that equivalently z is given by
Therefore z, and hence D f (y; z), are easily computable from x; y and v whenever
rf is explicitly invertible. In that case it is trivial to check whether a
given pair (y; v) is an admissible approximate solution in the sense of Definition
3.1: it is enough to obtain z verify if
our algorithm is based on this test, it is most
easy to implement when rf is explicitly invertible. We point out that this
case covers a wide range of important applications. For example, Bregman
functions with this property are readily available when the feasible set C is
an orthant, a polyhedron, a box, or a ball (see [9]).
Another important observation is that for oe = 0, we have that
Hence, the only point which satisfies Definition 3.1 for precisely the
exact solution of the proximal subproblem. Therefore our view of inexact
solution of generalized proximal subproblems is quite natural. We note,
in the passing, that it is motivated by the approach developed in [40] for
the classical ("linear") proximal point method. In that case, Definition 3.1
(albeit slightly modified) is equivalent to saying that the subproblems are
solved within fixed relative error tolerance (see also [37]). Such an approach
seems to be computationally more realistic/constructive than the common
summable-error-type requirements.
Regarding the existence of inexact solutions, the situation is clearly even
easier than for exact methods. Since we are supposing that the generalized
proximal problem (5) has always an exact solution in int(C), this problem
will certainly always have (possibly many) inexact solutions (y; v) satisfying
also y 2 C.
Now we can formally state our inexact generalized proximal method.
Algorithm 1 Inexact Generalized Proximal Method.
Initialization: Choose some c ? 0, and the error tolerance parameter oe 2
[0; 1). Choose some x
Iteration k: Choose the regularization parameter c k - c, and find (y
an inexact solution with tolerance oe of
satisfying
repeat.
We have already discussed the possibility of solving inexactly (9) with condition
(10). Another important observation is that since for
subproblem solution coincides with the exact one, in that case Algorithm
1 produces the same iterates as the standard exact generalized proximal
method. Hence, all our convergence results (some of them are new!) apply
also to the exact method. For oe 6= 0 however, there is no direct relation
between the iterates of Algorithm 1 and
considered in [14]. The advantage of our approach is that it allows an attractive
constructive stopping criterion (given by Definition 3.1) for approximate
solution of subproblems (at least, when rf is invertible).
Under our hypothesis, Algorithm 1 is well-defined. From now on, fx k g
and f(y k ; v k )g are sequences generated by Algorithm 1. Therefore, by the
construction of Algorithm 1 and by Definition 3.1, for all k it holds that
We now state our main convergence result. First, recall that a maximal
monotone operator T is paramonotone ([4, 5], see also [9, 21]) if
Some examples of paramonotone operators are subdifferentials of proper
closed convex functions, and strictly monotone maximal monotone operators

Theorem 3.2 Suppose that VIP(T; C) has solutions and one of the following
two conditions holds :
1.
2. T is paramonotone.
Then the sequence fx k g converges to a solution of VIP(T; C).
Thus we establish convergence of our inexact algorithm under assumptions
which are even weaker than the ones that have been used, until now, for
exact algorithms. Specifically, in the paramonotone case, we get rid of the
"pseudomonotonicity" assumption on T [6] which can be stated as follows:
Take any sequence fy k g ae Dom(T ) converging to y and any sequence
for each x 2 Dom(T ) there exists and element
(y) such that
Until now, this (or some other, related) technical assumption was employed
in the analysis of all generalized proximal methods (e.g., [14, 7, 6]). Among
other things, this resulted in splitting the proof of convergence for the case of
minimization and for paramonotone operators (the subdifferential of a convex
function is paramonotone, but it need not satisfy the above condition).
And of course, the additional requirement of pseudomonotonicity makes the
convergence result for paramonotone operators weaker. Since for the tolerance
parameter our Algorithm 1 reduces to the exact generalized
proximal method, Theorem 3.2 also constitutes a new convergence result for
the standard setting of exact proximal algorithms. We note that the stronger
than convergence consistency property of Bregman functions established in
this paper is crucial for obtaining this new result.
To obtain this stronger result, the proof will be somewhat more involved
than the usual, and some auxiliary analysis will be needed. However, we think
that this is worthwhile since it allows us to remove some (rather awkward)
additional assumptions.
Convergence Analysis
Given sequences fx k g, fy k g and fv k g generated by Algorithm 1, define ~
X as
all points x 2 C for which the index set
is finite. For x 2 ~
X, define k(x) as the smallest integer such that
0:
Of course, the set ~
X and the application k(\Delta) depend on the particular sequences
generated by the algorithm. These definitions will facilitate the
subsequent analysis. Note that, by monotonicity of T ,
and in fact,
Lemma 4.1 For any s 2 ~
X and k - k(s), it holds that
Proof. Take s 2 ~
X and k - k(s). Using Lemma 2.6, we get
By (14) and (15), we further obtain
which proves the first inequality in (16). Since the Bregman distance is
always nonnegative and oe 2 [0; 1), we have
The last inequality in (16) follows directly from the hypothesis s 2 ~
k(s) and the respective definitions.
As an immediate consequence, we obtain that the sequence fD f (-x; x k )g
is decreasing for any - x 2 X   .
Corollary 4.2 If the sequence fx k g has an accumulation point - x 2 X   then
the whole sequence converges to - x.
Proof. Suppose that some subsequence fx k j g converges to - x 2 X   . Using
Defintion 2.1 (item 4), we conclude that
lim
Since the whole sequence fD f (-x; x k )g is decreasing and it has a convergent
subsequence, it follows that it converges :
lim
Now the desired result follows from Theorem 2.4.
Corollary 4.3 Suppose that ~
;. Then the following statements hold:
1. The sequence fx k g is bounded ;
2.
3. For any s 2 ~
4. The sequence fy k g is bounded .
Proof. Take some s 2 ~
X. From Lemma 4.1 it follows that for all k greater
than k(s), D f (s; x k Therefore, D f (s; x k ) is bounded and
from Definition 2.1 (item 3), it follows that fx k g is bounded.
By Lemma 4.1, it follows that for any r 2 N
Therefore
Since r is arbitrary and the terms of both summations are nonnegative, (recall
the definition of k(s)), it follows that we can take the limit as r !1 in both
sides of the latter relation. Taking further into account that fc k g is bounded
away from zero, the second and third assertions of the Corollary easily follow.
As consequences, we also obtain that
lim
and
lim
X: (18)
Suppose now that fy k g is unbounded. Then there exists a pair of subsequences
and fy k j g such that fx k j g converges but fy k j g diverges. How-
ever, by (17) and Theorem 2.4, fy k j g must converge (to the same limit as
which contradicts the assumption. Hence, fy k g is bounded.
The next proposition establishes the first part of Theorem 3.2, namely
the convergence of the inexact generalized proximal algorithm in the case
when
Proposition 4.4 If ~
converges to some x 2 int(C)
which is a solution of VIP(T; C).
Proof. By Corollary 4.3, it follows that fx k g is bounded, so it has some
accumulation point - x 2 C, and for some subsequence fx k j g,
lim
Take any - x 2 ~
and, by H2,
lim
it follows that
lim
But the latter is impossible because D f (-x; x k ) is a decreasing sequence, at
least for k - k(-x) (by Lemma 4.1). Hence,
Next, we prove that -
x is a solution of VIP(T; C). By (17), we have that
lim
Because, by (15), D f (y
lim
converges to -
Theorem 2.4 and (19) imply that fy k j g also
converges to -
x. Applying Theorem 2.4 once again, this time with (20), we
conclude that fx k j +1 g also converges to -
x. Since -
and rf is
continuous in int(C), we therefore conclude that
lim
using (14) we get
lim
Now the fact that fy k j
together with the maximality
of T , implies that 0 2 T (-x). Thus we have a subsequence fx k j g
converging to -
. By Corollary 4.2, the whole sequence fx k g converges
to - x.
We proceed to analyze the case when T is paramonotone. By (18), we
already know that if s
the limit with respect to v k (for example, using the technical assumption of
pseudomonotonicity stated above), then we could conclude that 0 - h-v; -
x is an accumulation point of fx k g (hence
also of fy k g), and - v 2 T (-x); v s 2 T (s). By paramonotonicity, it follows
that v s 2 T (-x). Now by monotonicity, we further obtain that for any x 2 C
which means that -
However, in the absence of the assumption of pseudomonotonicity one cannot
use this well-established line of argument. To overcome the difficulty resulting
from the impossibility of directly passing onto the limit as was done above,
we shall need some auxiliary constructions.
Let A be the affine hull of the domain of T . Then there exists some V , a
subspace of R n , such that
for any x 2 Dom(T ). Denote by P the orthogonal projection onto V , and
for each k define
The idea is to show the following key facts:
k g has an accumulation point :
With these facts in hand, we could pass onto the limit in a manner similar
to the above, and complete the proof.
First, note that
This can be verified rather easily: if x 62 Dom(T ) then both sets in (21) are
empty, so it is enough to consider x
then
so that By monotonicity of T , for
any z 2 Dom(T ) and any w 2 T (z), it holds that
holds that
Therefore
which implies that u 2 T (x) by the maximality of T . Since also
follows that u 2 T
Lemma 4.5 If ~
some subsequence of fu k g
is bounded.
Proof. We assumed that Dom(T
and let P be the projection operator onto V discussed above. In particular,
Furthermore, the
operator -
defined by
is maximal monotone as an operator on the space V (this can be easily
verified using the maximal monotonicity of T on R n ). We also have that
T is bounded around zero [30]. So, P ffi T is
bounded around -
x, i.e., there exist some r ? 0 and M - 0 such that
Since ~
X. Therefore, by the definition of
~
X, there exists an infinite subsequence of indices fk j g such that
Note that u holds that
for each j,
Then for each j there exists -
Furthermore,
where the first inequality is by the monotonicity of T , and the second is
by (22). Using further the Cauchy-Schwarz and triangular inequalities, we
obtain
Since the sequence fy k g is bounded (Corollary 4.3, item 4), it follows that
We conclude the analysis by establishing the second part of Theorem 3.2.
Proposition 4.6 Suppose X   6= ; and T is paramonotone. Then fx k g converges
to some -
Proof. If ~
then the conclusion follows from Proposition 4.4.
Suppose now that
~
By Lemma 4.5, it follows that some subsequence of fu k g is bounded. Since
X, from Corollary 4.3 it follows that the whole sequence fx k g is
bounded. Hence, there exist two subsequences fx k j g, fu k j g which both converge
lim
lim
Recall from the proof of Lemma 4.5 that u 4.3
(item 2), we have that
lim
Therefore, by Theorem 2.4,
lim
and
by the maximality of T . Take now some s 2 X   . There exists some v s 2 T
such that
for all x 2 C. Therefore, using also the monotonicity of T ,
Note that for any x 2 Dom(T )
Taking passing onto the limit as j !1, (18) implies that
Together with (23), this implies that
Using now the paramonotonicity of T , we conclude that
Finally, for any x 2 C, we obtain
xi
0:
Therefore -
we have a subsequence fx k j g converging to -
from Corollary 4.2 it follows that the whole sequence fx k g converges to -
x.



--R

Legendre functions and the method of random Bregman projections.
The relaxation method of finding the common points of convex sets and its application to the solution of problems in convex programming.
Produits infinis de r'esolvantes.
An iterative solution of a variational inequality for certain monotone operators in a Hilbert space.
Corrigendum to
A generalized proximal point algorithm for the variational inequality problem in a Hilbert space.
Enlargement of monotone operators with applications to variational inequalities.
A variable metric proximal point algorithm for monotone operators.
An interior point method with Bregman functions for the variational inequality problem with para- monotone operators
The proximal minimization algorithm with D-functions
Convergence analysis of proximal-like optimization algorithm using Bregman functions
Variational Inequalities and Complementarity Problems
Nonlinear proximal point algorithms using Bregman func- tions
Approximate iterations in Bregman-function-based proximal algorithms
On the Douglas-Rachford splitting method and the proximal point algorithm for maximal monotone op- erators
Multiplicative iterative algorithms for convex pro- gramming
Finite termination of the proximal point algorithm.
New proximal point algorithms for convex minimization.
On some properties of generalized proximal point methods for quadratic and linear programming.
On some properties of generalized proximal point methods for the variational inequality problem.
On some properties of paramonotone operators.

Proximal minimization methods with generalized Bregman functions.
The proximal algorithm.
Asymptotic convergence analysis of the proximal point algorithm.
Regularisation d'inequations variationelles par approximations successives.
Proximit'e et dualit'e dans un espace Hilbertien.
Complementarity problems.
Weak convergence theorems for nonexpansive mappings in Banach spaces.
Local boundedness of nonlinear monotone operators.
On the maximality of sums of nonlinear monotone operators.
Convex Analysis.
Augmented Lagrangians and applications of the proximal point algorithm in convex programming.
Monotone operators and the proximal point algorithm.
A truly globally convergent Newton-type method for the monotone nonlinear complementarity problem
A comparison of rates of convergence of two inexact proximal point algorithms.
Forcing strong convergence of proximal point iterations in a Hilbert space
A globally convergent inexact Newton method for systems of monotone equations.
A hybrid approximate extragradient- proximal point algorithm using the enlargement of a maximal monotone operator
A hybrid projection - proximal point algorithm
Convergence of proximal-like algorithms
--TR

--CTR
Lev M. Bregman , Yair Censor , Simeon Reich , Yael Zepkowitz-Malachi, Finding the projection of a point onto the intersection of convex sets via projections onto half-spaces, Journal of Approximation Theory, v.124 n.2, p.194-218, October
