The asymptotic behavior of a nonlinear continuous time filtering problem is studied when the variance of the observation noise tends to 0. We suppose that the signal is a two-dimensional process from which only one of the components is noisy and that a one-dimensional function of this signal, depending only on the unnoisy component, is observed in a low noise channel. An approximate filter is considered in order to solve this problem. Under some detectability assumptions, we prove that the filtering error converges to 0, and an upper bound for the convergence rate is given. The efficiency of the approximate filter is compared with the efficiency of the optimal filter, and the order of magnitude of the error between the two filters, as the observation noise vanishes, is obtained. 