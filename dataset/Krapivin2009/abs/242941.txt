We describe a divide-and-conquer tridiagonalization approach for  matrices with repeated eigenvalues. Our algorithm hinges on the  fact that, under easily constructively verifiable conditions,   a symmetric matrix with band width  $b$ and $k$ distinct eigenvalues must be block diagonal with  diagonal blocks of size at most $b k$. A slight modification of  the usual orthogonal band-reduction algorithm allows us to reveal  this structure, which then leads to potential parallelism in the form of  independent diagonal blocks.  Compared to the usual Householder  reduction algorithm, the new approach exhibits improved data  locality, significantly more scope for parallelism, and the potential to  reduce arithmetic complexity by close to 50% for matrices that have only  two numerically distinct eigenvalues. The actual improvement   depends to a large extent on the number of distinct eigenvalues and  a good estimate thereof. However, at worst the algorithms behave like  a successive band-reduction approach to tridiagonalization.  Moreover, we provide   a numerically reliable and effective algorithm for computing  the eigenvalue decomposition of a   symmetric matrix with two numerically distinct eigenvalues.  Such matrices arise, for example, in invariant subspace decomposition  approaches to the symmetric eigenvalue problem. 